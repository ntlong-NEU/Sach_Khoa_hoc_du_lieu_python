{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace3d7c5-d3e0-4794-a1f2-a9ad82831335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chương 12: Các mô hình học máy có giám sát\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb95c1",
   "metadata": {},
   "source": [
    "Như chúng ta đã đề cập trước đó, học máy có giám sát là một trong những loại học máy được sử dụng phổ biến và thành công nhất. Trong chương này, chúng ta sẽ mô tả chi tiết hơn về học có giám sát và giải thích một số thuật toán học có giám sát phổ biến. Chúng ta đã thấy một ứng dụng của học máy có giám sát trong Chương 1: phân loại hoa diên vĩ thành nhiều loài bằng cách sử dụng các phép đo vật lý của hoa.\n",
    "\n",
    "Hãy nhớ rằng học có giám sát được sử dụng bất cứ khi nào chúng ta muốn dự đoán một kết quả nhất định từ một đầu vào đã cho và chúng ta có các ví dụ về các cặp đầu vào/đầu ra. Chúng ta xây dựng một mô hình học máy từ các cặp đầu vào/đầu ra này, bao gồm tập huấn luyện của chúng ta. Mục tiêu của chúng ta là đưa ra các dự đoán chính xác cho dữ liệu mới, chưa từng thấy. Học có giám sát thường đòi hỏi nỗ lực của con người để xây dựng tập huấn luyện, nhưng sau đó sẽ tự động hóa và thường tăng tốc một nhiệm vụ tốn nhiều công sức hoặc không khả thi.\n",
    "\n",
    "### Phân loại và Hồi quy\n",
    "\n",
    "Có hai loại chính của các bài toán học máy có giám sát, được gọi là phân loại và hồi quy.\n",
    "\n",
    "Trong phân loại, mục tiêu là dự đoán một nhãn lớp, là một lựa chọn từ một danh sách các khả năng được xác định trước. Trong Chương 1, chúng ta đã sử dụng ví dụ về việc phân loại hoa diên vĩ thành một trong ba loài có thể. Phân loại đôi khi được tách thành phân loại nhị phân, là trường hợp đặc biệt của việc phân biệt giữa chính xác hai lớp, và phân loại đa lớp, là phân loại giữa nhiều hơn hai lớp. Bạn có thể nghĩ về phân loại nhị phân như đang cố gắng trả lời một câu hỏi có/không. Phân loại email là thư rác hoặc không phải thư rác là một ví dụ về một bài toán phân loại nhị phân. Trong nhiệm vụ phân loại nhị phân này, câu hỏi có/không được hỏi sẽ là \"Email này có phải là thư rác không?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5a134",
   "metadata": {},
   "source": [
    ">Trong phân loại nhị phân, chúng ta thường nói về một lớp là lớp dương và lớp còn lại là lớp âm. Ở đây, dương không đại diện cho lợi ích hay giá trị, mà là đối tượng của nghiên cứu. Vì vậy, khi tìm kiếm thư rác, \"dương\" có thể có nghĩa là lớp thư rác. Việc lớp nào trong hai lớp được gọi là dương thường là một vấn đề chủ quan và cụ thể cho từng lĩnh vực.\n",
    "\n",
    "Ví dụ về hoa diên vĩ, mặt khác, là một ví dụ về một bài toán phân loại đa lớp. Một ví dụ khác là dự đoán một trang web đang ở ngôn ngữ nào từ văn bản trên trang web. Các lớp ở đây sẽ là một danh sách được xác định trước các ngôn ngữ có thể có.\n",
    "\n",
    "Đối với các nhiệm vụ hồi quy, mục tiêu là dự đoán một số liên tục, hoặc một số dấu phẩy động theo thuật ngữ lập trình (hoặc số thực theo thuật ngữ toán học). Dự đoán thu nhập hàng năm của một người từ trình độ học vấn, tuổi tác và nơi họ sống là một ví dụ về một nhiệm vụ hồi quy. Khi dự đoán thu nhập, giá trị dự đoán là một số tiền và có thể là bất kỳ số nào trong một phạm vi nhất định. Một ví dụ khác về một nhiệm vụ hồi quy là dự đoán năng suất của một trang trại ngô dựa trên các thuộc tính như năng suất trước đó, thời tiết và số lượng nhân viên làm việc trong trang trại. Năng suất một lần nữa có thể là một số tùy ý.\n",
    "\n",
    "Một cách dễ dàng để phân biệt giữa các nhiệm vụ phân loại và hồi quy là hỏi xem có một loại liên tục nào trong đầu ra không. Nếu có sự liên tục giữa các kết quả có thể có, thì bài toán đó là một bài toán hồi quy. Hãy nghĩ về việc dự đoán thu nhập hàng năm. Có một sự liên tục rõ ràng trong đầu ra. Việc một người kiếm được 40.000 đô la hay 40.001 đô la một năm không tạo ra sự khác biệt hữu hình, mặc dù đây là những số tiền khác nhau; nếu thuật toán của chúng ta dự đoán 39.999 đô la hoặc 40.001 đô la khi nó lẽ ra phải dự đoán 40.000 đô la, chúng ta không bận tâm nhiều.\n",
    "\n",
    "Ngược lại, đối với nhiệm vụ nhận dạng ngôn ngữ của một trang web (là một bài toán phân loại), không có vấn đề về mức độ. Một trang web ở một ngôn ngữ, hoặc nó ở một ngôn ngữ khác. Không có sự liên tục giữa các ngôn ngữ, và không có ngôn ngữ nào nằm giữa tiếng Anh và tiếng Pháp.¹\n",
    "\n",
    "--- \n",
    "¹Chúng tôi yêu cầu các nhà ngôn ngữ học tha thứ cho việc trình bày đơn giản hóa các ngôn ngữ như những thực thể riêng biệt và cố định."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcbb58d",
   "metadata": {},
   "source": [
    "### Tổng quát hóa, Quá khớp và Thiếu khớp\n",
    "\n",
    "Trong học có giám sát, chúng ta muốn xây dựng một mô hình trên dữ liệu huấn luyện và sau đó có thể đưa ra các dự đoán chính xác trên dữ liệu mới, chưa từng thấy có cùng các đặc điểm như tập huấn luyện mà chúng ta đã sử dụng. Nếu một mô hình có thể đưa ra các dự đoán chính xác trên dữ liệu chưa từng thấy, chúng ta nói rằng nó có khả năng tổng quát hóa từ tập huấn luyện sang tập kiểm tra. Chúng ta muốn xây dựng một mô hình có khả năng tổng quát hóa chính xác nhất có thể.\n",
    "\n",
    "Thông thường, chúng ta xây dựng một mô hình theo cách mà nó có thể đưa ra các dự đoán chính xác trên tập huấn luyện. Nếu tập huấn luyện và tập kiểm tra có đủ điểm chung, chúng ta mong đợi mô hình cũng sẽ chính xác trên tập kiểm tra. Tuy nhiên, có một số trường hợp điều này có thể sai. Ví dụ, nếu chúng ta cho phép mình xây dựng các mô hình rất phức tạp, chúng ta luôn có thể chính xác như chúng ta muốn trên tập huấn luyện."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19148219",
   "metadata": {},
   "source": [
    "Hãy xem một ví dụ bịa đặt để minh họa điểm này. Giả sử một nhà khoa học dữ liệu mới vào nghề muốn dự đoán liệu một khách hàng có mua thuyền hay không, dựa trên hồ sơ của những người mua thuyền trước đó và những khách hàng mà chúng ta biết không quan tâm đến việc mua thuyền. Mục tiêu là gửi email quảng cáo đến những người có khả năng thực sự mua hàng, nhưng không làm phiền những khách hàng sẽ không quan tâm.\n",
    "\n",
    "Giả sử chúng ta có hồ sơ khách hàng được hiển thị trong Bảng 2-1.\n",
    "\n",
    "**Bảng 2-1. Dữ liệu ví dụ về khách hàng**\n",
    "\n",
    "| Tuổi | Số xe sở hữu | Sở hữu nhà | Tình trạng hôn nhân | Số con | Sở hữu chó | Đã mua thuyền |\n",
    "|---|---|---|---|---|---|---|\n",
    "| 66 | 1 | có | góa | 2 | không | có |\n",
    "| 52 | 2 | có | đã kết hôn | 3 | không | có |\n",
    "| 22 | 0 | không | đã kết hôn | 0 | có | không |\n",
    "| 25 | 1 | không | độc thân | 1 | không | không |\n",
    "| 44 | 0 | không | đã ly hôn | 2 | có | không |\n",
    "| 39 | 1 | có | đã kết hôn | 2 | có | không |\n",
    "| 26 | 1 | không | độc thân | 2 | không | không |\n",
    "| 40 | 3 | có | đã kết hôn | 1 | có | không |\n",
    "| 53 | 2 | có | đã ly hôn | 2 | không | có |\n",
    "| 64 | 2 | có | đã ly hôn | 3 | không | không |\n",
    "| 58 | 2 | có | đã kết hôn | 2 | có | có |\n",
    "| 33 | 1 | không | độc thân | 1 | không | không |\n",
    "\n",
    "--- \n",
    "²Trong thế giới thực, đây thực sự là một vấn đề phức tạp. Mặc dù chúng ta biết rằng những khách hàng khác chưa mua thuyền của chúng ta, họ có thể đã mua một chiếc từ người khác, hoặc họ có thể vẫn đang tiết kiệm và có kế hoạch mua một chiếc trong tương lai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3de163",
   "metadata": {},
   "source": [
    "Sau khi xem xét dữ liệu một thời gian, nhà khoa học dữ liệu mới vào nghề của chúng ta đã đưa ra quy tắc sau: \"Nếu khách hàng trên 45 tuổi, và có ít hơn 3 người con hoặc chưa ly hôn, thì họ muốn mua một chiếc thuyền.\" Khi được hỏi quy tắc này của anh ta hoạt động tốt như thế nào, nhà khoa học dữ liệu của chúng ta trả lời: \"Nó chính xác 100 phần trăm!\" Và thực sự, trên dữ liệu có trong bảng, quy tắc này hoàn toàn chính xác. Có rất nhiều quy tắc có thể chúng ta có thể nghĩ ra để giải thích một cách hoàn hảo nếu ai đó trong bộ dữ liệu này muốn mua một chiếc thuyền. Không có độ tuổi nào xuất hiện hai lần trong dữ liệu, vì vậy chúng ta có thể nói những người 66, 52, 53 hoặc 58 tuổi muốn mua một chiếc thuyền, trong khi tất cả những người khác thì không. Mặc dù chúng ta có thể tạo ra nhiều quy tắc hoạt động tốt trên dữ liệu này, hãy nhớ rằng chúng ta không quan tâm đến việc đưa ra dự đoán cho bộ dữ liệu này; chúng ta đã biết câu trả lời cho những khách hàng này. Chúng ta muốn biết liệu khách hàng mới có khả năng mua thuyền hay không. Do đó, chúng ta muốn tìm một quy tắc sẽ hoạt động tốt cho khách hàng mới, và việc đạt được độ chính xác 100 phần trăm trên tập huấn luyện không giúp chúng ta ở đó. Chúng ta có thể không mong đợi rằng quy tắc mà nhà khoa học dữ liệu của chúng ta nghĩ ra sẽ hoạt động rất tốt đối với khách hàng mới. Nó có vẻ quá phức tạp và được hỗ trợ bởi rất ít dữ liệu. Ví dụ, phần \"hoặc chưa ly hôn\" của quy tắc phụ thuộc vào một khách hàng duy nhất.\n",
    "\n",
    "Thước đo duy nhất về việc liệu một thuật toán có hoạt động tốt trên dữ liệu mới hay không là đánh giá trên tập kiểm tra. Tuy nhiên, một cách trực quan³ chúng ta mong đợi các mô hình đơn giản sẽ tổng quát hóa tốt hơn cho dữ liệu mới. Nếu quy tắc là \"Những người trên 50 tuổi muốn mua thuyền\", và điều này sẽ giải thích hành vi của tất cả các khách hàng, chúng ta sẽ tin tưởng nó hơn là quy tắc liên quan đến con cái và tình trạng hôn nhân ngoài tuổi tác. Do đó, chúng ta luôn muốn tìm mô hình đơn giản nhất. Xây dựng một mô hình quá phức tạp so với lượng thông tin chúng ta có, như nhà khoa học dữ liệu mới vào nghề của chúng ta đã làm, được gọi là quá khớp (overfitting). Quá khớp xảy ra khi bạn điều chỉnh một mô hình quá sát với các đặc thù của tập huấn luyện và thu được một mô hình hoạt động tốt trên tập huấn luyện nhưng không thể tổng quát hóa cho dữ liệu mới. Mặt khác, nếu mô hình của bạn quá đơn giản—chẳng hạn như \"Mọi người sở hữu một ngôi nhà đều mua một chiếc thuyền\"—thì bạn có thể không thể nắm bắt được tất cả các khía cạnh và sự biến thiên trong dữ liệu, và mô hình của bạn sẽ hoạt động kém ngay cả trên tập huấn luyện. Chọn một mô hình quá đơn giản được gọi là thiếu khớp (underfitting).\n",
    "\n",
    "Chúng ta càng cho phép mô hình của mình phức tạp hơn, chúng ta càng có thể dự đoán tốt hơn trên dữ liệu huấn luyện. Tuy nhiên, nếu mô hình của chúng ta trở nên quá phức tạp, chúng ta bắt đầu tập trung quá nhiều vào từng điểm dữ liệu riêng lẻ trong tập huấn luyện của mình, và mô hình sẽ không tổng quát hóa tốt cho dữ liệu mới.\n",
    "\n",
    "Có một điểm ngọt ngào ở giữa sẽ mang lại hiệu suất tổng quát hóa tốt nhất. Đây là mô hình chúng ta muốn tìm.\n",
    "\n",
    "Sự đánh đổi giữa quá khớp và thiếu khớp được minh họa trong Hình 2-1.\n",
    "\n",
    "--- \n",
    "³Và cũng có thể chứng minh được, với phép toán phù hợp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f906d160",
   "metadata": {},
   "source": [
    "![Biểu đồ đánh đổi độ phức tạp của mô hình](https://i.imgur.com/your_image_path_here.png)\n",
    "**Hình 2-1. Đánh đổi độ phức tạp của mô hình với độ chính xác trên tập huấn luyện và tập kiểm tra**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e661aa",
   "metadata": {},
   "source": [
    "### Mối quan hệ giữa độ phức tạp của mô hình và kích thước tập dữ liệu\n",
    "\n",
    "Điều quan trọng cần lưu ý là độ phức tạp của mô hình có mối liên hệ mật thiết với sự biến thiên của các đầu vào có trong tập dữ liệu huấn luyện của bạn: tập dữ liệu của bạn càng chứa nhiều điểm dữ liệu đa dạng thì bạn càng có thể sử dụng một mô hình phức tạp hơn mà không bị quá khớp. Thông thường, việc thu thập nhiều điểm dữ liệu hơn sẽ mang lại nhiều sự đa dạng hơn, do đó, các tập dữ liệu lớn hơn cho phép xây dựng các mô hình phức tạp hơn. Tuy nhiên, việc chỉ sao chép cùng một điểm dữ liệu hoặc thu thập dữ liệu rất giống nhau sẽ không hữu ích.\n",
    "\n",
    "Quay lại ví dụ bán thuyền, nếu chúng ta thấy thêm 10.000 hàng dữ liệu khách hàng và tất cả chúng đều tuân thủ quy tắc \"Nếu khách hàng trên 45 tuổi và có ít hơn 3 người con hoặc chưa ly hôn thì họ muốn mua thuyền\", chúng ta sẽ có nhiều khả năng tin rằng đây là một quy tắc tốt hơn so với khi nó được phát triển chỉ bằng 12 hàng trong Bảng 2-1.\n",
    "\n",
    "Có nhiều dữ liệu hơn và xây dựng các mô hình phức tạp hơn một cách phù hợp thường có thể tạo ra những điều kỳ diệu cho các tác vụ học có giám sát. Trong cuốn sách này, chúng ta sẽ tập trung vào việc làm việc với các tập dữ liệu có kích thước cố định. Trong thế giới thực, bạn thường có khả năng quyết định thu thập bao nhiêu dữ liệu, điều này có thể có lợi hơn là tinh chỉnh và điều chỉnh mô hình của bạn. Đừng bao giờ đánh giá thấp sức mạnh của việc có nhiều dữ liệu hơn.\n",
    "\n",
    "### Các thuật toán học máy có giám sát\n",
    "\n",
    "Bây giờ chúng ta sẽ xem xét các thuật toán học máy phổ biến nhất và giải thích cách chúng học từ dữ liệu và cách chúng đưa ra dự đoán. Chúng ta cũng sẽ thảo luận về cách khái niệm độ phức tạp của mô hình thể hiện cho từng mô hình này và cung cấp một cái nhìn tổng quan về cách mỗi thuật toán xây dựng một mô hình. Chúng ta sẽ kiểm tra điểm mạnh và điểm yếu của từng thuật toán và loại dữ liệu nào chúng có thể được áp dụng tốt nhất. Chúng ta cũng sẽ giải thích ý nghĩa của các tham số và tùy chọn quan trọng nhất. Nhiều thuật toán có một biến thể phân loại và hồi quy, và chúng ta sẽ mô tả cả hai.\n",
    "\n",
    "Không cần thiết phải đọc chi tiết các mô tả của từng thuật toán, nhưng việc hiểu các mô hình sẽ giúp bạn có cảm nhận tốt hơn về các cách khác nhau mà các thuật toán học máy có thể hoạt động. Chương này cũng có thể được sử dụng như một hướng dẫn tham khảo và bạn có thể quay lại nó khi bạn không chắc chắn về hoạt động của bất kỳ thuật toán nào."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafed510",
   "metadata": {},
   "source": [
    "### Một số tập dữ liệu mẫu\n",
    "\n",
    "Chúng ta sẽ sử dụng một số tập dữ liệu để minh họa các thuật toán khác nhau. Một số tập dữ liệu sẽ nhỏ và tổng hợp (nghĩa là được tạo ra), được thiết kế để làm nổi bật các khía cạnh cụ thể của các thuật toán. Các tập dữ liệu khác sẽ là các ví dụ lớn, trong thế giới thực.\n",
    "\n",
    "Một ví dụ về tập dữ liệu phân loại hai lớp tổng hợp là tập dữ liệu forge, có hai đặc trưng. Đoạn mã sau đây tạo ra một biểu đồ phân tán (Hình 2-2) trực quan hóa tất cả các điểm dữ liệu trong tập dữ liệu này. Biểu đồ có đặc trưng đầu tiên trên trục x và đặc trưng thứ hai trên trục y. Như thường lệ trong biểu đồ phân tán, mỗi điểm dữ liệu được biểu diễn bằng một dấu chấm. Màu sắc và hình dạng của dấu chấm cho biết lớp của nó:\n",
    "\n",
    "--- \n",
    "⁴Thảo luận về tất cả chúng nằm ngoài phạm vi của cuốn sách này, và chúng tôi giới thiệu bạn đến tài liệu của scikit-learn để biết thêm chi tiết."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd45bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataset\n",
    "X, y = mglearn.datasets.make_forge()\n",
    "# plot dataset\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.legend([\"Class 0\", \"Class 1\"], loc=4)\n",
    "plt.xlabel(\"First feature\")\n",
    "plt.ylabel(\"Second feature\")\n",
    "print(\"X.shape: {}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef7231",
   "metadata": {},
   "source": [
    "**Hình 2-2. Biểu đồ phân tán của tập dữ liệu forge**\n",
    "\n",
    "Như bạn có thể thấy từ `X.shape`, tập dữ liệu này bao gồm 26 điểm dữ liệu, với 2 đặc trưng.\n",
    "Để minh họa các thuật toán hồi quy, chúng ta sẽ sử dụng tập dữ liệu `wave` tổng hợp. Tập dữ liệu `wave` có một đặc trưng đầu vào duy nhất và một biến mục tiêu liên tục (hoặc phản hồi) mà chúng ta muốn mô hình hóa. Biểu đồ được tạo ở đây (Hình 2-3) cho thấy đặc trưng duy nhất trên trục x và mục tiêu hồi quy (đầu ra) trên trục y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df4c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mglearn.datasets.make_wave(n_samples=40)\n",
    "plt.plot(X, y, 'o')\n",
    "plt.ylim(-3, 3)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4294c3",
   "metadata": {},
   "source": [
    "**Hình 2-3. Biểu đồ của tập dữ liệu wave, với trục x hiển thị đặc trưng và trục y hiển thị mục tiêu hồi quy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de252b",
   "metadata": {},
   "source": [
    "Chúng ta đang sử dụng các tập dữ liệu rất đơn giản, có chiều thấp này vì chúng ta có thể dễ dàng hình dung chúng - một trang in có hai chiều, vì vậy dữ liệu có nhiều hơn hai đặc trưng rất khó hiển thị. Bất kỳ trực giác nào có được từ các tập dữ liệu có ít đặc trưng (còn gọi là tập dữ liệu có chiều thấp) có thể không đúng trong các tập dữ liệu có nhiều đặc trưng (tập dữ liệu có chiều cao). Miễn là bạn ghi nhớ điều đó, việc kiểm tra các thuật toán trên các tập dữ liệu có chiều thấp có thể rất hữu ích.\n",
    "\n",
    "Chúng ta sẽ bổ sung các tập dữ liệu tổng hợp nhỏ này bằng hai tập dữ liệu trong thế giới thực được bao gồm trong scikit-learn. Một là tập dữ liệu Ung thư vú Wisconsin (gọi tắt là cancer), ghi lại các phép đo lâm sàng của các khối u ung thư vú. Mỗi khối u được dán nhãn là \"lành tính\" (đối với các khối u vô hại) hoặc \"ác tính\" (đối với các khối u ung thư), và nhiệm vụ là học cách dự đoán liệu một khối u có ác tính hay không dựa trên các phép đo của mô.\n",
    "\n",
    "Dữ liệu có thể được tải bằng cách sử dụng hàm `load_breast_cancer` từ scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91070ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "print(\"cancer.keys(): \\n{}\".format(cancer.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f13dd",
   "metadata": {},
   "source": [
    "> Các tập dữ liệu được bao gồm trong scikit-learn thường được lưu trữ dưới dạng đối tượng `Bunch`, chứa một số thông tin về tập dữ liệu cũng như dữ liệu thực tế. Tất cả những gì bạn cần biết về đối tượng `Bunch` là chúng hoạt động giống như từ điển, với lợi ích bổ sung là bạn có thể truy cập các giá trị bằng cách sử dụng dấu chấm (như trong `bunch.key` thay vì `bunch['key']`).\n",
    "\n",
    "Tập dữ liệu bao gồm 569 điểm dữ liệu, mỗi điểm có 30 đặc trưng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of cancer data: {}\".format(cancer.data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f74062",
   "metadata": {},
   "source": [
    "Trong số 569 điểm dữ liệu này, 212 được dán nhãn là ác tính và 357 là lành tính:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d4c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample counts per class:\\n{}\".format(\n",
    " {n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23de1833",
   "metadata": {},
   "source": [
    "Để có được mô tả về ý nghĩa ngữ nghĩa của từng đặc trưng, chúng ta có thể xem thuộc tính `feature_names`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature names:\\n{}\".format(cancer.feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02caeba2",
   "metadata": {},
   "source": [
    "Bạn có thể tìm hiểu thêm về dữ liệu bằng cách đọc `cancer.DESCR` nếu bạn quan tâm.\n",
    "\n",
    "Chúng ta cũng sẽ sử dụng một tập dữ liệu hồi quy trong thế giới thực, tập dữ liệu Nhà ở Boston. Nhiệm vụ liên quan đến tập dữ liệu này là dự đoán giá trị trung vị của các ngôi nhà ở một số khu phố ở Boston vào những năm 1970, sử dụng thông tin như tỷ lệ tội phạm, sự gần gũi với sông Charles, khả năng tiếp cận đường cao tốc, v.v. Tập dữ liệu chứa 506 điểm dữ liệu, được mô tả bởi 13 đặc trưng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dc0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(\"Data shape: {}\".format(boston.data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc0ac4b",
   "metadata": {},
   "source": [
    "Một lần nữa, bạn có thể nhận thêm thông tin về tập dữ liệu bằng cách đọc thuộc tính `DESCR` của `boston`. Đối với mục đích của chúng ta ở đây, chúng ta sẽ thực sự mở rộng tập dữ liệu này bằng cách không chỉ xem xét 13 phép đo này làm đặc trưng đầu vào mà còn xem xét tất cả các tích (còn gọi là tương tác) giữa các đặc trưng. Nói cách khác, chúng ta sẽ không chỉ xem xét tỷ lệ tội phạm và khả năng tiếp cận đường cao tốc làm đặc trưng mà còn cả tích của tỷ lệ tội phạm và khả năng tiếp cận đường cao tốc. Việc bao gồm các đặc trưng phái sinh như thế này được gọi là kỹ thuật đặc trưng, mà chúng ta sẽ thảo luận chi tiết hơn trong Chương 4. Tập dữ liệu phái sinh này có thể được tải bằng cách sử dụng hàm `load_extended_boston`:\n",
    "\n",
    "--- \n",
    "⁵Đây được gọi là hệ số nhị thức, là số cách kết hợp của k phần tử có thể được chọn từ một tập hợp n phần tử. Thường thì điều này được viết là $(\\frac{n}{k})$ và được nói là \"n chọn k\"—trong trường hợp này là \"13 chọn 2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952081b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mglearn.datasets.load_extended_boston()\n",
    "print(\"X.shape: {}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b86861",
   "metadata": {},
   "source": [
    "Kết quả 104 đặc trưng là 13 đặc trưng ban đầu cùng với 91 kết hợp có thể có của hai đặc trưng trong 13 đặc trưng đó.⁵\n",
    "\n",
    "Chúng ta sẽ sử dụng các tập dữ liệu này để giải thích và minh họa các thuộc tính của các thuật toán học máy khác nhau. Nhưng bây giờ, chúng ta hãy đến với chính các thuật toán. Đầu tiên, chúng ta sẽ xem lại thuật toán k-láng giềng gần nhất (k-NN) mà chúng ta đã thấy trong chương trước."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae246dc",
   "metadata": {},
   "source": [
    "### K-Láng giềng gần nhất\n",
    "\n",
    "Thuật toán k-NN được cho là thuật toán học máy đơn giản nhất. Việc xây dựng mô hình chỉ bao gồm việc lưu trữ tập dữ liệu huấn luyện. Để đưa ra dự đoán cho một điểm dữ liệu mới, thuật toán sẽ tìm các điểm dữ liệu gần nhất trong tập dữ liệu huấn luyện - \"những người hàng xóm gần nhất\" của nó.\n",
    "\n",
    "#### Phân loại K-Láng giềng\n",
    "\n",
    "Trong phiên bản đơn giản nhất của nó, thuật toán k-NN chỉ xem xét chính xác một láng giềng gần nhất, đó là điểm dữ liệu huấn luyện gần nhất với điểm mà chúng ta muốn đưa ra dự đoán. Dự đoán sau đó chỉ đơn giản là đầu ra đã biết cho điểm huấn luyện này. Hình 2-4 minh họa điều này cho trường hợp phân loại trên tập dữ liệu forge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e643937",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_knn_classification(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b25174",
   "metadata": {},
   "source": [
    "**Hình 2-4. Dự đoán được thực hiện bởi mô hình một láng giềng gần nhất trên tập dữ liệu forge**\n",
    "\n",
    "Ở đây, chúng ta đã thêm ba điểm dữ liệu mới, được hiển thị dưới dạng các ngôi sao. Đối với mỗi điểm, chúng ta đã đánh dấu điểm gần nhất trong tập huấn luyện. Dự đoán của thuật toán một láng giềng gần nhất là nhãn của điểm đó (được hiển thị bằng màu của dấu thập)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43971eb6",
   "metadata": {},
   "source": [
    "Thay vì chỉ xem xét láng giềng gần nhất, chúng ta cũng có thể xem xét một số lượng tùy ý, k, của các láng giềng. Đây là nơi xuất phát tên của thuật toán k-láng giềng gần nhất. Khi xem xét nhiều hơn một láng giềng, chúng ta sử dụng bỏ phiếu để gán một nhãn. Điều này có nghĩa là đối với mỗi điểm kiểm tra, chúng ta đếm xem có bao nhiêu láng giềng thuộc lớp 0 và bao nhiêu láng giềng thuộc lớp 1. Sau đó, chúng ta gán lớp thường xuyên hơn: nói cách khác, lớp đa số trong số k-láng giềng gần nhất. Ví dụ sau (Hình 2-5) sử dụng ba láng giềng gần nhất:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa4093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_knn_classification(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb030a9",
   "metadata": {},
   "source": [
    "**Hình 2-5. Dự đoán được thực hiện bởi mô hình ba láng giềng gần nhất trên tập dữ liệu forge**\n",
    "\n",
    "Một lần nữa, dự đoán được hiển thị dưới dạng màu của dấu thập. Bạn có thể thấy rằng dự đoán cho điểm dữ liệu mới ở phía trên bên trái không giống với dự đoán khi chúng ta chỉ sử dụng một láng giềng.\n",
    "\n",
    "Mặc dù minh họa này dành cho một bài toán phân loại nhị phân, phương pháp này có thể được áp dụng cho các tập dữ liệu với bất kỳ số lượng lớp nào. Đối với nhiều lớp hơn, chúng ta đếm xem có bao nhiêu láng giềng thuộc mỗi lớp và một lần nữa dự đoán lớp phổ biến nhất.\n",
    "\n",
    "Bây giờ hãy xem cách chúng ta có thể áp dụng thuật toán k-láng giềng gần nhất bằng scikit-learn. Đầu tiên, chúng ta chia dữ liệu của mình thành một tập huấn luyện và một tập kiểm tra để chúng ta có thể đánh giá hiệu suất tổng quát hóa, như đã thảo luận trong Chương 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275e3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = mglearn.datasets.make_forge()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc1d30",
   "metadata": {},
   "source": [
    "Tiếp theo, chúng ta nhập và khởi tạo lớp. Đây là lúc chúng ta có thể đặt các tham số, như số lượng láng giềng cần sử dụng. Ở đây, chúng ta đặt nó thành 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef15404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce0624a",
   "metadata": {},
   "source": [
    "Bây giờ, chúng ta điều chỉnh bộ phân loại bằng cách sử dụng tập huấn luyện. Đối với `KNeighborsClassifier`, điều này có nghĩa là lưu trữ tập dữ liệu, để chúng ta có thể tính toán các láng giềng trong quá trình dự đoán:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454d2e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c5e6e3",
   "metadata": {},
   "source": [
    "Để đưa ra dự đoán trên dữ liệu kiểm tra, chúng ta gọi phương thức `predict`. Đối với mỗi điểm dữ liệu trong tập kiểm tra, phương thức này tính toán các láng giềng gần nhất của nó trong tập huấn luyện và tìm ra lớp phổ biến nhất trong số đó:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac5ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set predictions: {}\".format(clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c0f14c",
   "metadata": {},
   "source": [
    "Để đánh giá mức độ tổng quát hóa của mô hình, chúng ta có thể gọi phương thức `score` với dữ liệu kiểm tra cùng với nhãn kiểm tra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d97911",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f14bd5",
   "metadata": {},
   "source": [
    "Chúng ta thấy rằng mô hình của mình có độ chính xác khoảng 86%, nghĩa là mô hình đã dự đoán đúng lớp cho 86% các mẫu trong tập dữ liệu kiểm tra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1dd589",
   "metadata": {},
   "source": [
    "#### Phân tích KNeighborsClassifier\n",
    "\n",
    "Đối với các tập dữ liệu hai chiều, chúng ta cũng có thể minh họa dự đoán cho tất cả các điểm kiểm tra có thể có trong mặt phẳng xy. Chúng ta tô màu mặt phẳng theo lớp sẽ được gán cho một điểm trong vùng này. Điều này cho phép chúng ta xem ranh giới quyết định, là ranh giới giữa nơi thuật toán gán lớp 0 và nơi nó gán lớp 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609500c0",
   "metadata": {},
   "source": [
    "Đoạn mã sau tạo ra các trực quan hóa của các ranh giới quyết định cho một, ba và chín láng giềng được hiển thị trong Hình 2-6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef3dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "for n_neighbors, ax in zip([1, 3, 9], axes):\n",
    " # phương thức fit trả về đối tượng self, vì vậy chúng ta có thể khởi tạo\n",
    " # và fit trong một dòng\n",
    " clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\n",
    " mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\n",
    " mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    " ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\n",
    " ax.set_xlabel(\"feature 0\")\n",
    " ax.set_ylabel(\"feature 1\")\n",
    "axes[0].legend(loc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ff1fc",
   "metadata": {},
   "source": [
    "**Hình 2-6. Ranh giới quyết định được tạo bởi mô hình láng giềng gần nhất cho các giá trị khác nhau của `n_neighbors`**\n",
    "\n",
    "Như bạn có thể thấy ở bên trái trong hình, việc sử dụng một láng giềng duy nhất dẫn đến một ranh giới quyết định theo sát dữ liệu huấn luyện. Việc xem xét ngày càng nhiều láng giềng dẫn đến một ranh giới quyết định mượt mà hơn. Một ranh giới mượt mà hơn tương ứng với một mô hình đơn giản hơn. Nói cách khác, việc sử dụng ít láng giềng tương ứng với độ phức tạp mô hình cao (như được hiển thị ở phía bên phải của Hình 2-1), và việc sử dụng nhiều láng giềng tương ứng với độ phức tạp mô hình thấp (như được hiển thị ở phía bên trái của Hình 2-1). Nếu bạn xem xét trường hợp cực đoan trong đó số lượng láng giềng là số lượng tất cả các điểm dữ liệu trong tập huấn luyện, mỗi điểm kiểm tra sẽ có chính xác cùng một láng giềng (tất cả các điểm huấn luyện) và tất cả các dự đoán sẽ giống nhau: lớp thường xuyên nhất trong tập huấn luyện.\n",
    "\n",
    "Hãy điều tra xem liệu chúng ta có thể xác nhận mối liên hệ giữa độ phức tạp của mô hình và sự tổng quát hóa mà chúng ta đã thảo luận trước đó hay không. Chúng ta sẽ làm điều này trên tập dữ liệu thực tế về Ung thư vú. Chúng ta bắt đầu bằng cách chia tập dữ liệu thành một tập huấn luyện và một tập kiểm tra. Sau đó, chúng ta đánh giá hiệu suất của tập huấn luyện và tập kiểm tra với các số lượng láng giềng khác nhau. Kết quả được hiển thị trong Hình 2-7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1de2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    " cancer.data, cancer.target, stratify=cancer.target, random_state=66)\n",
    "\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "# thử n_neighbors từ 1 đến 10\n",
    "neighbors_settings = range(1, 11)\n",
    "\n",
    "for n_neighbors in neighbors_settings:\n",
    " # xây dựng mô hình\n",
    " clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    " clf.fit(X_train, y_train)\n",
    " # ghi lại độ chính xác của tập huấn luyện\n",
    " training_accuracy.append(clf.score(X_train, y_train))\n",
    " # ghi lại độ chính xác tổng quát hóa\n",
    " test_accuracy.append(clf.score(X_test, y_test))\n",
    "\n",
    "plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ace667",
   "metadata": {},
   "source": [
    "**Hình 2-7. So sánh độ chính xác của tập huấn luyện và tập kiểm tra như một hàm của `n_neighbors`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b71491",
   "metadata": {},
   "source": [
    "Biểu đồ cho thấy độ chính xác của tập huấn luyện và tập kiểm tra trên trục y so với cài đặt của `n_neighbors` trên trục x. Mặc dù các biểu đồ trong thế giới thực hiếm khi rất mượt mà, chúng ta vẫn có thể nhận ra một số đặc điểm của quá khớp và thiếu khớp (lưu ý rằng vì việc xem xét ít láng giềng hơn tương ứng với một mô hình phức tạp hơn, biểu đồ bị lật theo chiều ngang so với hình minh họa trong Hình 2-1). Xem xét một láng giềng gần nhất duy nhất, dự đoán trên tập huấn luyện là hoàn hảo. Nhưng khi xem xét nhiều láng giềng hơn, mô hình trở nên đơn giản hơn và độ chính xác của tập huấn luyện giảm xuống. Độ chính xác của tập kiểm tra khi sử dụng một láng giềng duy nhất thấp hơn khi sử dụng nhiều láng giềng hơn, cho thấy rằng việc sử dụng một láng giềng gần nhất duy nhất dẫn đến một mô hình quá phức tạp. Mặt khác, khi xem xét 10 láng giềng, mô hình quá đơn giản và hiệu suất thậm chí còn tệ hơn. Hiệu suất tốt nhất nằm ở đâu đó ở giữa, sử dụng khoảng sáu láng giềng. Tuy nhiên, điều tốt là cần ghi nhớ thang đo của biểu đồ. Hiệu suất kém nhất là khoảng 88% độ chính xác, vẫn có thể chấp nhận được."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39518d3",
   "metadata": {},
   "source": [
    "#### Hồi quy k-láng giềng\n",
    "\n",
    "Cũng có một biến thể hồi quy của thuật toán k-láng giềng gần nhất. Một lần nữa, hãy bắt đầu bằng cách sử dụng một láng giềng gần nhất duy nhất, lần này sử dụng tập dữ liệu wave. Chúng ta đã thêm ba điểm dữ liệu kiểm tra dưới dạng các ngôi sao màu xanh lá cây trên trục x. Dự đoán sử dụng một láng giềng duy nhất chỉ là giá trị mục tiêu của láng giềng gần nhất. Những điều này được hiển thị dưới dạng các ngôi sao màu xanh lam trong Hình 2-8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f00d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_knn_regression(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f3221c",
   "metadata": {},
   "source": [
    "**Hình 2-8. Dự đoán được thực hiện bởi hồi quy một láng giềng gần nhất trên tập dữ liệu wave**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea054eb",
   "metadata": {},
   "source": [
    "Một lần nữa, chúng ta có thể sử dụng nhiều hơn một láng giềng gần nhất cho hồi quy. Khi sử dụng nhiều láng giềng gần nhất, dự đoán là giá trị trung bình của các láng giềng có liên quan (Hình 2-9):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56258d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_knn_regression(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9528f1c4",
   "metadata": {},
   "source": [
    "**Hình 2-9. Dự đoán được thực hiện bởi hồi quy ba láng giềng gần nhất trên tập dữ liệu wave**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1afa5d",
   "metadata": {},
   "source": [
    "Thuật toán k-láng giềng gần nhất cho hồi quy được triển khai trong lớp `KNeighborsRegressor` trong scikit-learn. Nó được sử dụng tương tự như `KNeighborsClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f13d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "X, y = mglearn.datasets.make_wave(n_samples=40)\n",
    "\n",
    "# chia tập dữ liệu wave thành một tập huấn luyện và một tập kiểm tra\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# khởi tạo mô hình và đặt số lượng láng giềng cần xem xét thành 3\n",
    "reg = KNeighborsRegressor(n_neighbors=3)\n",
    "# điều chỉnh mô hình bằng cách sử dụng dữ liệu huấn luyện và mục tiêu huấn luyện\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e6ae1",
   "metadata": {},
   "source": [
    "Bây giờ chúng ta có thể đưa ra dự đoán trên tập kiểm tra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0bbfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set predictions:\\n{}\".format(reg.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2fcf53",
   "metadata": {},
   "source": [
    "Chúng ta cũng có thể đánh giá mô hình bằng cách sử dụng phương thức `score`, phương thức này đối với các bộ hồi quy sẽ trả về điểm R². Điểm R², còn được gọi là hệ số xác định, là một thước đo về mức độ tốt của một dự đoán đối với một mô hình hồi quy và mang lại một điểm từ 0 đến 1. Giá trị 1 tương ứng với một dự đoán hoàn hảo và giá trị 0 tương ứng với một mô hình hằng số chỉ dự đoán giá trị trung bình của các phản hồi của tập huấn luyện, `y_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set R^2: {:.2f}\".format(reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4239f5",
   "metadata": {},
   "source": [
    "Ở đây, điểm số là 0,83, cho thấy một mô hình khá tốt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9fab98",
   "metadata": {},
   "source": [
    "#### Phân tích KNeighborsRegressor\n",
    "\n",
    "Đối với tập dữ liệu một chiều của chúng ta, chúng ta có thể thấy các dự đoán trông như thế nào đối với tất cả các giá trị đặc trưng có thể có (Hình 2-10). Để làm điều này, chúng ta tạo một tập dữ liệu kiểm tra bao gồm nhiều điểm trên đường thẳng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "# tạo 1.000 điểm dữ liệu, cách đều nhau từ -3 đến 3\n",
    "line = np.linspace(-3, 3, 1000).reshape(-1, 1)\n",
    "for n_neighbors, ax in zip([1, 3, 9], axes):\n",
    " # đưa ra dự đoán bằng cách sử dụng 1, 3 hoặc 9 láng giềng\n",
    " reg = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    " reg.fit(X_train, y_train)\n",
    " ax.plot(line, reg.predict(line))\n",
    " ax.plot(X_train, y_train, '^', c=mglearn.cm2(0), markersize=8)\n",
    " ax.plot(X_test, y_test, 'v', c=mglearn.cm2(1), markersize=8)\n",
    "\n",
    " ax.set_title(\n",
    " \"{} neighbor(s)\\n train score: {:.2f} test score: {:.2f}\".format(\n",
    " n_neighbors, reg.score(X_train, y_train),\n",
    " reg.score(X_test, y_test)))\n",
    " ax.set_xlabel(\"Feature\")\n",
    " ax.set_ylabel(\"Target\")\n",
    "axes[0].legend([\"Model predictions\", \"Training data/target\",\n",
    " \"Test data/target\"], loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d87eca",
   "metadata": {},
   "source": [
    "**Hình 2-10. So sánh các dự đoán được thực hiện bởi hồi quy láng giềng gần nhất cho các giá trị khác nhau của `n_neighbors`**\n",
    "\n",
    "Như chúng ta có thể thấy từ biểu đồ, chỉ sử dụng một láng giềng duy nhất, mỗi điểm trong tập huấn luyện có ảnh hưởng rõ ràng đến các dự đoán và các giá trị dự đoán đi qua tất cả các điểm dữ liệu. Điều này dẫn đến một dự đoán rất không ổn định. Việc xem xét nhiều láng giềng hơn sẽ dẫn đến các dự đoán mượt mà hơn, nhưng những dự đoán này không phù hợp với dữ liệu huấn luyện bằng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe181a9",
   "metadata": {},
   "source": [
    "#### Điểm mạnh, điểm yếu và tham số\n",
    "\n",
    "Về nguyên tắc, có hai tham số quan trọng đối với bộ phân loại `KNeighbors`: số lượng láng giềng và cách bạn đo khoảng cách giữa các điểm dữ liệu. Trong thực tế, việc sử dụng một số lượng nhỏ láng giềng như ba hoặc năm thường hoạt động tốt, nhưng bạn chắc chắn nên điều chỉnh tham số này. Việc chọn đúng thước đo khoảng cách hơi nằm ngoài phạm vi của cuốn sách này. Theo mặc định, khoảng cách Euclide được sử dụng, hoạt động tốt trong nhiều cài đặt.\n",
    "\n",
    "Một trong những điểm mạnh của k-NN là mô hình rất dễ hiểu và thường mang lại hiệu suất hợp lý mà không cần nhiều điều chỉnh. Việc sử dụng thuật toán này là một phương pháp cơ bản tốt để thử trước khi xem xét các kỹ thuật nâng cao hơn. Việc xây dựng mô hình láng giềng gần nhất thường rất nhanh, nhưng khi tập huấn luyện của bạn rất lớn (về số lượng đặc trưng hoặc số lượng mẫu) thì việc dự đoán có thể chậm. Khi sử dụng thuật toán k-NN, điều quan trọng là phải tiền xử lý dữ liệu của bạn (xem Chương 3). Cách tiếp cận này thường không hoạt động tốt trên các tập dữ liệu có nhiều đặc trưng (hàng trăm hoặc nhiều hơn) và nó hoạt động đặc biệt kém với các tập dữ liệu trong đó hầu hết các đặc trưng là 0 trong hầu hết thời gian (được gọi là tập dữ liệu thưa thớt).\n",
    "\n",
    "Vì vậy, mặc dù thuật toán k-láng giềng gần nhất dễ hiểu, nhưng nó không thường được sử dụng trong thực tế, do việc dự đoán chậm và không có khả năng xử lý nhiều đặc trưng. Phương pháp chúng ta sẽ thảo luận tiếp theo không có nhược điểm nào trong số này."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10da5e9",
   "metadata": {},
   "source": [
    "### Mô hình tuyến tính\n",
    "\n",
    "Mô hình tuyến tính là một lớp các mô hình được sử dụng rộng rãi trong thực tế và đã được nghiên cứu rộng rãi trong vài thập kỷ qua, với nguồn gốc có từ hơn một trăm năm trước. Mô hình tuyến tính đưa ra dự đoán bằng cách sử dụng một hàm tuyến tính của các đặc trưng đầu vào, mà chúng ta sẽ giải thích ngay sau đây.\n",
    "\n",
    "#### Mô hình tuyến tính cho hồi quy\n",
    "\n",
    "Đối với hồi quy, công thức dự đoán chung cho một mô hình tuyến tính trông như sau:\n",
    "$\\hat{y} = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b$\n",
    "\n",
    "Ở đây, $x[0]$ đến $x[p]$ biểu thị các đặc trưng (trong ví dụ này, số lượng đặc trưng là p) của một điểm dữ liệu duy nhất, w và b là các tham số của mô hình được học, và ŷ là dự đoán mà mô hình đưa ra. Đối với một tập dữ liệu có một đặc trưng duy nhất, đây là:\n",
    "\n",
    "$\\hat{y} = w[0] * x[0] + b$\n",
    "\n",
    "mà bạn có thể nhớ từ toán học trung học là phương trình của một đường thẳng. Ở đây, $w[0]$ là độ dốc và b là độ lệch trục y. Đối với nhiều đặc trưng hơn, w chứa các độ dốc dọc theo mỗi trục đặc trưng. Ngoài ra, bạn có thể nghĩ về phản hồi dự đoán là một tổng có trọng số của các đặc trưng đầu vào, với các trọng số (có thể là âm) được cho bởi các mục của w.\n",
    "\n",
    "Việc cố gắng học các tham số $w[0]$ và b trên tập dữ liệu wave một chiều của chúng ta có thể dẫn đến đường thẳng sau (xem Hình 2-11):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccfd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_linear_regression_wave()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d05a72",
   "metadata": {},
   "source": [
    "**Hình 2-11. Dự đoán của một mô hình tuyến tính trên tập dữ liệu wave**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9b0643",
   "metadata": {},
   "source": [
    "Chúng ta đã thêm một lưới tọa độ vào biểu đồ để dễ hiểu hơn về đường thẳng. Nhìn vào $w[0]$, chúng ta thấy rằng độ dốc phải vào khoảng 0,4, điều mà chúng ta có thể xác nhận một cách trực quan trong biểu đồ. Điểm cắt là nơi đường dự đoán cắt trục y: nó hơi thấp hơn 0, điều mà bạn cũng có thể xác nhận trong hình ảnh.\n",
    "\n",
    "Các mô hình tuyến tính cho hồi quy có thể được mô tả là các mô hình hồi quy trong đó dự đoán là một đường thẳng đối với một đặc trưng duy nhất, một mặt phẳng khi sử dụng hai đặc trưng, hoặc một siêu phẳng trong các chiều cao hơn (nghĩa là, khi sử dụng nhiều đặc trưng hơn).\n",
    "\n",
    "Nếu bạn so sánh các dự đoán được thực hiện bởi đường thẳng với các dự đoán được thực hiện bởi `KNeighborsRegressor` trong Hình 2-10, việc sử dụng một đường thẳng để đưa ra dự đoán có vẻ rất hạn chế. Có vẻ như tất cả các chi tiết nhỏ của dữ liệu đều bị mất. Theo một nghĩa nào đó, điều này là đúng. Đó là một giả định mạnh mẽ (và hơi phi thực tế) rằng mục tiêu y của chúng ta là một tổ hợp tuyến tính của các đặc trưng. Nhưng việc xem xét dữ liệu một chiều mang lại một góc nhìn hơi sai lệch. Đối với các tập dữ liệu có nhiều đặc trưng, các mô hình tuyến tính có thể rất mạnh mẽ. Đặc biệt, nếu bạn có nhiều đặc trưng hơn số điểm dữ liệu huấn luyện, bất kỳ mục tiêu y nào cũng có thể được mô hình hóa một cách hoàn hảo (trên tập huấn luyện) dưới dạng một hàm tuyến tính.\n",
    "\n",
    "Có nhiều mô hình tuyến tính khác nhau cho hồi quy. Sự khác biệt giữa các mô hình này nằm ở cách các tham số mô hình w và b được học từ dữ liệu huấn luyện và cách độ phức tạp của mô hình có thể được kiểm soát. Bây giờ chúng ta sẽ xem xét các mô hình tuyến tính phổ biến nhất cho hồi quy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209dcdb0",
   "metadata": {},
   "source": [
    "#### Hồi quy tuyến tính (hay còn gọi là bình phương tối thiểu thông thường)\n",
    "\n",
    "Hồi quy tuyến tính, hay bình phương tối thiểu thông thường (OLS), là phương pháp tuyến tính đơn giản và cổ điển nhất cho hồi quy. Hồi quy tuyến tính tìm các tham số w và b để giảm thiểu sai số bình phương trung bình giữa các dự đoán và các mục tiêu hồi quy thực, y, trên tập huấn luyện. Sai số bình phương trung bình là tổng của các chênh lệch bình phương giữa các dự đoán và các giá trị thực. Hồi quy tuyến tính không có tham số, đây là một lợi ích, nhưng nó cũng không có cách nào để kiểm soát độ phức tạp của mô hình.\n",
    "\n",
    "Đây là đoạn mã tạo ra mô hình bạn có thể thấy trong Hình 2-11:\n",
    "\n",
    "--- \n",
    "⁶Điều này dễ thấy nếu bạn biết một chút về đại số tuyến tính."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X, y = mglearn.datasets.make_wave(n_samples=60)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66e8f7b",
   "metadata": {},
   "source": [
    "Các tham số \"độ dốc\" (w), còn được gọi là trọng số hoặc hệ số, được lưu trữ trong thuộc tính `coef_`, trong khi độ lệch hoặc điểm cắt (b) được lưu trữ trong thuộc tính `intercept_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12940e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lr.coef_: {}\".format(lr.coef_))\n",
    "print(\"lr.intercept_: {}\".format(lr.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ea2cf",
   "metadata": {},
   "source": [
    "> Bạn có thể nhận thấy dấu gạch dưới trông lạ ở cuối `coef` và `intercept`. Scikit-learn luôn lưu trữ bất cứ thứ gì có nguồn gốc từ dữ liệu huấn luyện trong các thuộc tính kết thúc bằng dấu gạch dưới. Điều đó là để tách chúng khỏi các tham số được người dùng đặt.\n",
    "\n",
    "Thuộc tính `intercept_` luôn là một số thực duy nhất, trong khi thuộc tính `coef_` là một mảng NumPy với một mục cho mỗi đặc trưng đầu vào. Vì chúng ta chỉ có một đặc trưng đầu vào duy nhất trong tập dữ liệu wave, `lr.coef_` chỉ có một mục duy nhất.\n",
    "\n",
    "Hãy xem hiệu suất của tập huấn luyện và tập kiểm tra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12902835",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1f474f",
   "metadata": {},
   "source": [
    "Điểm R² khoảng 0,66 không tốt lắm, nhưng chúng ta có thể thấy rằng các điểm số trên tập huấn luyện và tập kiểm tra rất gần nhau. Điều này có nghĩa là chúng ta có khả năng bị thiếu khớp, chứ không phải quá khớp. Đối với tập dữ liệu một chiều này, có rất ít nguy cơ bị quá khớp, vì mô hình rất đơn giản (hoặc bị hạn chế). Tuy nhiên, với các tập dữ liệu có chiều cao hơn (nghĩa là các tập dữ liệu có số lượng lớn đặc trưng), các mô hình tuyến tính trở nên mạnh mẽ hơn và có nhiều khả năng bị quá khớp hơn. Hãy xem cách `LinearRegression` hoạt động trên một tập dữ liệu phức tạp hơn, như tập dữ liệu Nhà ở Boston. Hãy nhớ rằng tập dữ liệu này có 506 mẫu và 105 đặc trưng phái sinh. Đầu tiên, chúng ta tải tập dữ liệu và chia nó thành một tập huấn luyện và một tập kiểm tra. Sau đó, chúng ta xây dựng mô hình hồi quy tuyến tính như trước:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7c0ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mglearn.datasets.load_extended_boston()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "lr = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2984cb",
   "metadata": {},
   "source": [
    "Khi so sánh điểm số của tập huấn luyện và tập kiểm tra, chúng ta thấy rằng chúng ta dự đoán rất chính xác trên tập huấn luyện, nhưng điểm R² trên tập kiểm tra kém hơn nhiều:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f0c5f2",
   "metadata": {},
   "source": [
    "Sự khác biệt giữa hiệu suất trên tập huấn luyện và tập kiểm tra này là một dấu hiệu rõ ràng của việc quá khớp, và do đó chúng ta nên cố gắng tìm một mô hình cho phép chúng ta kiểm soát độ phức tạp. Một trong những lựa chọn thay thế được sử dụng phổ biến nhất cho hồi quy tuyến tính tiêu chuẩn là hồi quy Ridge, mà chúng ta sẽ xem xét tiếp theo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0462f87b",
   "metadata": {},
   "source": [
    "#### Hồi quy Ridge\n",
    "\n",
    "Hồi quy Ridge cũng là một mô hình tuyến tính cho hồi quy, vì vậy công thức nó sử dụng để đưa ra dự đoán cũng giống như công thức được sử dụng cho bình phương tối thiểu thông thường. Tuy nhiên, trong hồi quy Ridge, các hệ số (w) được chọn không chỉ để chúng dự đoán tốt trên dữ liệu huấn luyện mà còn để phù hợp với một ràng buộc bổ sung. Chúng ta cũng muốn độ lớn của các hệ số càng nhỏ càng tốt; nói cách khác, tất cả các mục của w phải gần bằng không. Một cách trực quan, điều này có nghĩa là mỗi đặc trưng nên có càng ít ảnh hưởng đến kết quả càng tốt (dẫn đến có một độ dốc nhỏ), trong khi vẫn dự đoán tốt. Ràng buộc này là một ví dụ về cái được gọi là chính quy hóa (regularization). Chính quy hóa có nghĩa là hạn chế một cách rõ ràng một mô hình để tránh quá khớp. Loại cụ thể được sử dụng bởi hồi quy Ridge được gọi là chính quy hóa L2.⁷\n",
    "\n",
    "Hồi quy Ridge được triển khai trong `linear_model.Ridge`. Hãy xem nó hoạt động tốt như thế nào trên tập dữ liệu Nhà ở Boston mở rộng:\n",
    "\n",
    "--- \n",
    "⁷Về mặt toán học, Ridge phạt chuẩn L2 của các hệ số, hoặc độ dài Euclide của w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c7b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffee39c1",
   "metadata": {},
   "source": [
    "Như bạn có thể thấy, điểm số của tập huấn luyện của `Ridge` thấp hơn so với `LinearRegression`, trong khi điểm số của tập kiểm tra cao hơn. Điều này phù hợp với kỳ vọng của chúng ta. Với hồi quy tuyến tính, chúng ta đã quá khớp dữ liệu của mình. `Ridge` là một mô hình bị hạn chế hơn, vì vậy chúng ta ít có khả năng bị quá khớp hơn. Một mô hình ít phức tạp hơn có nghĩa là hiệu suất kém hơn trên tập huấn luyện, nhưng tổng quát hóa tốt hơn. Vì chúng ta chỉ quan tâm đến hiệu suất tổng quát hóa, chúng ta nên chọn mô hình `Ridge` thay vì mô hình `LinearRegression`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb7ee6f",
   "metadata": {},
   "source": [
    "Mô hình `Ridge` tạo ra một sự đánh đổi giữa sự đơn giản của mô hình (các hệ số gần bằng không) và hiệu suất của nó trên tập huấn luyện. Mức độ quan trọng mà mô hình đặt vào sự đơn giản so với hiệu suất của tập huấn luyện có thể được người dùng chỉ định bằng cách sử dụng tham số `alpha`. Trong ví dụ trước, chúng ta đã sử dụng tham số mặc định `alpha=1.0`. Tuy nhiên, không có lý do gì điều này sẽ mang lại cho chúng ta sự đánh đổi tốt nhất. Cài đặt tối ưu của `alpha` phụ thuộc vào tập dữ liệu cụ thể mà chúng ta đang sử dụng. Việc tăng `alpha` buộc các hệ số phải di chuyển nhiều hơn về phía không, điều này làm giảm hiệu suất của tập huấn luyện nhưng có thể giúp tổng quát hóa. Ví dụ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad35242",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a860c6",
   "metadata": {},
   "source": [
    "Việc giảm `alpha` cho phép các hệ số ít bị hạn chế hơn, nghĩa là chúng ta di chuyển sang phải trong Hình 2-1. Đối với các giá trị rất nhỏ của `alpha`, các hệ số hầu như không bị hạn chế chút nào, và chúng ta kết thúc với một mô hình giống với Hồi quy tuyến tính:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7cc947",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af047ce",
   "metadata": {},
   "source": [
    "Ở đây, `alpha=0.1` có vẻ hoạt động tốt. Chúng ta có thể thử giảm `alpha` hơn nữa để cải thiện khả năng tổng quát hóa. Hiện tại, hãy lưu ý cách tham số `alpha` tương ứng với độ phức tạp của mô hình như được hiển thị trong Hình 2-1. Chúng ta sẽ thảo luận các phương pháp để chọn đúng tham số trong Chương 5.\n",
    "\n",
    "Chúng ta cũng có thể có được một cái nhìn sâu sắc hơn về chất lượng về cách tham số `alpha` thay đổi mô hình bằng cách kiểm tra thuộc tính `coef_` của các mô hình với các giá trị `alpha` khác nhau. Một `alpha` cao hơn có nghĩa là một mô hình bị hạn chế hơn, vì vậy chúng ta mong đợi các mục của `coef_` có độ lớn nhỏ hơn đối với một giá trị `alpha` cao hơn so với một giá trị `alpha` thấp. Điều này được xác nhận trong biểu đồ trong Hình 2-12:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe90cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ridge.coef_, 's', label=\"Ridge alpha=1\")\n",
    "plt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\")\n",
    "plt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.1\")\n",
    "\n",
    "plt.plot(lr.coef_, 'o', label=\"LinearRegression\")\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.hlines(0, 0, len(lr.coef_))\n",
    "plt.ylim(-25, 25)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac3ae93",
   "metadata": {},
   "source": [
    "**Hình 2-12. So sánh độ lớn của hệ số đối với hồi quy Ridge với các giá trị alpha khác nhau và hồi quy tuyến tính**\n",
    "\n",
    "Ở đây, trục x liệt kê các mục của `coef_`: $x=0$ cho thấy hệ số được liên kết với đặc trưng đầu tiên, $x=1$ là hệ số được liên kết với đặc trưng thứ hai, và cứ thế cho đến $x=100$. Trục y cho thấy các giá trị số của các giá trị tương ứng của các hệ số. Điểm chính ở đây là đối với `alpha=10`, các hệ số chủ yếu nằm trong khoảng từ -3 đến 3. Các hệ số cho mô hình Ridge với `alpha=1` lớn hơn một chút. Các dấu chấm tương ứng với `alpha=0.1` có độ lớn vẫn lớn hơn, và nhiều dấu chấm tương ứng với hồi quy tuyến tính không có bất kỳ sự chính quy hóa nào (sẽ là `alpha=0`) lớn đến mức chúng nằm ngoài biểu đồ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199dd8dc",
   "metadata": {},
   "source": [
    "Một cách khác để hiểu ảnh hưởng của chính quy hóa là cố định một giá trị của `alpha` nhưng thay đổi lượng dữ liệu huấn luyện có sẵn. Đối với Hình 2-13, chúng ta đã lấy mẫu con tập dữ liệu Nhà ở Boston và đánh giá `LinearRegression` và `Ridge(alpha=1)` trên các tập hợp con có kích thước tăng dần (các biểu đồ cho thấy hiệu suất của mô hình như một hàm của kích thước tập dữ liệu được gọi là đường cong học tập):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_ridge_n_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a692d4",
   "metadata": {},
   "source": [
    "**Hình 2-13. Đường cong học tập cho hồi quy Ridge và hồi quy tuyến tính trên tập dữ liệu Nhà ở Boston**\n",
    "\n",
    "Như người ta mong đợi, điểm số huấn luyện cao hơn điểm số kiểm tra đối với tất cả các kích thước tập dữ liệu, đối với cả hồi quy Ridge và hồi quy tuyến tính. Bởi vì Ridge được chính quy hóa, điểm số huấn luyện của Ridge thấp hơn điểm số huấn luyện của hồi quy tuyến tính trên mọi phương diện. Tuy nhiên, điểm số kiểm tra của Ridge tốt hơn, đặc biệt là đối với các tập hợp con nhỏ của dữ liệu. Đối với ít hơn 400 điểm dữ liệu, hồi quy tuyến tính không thể học được bất cứ điều gì. Khi ngày càng có nhiều dữ liệu có sẵn cho mô hình, cả hai mô hình đều cải thiện, và hồi quy tuyến tính cuối cùng cũng bắt kịp Ridge. Bài học ở đây là với đủ dữ liệu huấn luyện, chính quy hóa trở nên ít quan trọng hơn, và với đủ dữ liệu, Ridge và hồi quy tuyến tính sẽ có cùng hiệu suất (việc điều này xảy ra ở đây khi sử dụng toàn bộ tập dữ liệu chỉ là tình cờ). Một khía cạnh thú vị khác của Hình 2-13 là sự sụt giảm hiệu suất huấn luyện của hồi quy tuyến tính. Nếu thêm nhiều dữ liệu hơn, một mô hình sẽ khó bị quá khớp hoặc ghi nhớ dữ liệu hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220907ca",
   "metadata": {},
   "source": [
    "#### Lasso\n",
    "\n",
    "Một phương pháp thay thế cho Ridge để chính quy hóa hồi quy tuyến tính là Lasso. Cũng như hồi quy Ridge, việc sử dụng Lasso cũng hạn chế các hệ số gần bằng không, nhưng theo một cách hơi khác, được gọi là chính quy hóa L1.⁸ Hậu quả của chính quy hóa L1 là khi sử dụng Lasso, một số hệ số bằng không. Điều này có nghĩa là một số đặc trưng hoàn toàn bị mô hình bỏ qua. Điều này có thể được xem như một dạng lựa chọn đặc trưng tự động. Việc có một số hệ số bằng không thường làm cho một mô hình dễ giải thích hơn và có thể tiết lộ các đặc trưng quan trọng nhất của mô hình của bạn.\n",
    "\n",
    "Hãy áp dụng Lasso cho tập dữ liệu Nhà ở Boston mở rộng:\n",
    "\n",
    "--- \n",
    "⁸Lasso phạt chuẩn L1 của véc tơ hệ số—hay nói cách khác, tổng các giá trị tuyệt đối của các hệ số."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b95d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11dd3a",
   "metadata": {},
   "source": [
    "Như bạn có thể thấy, Lasso hoạt động khá tệ, cả trên tập huấn luyện và tập kiểm tra. Điều này cho thấy rằng chúng ta đang bị thiếu khớp, và chúng ta thấy rằng nó chỉ sử dụng 4 trong số 105 đặc trưng. Tương tự như Ridge, Lasso cũng có một tham số chính quy hóa, `alpha`, kiểm soát mức độ mạnh mẽ của việc đẩy các hệ số về không. Trong ví dụ trước, chúng ta đã sử dụng giá trị mặc định là `alpha=1.0`. Để giảm thiểu tình trạng thiếu khớp, hãy thử giảm `alpha`. Khi chúng ta làm điều này, chúng ta cũng cần tăng cài đặt mặc định của `max_iter` (số lần lặp tối đa để chạy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6967e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chúng ta tăng cài đặt mặc định của \"max_iter\",\n",
    "# nếu không mô hình sẽ cảnh báo chúng ta rằng chúng ta nên tăng max_iter.\n",
    "lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd31b5c",
   "metadata": {},
   "source": [
    "Một `alpha` thấp hơn cho phép chúng ta phù hợp với một mô hình phức tạp hơn, hoạt động tốt hơn trên dữ liệu huấn luyện và kiểm tra. Hiệu suất tốt hơn một chút so với việc sử dụng Ridge và chúng ta chỉ sử dụng 33 trong số 105 đặc trưng. Điều này làm cho mô hình này có khả năng dễ hiểu hơn.\n",
    "\n",
    "Tuy nhiên, nếu chúng ta đặt `alpha` quá thấp, chúng ta lại loại bỏ tác dụng của chính quy hóa và cuối cùng bị quá khớp, với kết quả tương tự như `LinearRegression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f8c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d733b9",
   "metadata": {},
   "source": [
    "Một lần nữa, chúng ta có thể vẽ biểu đồ các hệ số của các mô hình khác nhau, tương tự như Hình 2-12. Kết quả được hiển thị trong Hình 2-14:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9a778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lasso.coef_, 's', label=\"Lasso alpha=1\")\n",
    "plt.plot(lasso001.coef_, '^', label=\"Lasso alpha=0.01\")\n",
    "plt.plot(lasso00001.coef_, 'v', label=\"Lasso alpha=0.0001\")\n",
    "\n",
    "plt.plot(ridge01.coef_, 'o', label=\"Ridge alpha=0.1\")\n",
    "plt.legend(ncol=2, loc=(0, 1.05))\n",
    "plt.ylim(-25, 25)\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c528f",
   "metadata": {},
   "source": [
    "**Hình 2-14. So sánh độ lớn của hệ số đối với hồi quy Lasso với các giá trị alpha khác nhau và hồi quy Ridge**\n",
    "\n",
    "Đối với `alpha=1`, chúng ta không chỉ thấy rằng hầu hết các hệ số đều bằng không (điều mà chúng ta đã biết), mà các hệ số còn lại cũng có độ lớn nhỏ. Giảm `alpha` xuống 0,01, chúng ta thu được giải pháp được hiển thị dưới dạng các dấu chấm màu xanh lá cây, điều này làm cho hầu hết các đặc trưng bằng không. Sử dụng `alpha=0.0001`, chúng ta nhận được một mô hình khá không được chính quy hóa, với hầu hết các hệ số khác không và có độ lớn lớn. Để so sánh, giải pháp Ridge tốt nhất được hiển thị bằng màu xanh mòng két. Mô hình Ridge với `alpha=0.1` có hiệu suất dự đoán tương tự như mô hình Lasso với `alpha=0.01`, nhưng khi sử dụng Ridge, tất cả các hệ số đều khác không.\n",
    "\n",
    "Trong thực tế, hồi quy Ridge thường là lựa chọn đầu tiên giữa hai mô hình này. Tuy nhiên, nếu bạn có một lượng lớn đặc trưng và chỉ mong đợi một vài trong số chúng là quan trọng, Lasso có thể là một lựa chọn tốt hơn. Tương tự, nếu bạn muốn có một mô hình dễ giải thích, Lasso sẽ cung cấp một mô hình dễ hiểu hơn, vì nó sẽ chỉ chọn một tập hợp con của các đặc trưng đầu vào. Scikit-learn cũng cung cấp lớp `ElasticNet`, kết hợp các hình phạt của Lasso và Ridge. Trong thực tế, sự kết hợp này hoạt động tốt nhất, mặc dù với cái giá là phải điều chỉnh hai tham số: một cho chính quy hóa L1 và một cho chính quy hóa L2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b31923e",
   "metadata": {},
   "source": [
    "#### Mô hình tuyến tính cho phân loại\n",
    "\n",
    "Mô hình tuyến tính cũng được sử dụng rộng rãi cho phân loại. Trước tiên, hãy xem xét phân loại nhị phân. Trong trường hợp này, một dự đoán được thực hiện bằng công thức sau:\n",
    "\n",
    "$\\hat{y} = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0$\n",
    "\n",
    "Công thức trông rất giống với công thức cho hồi quy tuyến tính, nhưng thay vì chỉ trả về tổng có trọng số của các đặc trưng, chúng ta đặt ngưỡng cho giá trị dự đoán tại 0. Nếu hàm nhỏ hơn 0, chúng ta dự đoán lớp -1; nếu nó lớn hơn 0, chúng ta dự đoán lớp +1. Quy tắc dự đoán này phổ biến cho tất cả các mô hình tuyến tính cho phân loại. Một lần nữa, có nhiều cách khác nhau để tìm các hệ số (w) và điểm cắt (b).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cb276e",
   "metadata": {},
   "source": [
    "Đối với các mô hình tuyến tính cho hồi quy, đầu ra, ŷ, là một hàm tuyến tính của các đặc trưng: một đường thẳng, một mặt phẳng, hoặc một siêu phẳng (trong các chiều cao hơn). Đối với các mô hình tuyến tính cho phân loại, ranh giới quyết định là một hàm tuyến tính của đầu vào. Nói cách khác, một bộ phân loại tuyến tính (nhị phân) là một bộ phân loại tách hai lớp bằng một đường thẳng, một mặt phẳng, hoặc một siêu phẳng. Chúng ta sẽ thấy các ví dụ về điều đó trong phần này.\n",
    "\n",
    "Có nhiều thuật toán để học các mô hình tuyến tính. Tất cả các thuật toán này đều khác nhau theo hai cách sau:\n",
    "\n",
    "* Cách chúng đo lường mức độ phù hợp của một tổ hợp cụ thể của các hệ số và điểm cắt với dữ liệu huấn luyện\n",
    "* Nếu có và loại chính quy hóa nào chúng sử dụng\n",
    "\n",
    "Các thuật toán khác nhau chọn các cách khác nhau để đo lường ý nghĩa của \"phù hợp tốt với tập huấn luyện\". Vì các lý do toán học kỹ thuật, không thể điều chỉnh w và b để giảm thiểu số lượng phân loại sai mà các thuật toán tạo ra, như người ta có thể hy vọng. Đối với mục đích của chúng ta, và nhiều ứng dụng, các lựa chọn khác nhau cho mục 1 trong danh sách trên (được gọi là hàm mất mát) có ít ý nghĩa.\n",
    "\n",
    "Hai thuật toán phân loại tuyến tính phổ biến nhất là hồi quy logistic, được triển khai trong `linear_model.LogisticRegression`, và máy véc-tơ hỗ trợ tuyến tính (SVM tuyến tính), được triển khai trong `svm.LinearSVC` (SVC là viết tắt của support vector classifier - bộ phân loại véc-tơ hỗ trợ). Mặc dù có tên như vậy, `LogisticRegression` là một thuật toán phân loại chứ không phải là một thuật toán hồi quy, và không nên nhầm lẫn nó với `LinearRegression`.\n",
    "\n",
    "Chúng ta có thể áp dụng các mô hình `LogisticRegression` và `LinearSVC` cho tập dữ liệu forge, và trực quan hóa ranh giới quyết định được tìm thấy bởi các mô hình tuyến tính (Hình 2-15):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f55fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X, y = mglearn.datasets.make_forge()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "for model, ax in zip([LinearSVC(), LogisticRegression()], axes):\n",
    "    clf = model.fit(X, y)\n",
    "    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\n",
    "                                    ax=ax, alpha=.7)\n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "    ax.set_title(\"f{}\".format(clf.__class__.__name__))\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "axes[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620a46dc",
   "metadata": {},
   "source": [
    "**Hình 2-15. Ranh giới quyết định của một SVM tuyến tính và hồi quy logistic trên tập dữ liệu forge với các tham số mặc định**\n",
    "\n",
    "Trong hình này, chúng ta có đặc trưng đầu tiên của tập dữ liệu forge trên trục x và đặc trưng thứ hai trên trục y, như trước đây. Chúng ta hiển thị các ranh giới quyết định được tìm thấy bởi `LinearSVC` và `LogisticRegression` tương ứng dưới dạng các đường thẳng, tách vùng được phân loại là lớp 1 ở trên khỏi vùng được phân loại là lớp 0 ở dưới. Nói cách khác, bất kỳ điểm dữ liệu mới nào nằm trên đường màu đen sẽ được phân loại là lớp 1 bởi bộ phân loại tương ứng, trong khi bất kỳ điểm nào nằm dưới đường màu đen sẽ được phân loại là lớp 0.\n",
    "\n",
    "Hai mô hình này đưa ra các ranh giới quyết định tương tự nhau. Lưu ý rằng cả hai đều phân loại sai hai điểm. Theo mặc định, cả hai mô hình đều áp dụng chính quy hóa L2, theo cách tương tự như `Ridge` đối với hồi quy.\n",
    "\n",
    "Đối với `LogisticRegression` và `LinearSVC`, tham số đánh đổi xác định độ mạnh của chính quy hóa được gọi là C, và các giá trị C cao hơn tương ứng với ít chính quy hóa hơn. Nói cách khác, khi bạn sử dụng một giá trị cao cho tham số C, `LogisticRegression` và `LinearSVC` cố gắng khớp với tập huấn luyện tốt nhất có thể, trong khi với các giá trị thấp của tham số C, các mô hình đặt nhiều trọng tâm hơn vào việc tìm một véc-tơ hệ số (w) gần bằng không."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3065c5e",
   "metadata": {},
   "source": [
    "Có một khía cạnh thú vị khác về cách tham số C hoạt động. Việc sử dụng các giá trị C thấp sẽ khiến các thuật toán cố gắng điều chỉnh theo \"đa số\" các điểm dữ liệu, trong khi việc sử dụng một giá trị C cao hơn nhấn mạnh tầm quan trọng của việc mỗi điểm dữ liệu riêng lẻ được phân loại chính xác. Đây là một minh họa sử dụng `LinearSVC` (Hình 2-16):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3436c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_linear_svc_regularization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b516b15",
   "metadata": {},
   "source": [
    "**Hình 2-16. Ranh giới quyết định của một SVM tuyến tính trên tập dữ liệu forge cho các giá trị C khác nhau**\n",
    "\n",
    "Ở phía bên trái, chúng ta có một C rất nhỏ tương ứng với rất nhiều chính quy hóa. Hầu hết các điểm trong lớp 0 đều ở trên cùng, và hầu hết các điểm trong lớp 1 đều ở dưới cùng. Mô hình được chính quy hóa mạnh mẽ chọn một đường tương đối ngang, phân loại sai hai điểm. Trong biểu đồ ở giữa, C cao hơn một chút, và mô hình tập trung nhiều hơn vào hai mẫu bị phân loại sai, làm nghiêng ranh giới quyết định. Cuối cùng, ở phía bên phải, giá trị C rất cao trong mô hình làm nghiêng ranh giới quyết định rất nhiều, bây giờ phân loại chính xác tất cả các điểm trong lớp 0. Một trong những điểm trong lớp 1 vẫn bị phân loại sai, vì không thể phân loại chính xác tất cả các điểm trong tập dữ liệu này bằng một đường thẳng. Mô hình được minh họa ở phía bên phải cố gắng hết sức để phân loại chính xác tất cả các điểm, nhưng có thể không nắm bắt được bố cục tổng thể của các lớp. Nói cách khác, mô hình này có khả năng bị quá khớp.\n",
    "\n",
    "Tương tự như trường hợp hồi quy, các mô hình tuyến tính cho phân loại có vẻ rất hạn chế trong các không gian có chiều thấp, chỉ cho phép các ranh giới quyết định là các đường thẳng hoặc mặt phẳng. Một lần nữa, trong các chiều cao, các mô hình tuyến tính cho phân loại trở nên rất mạnh mẽ, và việc phòng ngừa quá khớp ngày càng trở nên quan trọng khi xem xét nhiều đặc trưng hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c64d0d",
   "metadata": {},
   "source": [
    "Hãy phân tích `LogisticRegression` chi tiết hơn trên tập dữ liệu Ung thư vú:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba6924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
    "logreg = LogisticRegression().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569cdb2c",
   "metadata": {},
   "source": [
    "Giá trị mặc định của C=1 cung cấp hiệu suất khá tốt, với độ chính xác 95% trên cả tập huấn luyện và tập kiểm tra. Nhưng vì hiệu suất của tập huấn luyện và tập kiểm tra rất gần nhau, có khả năng chúng ta đang bị thiếu khớp. Hãy thử tăng C để phù hợp với một mô hình linh hoạt hơn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db11282",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg100 = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30361fa0",
   "metadata": {},
   "source": [
    "Sử dụng `C=100` dẫn đến độ chính xác của tập huấn luyện cao hơn, và cũng làm tăng nhẹ độ chính xác của tập kiểm tra, xác nhận trực giác của chúng ta rằng một mô hình phức tạp hơn sẽ hoạt động tốt hơn.\n",
    "\n",
    "Chúng ta cũng có thể điều tra điều gì sẽ xảy ra nếu chúng ta sử dụng một mô hình được chính quy hóa nhiều hơn so với mặc định là `C=1`, bằng cách đặt `C=0.01`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299716f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg001.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg001.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a877e0",
   "metadata": {},
   "source": [
    "Như mong đợi, khi di chuyển sang bên trái hơn dọc theo thang đo được hiển thị trong Hình 2-1 từ một mô hình đã bị thiếu khớp, cả độ chính xác của tập huấn luyện và tập kiểm tra đều giảm so với các tham số mặc định.\n",
    "\n",
    "Cuối cùng, hãy xem xét các hệ số được học bởi các mô hình với ba cài đặt khác nhau của tham số chính quy hóa C (Hình 2-17):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0431243",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logreg.coef_.T, 'o', label=\"C=1\")\n",
    "plt.plot(logreg100.coef_.T, '^', label=\"C=100\")\n",
    "plt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\n",
    "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
    "plt.hlines(0, 0, cancer.data.shape[1])\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63a8bf3",
   "metadata": {},
   "source": [
    "**Hình 2-17. Các hệ số được học bởi hồi quy logistic trên tập dữ liệu Ung thư vú cho các giá trị C khác nhau**\n",
    "\n",
    "Vì `LogisticRegression` áp dụng chính quy hóa L2 theo mặc định, kết quả trông tương tự như kết quả được tạo bởi `Ridge` trong Hình 2-12. Chính quy hóa mạnh hơn đẩy các hệ số ngày càng gần về 0, mặc dù các hệ số không bao giờ bằng 0. Kiểm tra biểu đồ kỹ hơn, chúng ta cũng có thể thấy một hiệu ứng thú vị ở hệ số thứ ba, cho \"mean perimeter\". Đối với `C=100` và `C=1`, hệ số là âm, trong khi đối với `C=0.001`, hệ số là dương, với độ lớn thậm chí còn lớn hơn so với `C=1`. Giải thích một mô hình như thế này, người ta có thể nghĩ rằng hệ số cho chúng ta biết một đặc trưng được liên kết với lớp nào. Ví dụ, người ta có thể nghĩ rằng một đặc trưng \"texture error\" cao có liên quan đến một mẫu là \"ác tính\". Tuy nhiên, sự thay đổi dấu trong hệ số cho \"mean perimeter\" có nghĩa là tùy thuộc vào mô hình chúng ta xem xét, một \"mean perimeter\" cao có thể được coi là chỉ báo của \"lành tính\" hoặc chỉ báo của \"ác tính\". Điều này minh họa rằng việc giải thích các hệ số của các mô hình tuyến tính phải luôn được thực hiện một cách thận trọng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd182fd",
   "metadata": {},
   "source": [
    "Nếu chúng ta muốn một mô hình dễ giải thích hơn, việc sử dụng chính quy hóa L1 có thể hữu ích, vì nó giới hạn mô hình chỉ sử dụng một vài đặc trưng. Đây là biểu đồ hệ số và độ chính xác phân loại cho chính quy hóa L1 (Hình 2-18):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):\n",
    "    lr_l1 = LogisticRegression(C=C, penalty=\"l1\").fit(X_train, y_train)\n",
    "    print(\"Training accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n",
    "        C, lr_l1.score(X_train, y_train)))\n",
    "    print(\"Test accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n",
    "        C, lr_l1.score(X_test, y_test)))\n",
    "    plt.plot(lr_l1.coef_.T, marker, label=\"C={:.3f}\".format(C))\n",
    "\n",
    "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
    "plt.hlines(0, 0, cancer.data.shape[1])\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "\n",
    "plt.ylim(-5, 5)\n",
    "plt.legend(loc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7295c7",
   "metadata": {},
   "source": [
    "**Hình 2-18. Các hệ số được học bởi hồi quy logistic với hình phạt L1 trên tập dữ liệu Ung thư vú cho các giá trị C khác nhau**\n",
    "\n",
    "Như bạn có thể thấy, có nhiều điểm tương đồng giữa các mô hình tuyến tính cho phân loại nhị phân và các mô hình tuyến tính cho hồi quy. Như trong hồi quy, sự khác biệt chính giữa các mô hình là tham số hình phạt, ảnh hưởng đến sự chính quy hóa và liệu mô hình sẽ sử dụng tất cả các đặc trưng có sẵn hay chỉ chọn một tập hợp con."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f48ac",
   "metadata": {},
   "source": [
    "#### Mô hình tuyến tính cho phân loại đa lớp\n",
    "\n",
    "Nhiều mô hình phân loại tuyến tính chỉ dành cho phân loại nhị phân và không mở rộng một cách tự nhiên cho trường hợp đa lớp (ngoại trừ hồi quy logistic). Một kỹ thuật phổ biến để mở rộng một thuật toán phân loại nhị phân thành một thuật toán phân loại đa lớp là phương pháp một-so-với-phần-còn-lại (one-vs.-rest). Trong phương pháp một-so-với-phần-còn-lại, một mô hình nhị phân được học cho mỗi lớp cố gắng tách lớp đó ra khỏi tất cả các lớp khác, dẫn đến có bao nhiêu mô hình nhị phân thì có bấy nhiêu lớp. Để đưa ra dự đoán, tất cả các bộ phân loại nhị phân được chạy trên một điểm kiểm tra. Bộ phân loại có điểm số cao nhất trên lớp duy nhất của nó sẽ \"thắng\", và nhãn lớp này được trả về làm dự đoán."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68deb31a",
   "metadata": {},
   "source": [
    "Việc có một bộ phân loại nhị phân cho mỗi lớp dẫn đến việc có một véc tơ hệ số (w) và một điểm cắt (b) cho mỗi lớp. Lớp có kết quả của công thức tin cậy phân loại được đưa ra ở đây cao nhất là nhãn lớp được gán:\n",
    "\n",
    "$w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b$\n",
    "\n",
    "Toán học đằng sau hồi quy logistic đa lớp khác một chút so với phương pháp một-so-với-phần-còn-lại, nhưng chúng cũng dẫn đến một véc tơ hệ số và điểm cắt cho mỗi lớp, và phương pháp đưa ra dự đoán tương tự được áp dụng.\n",
    "\n",
    "Hãy áp dụng phương pháp một-so-với-phần-còn-lại cho một tập dữ liệu phân loại ba lớp đơn giản. Chúng ta sử dụng một tập dữ liệu hai chiều, trong đó mỗi lớp được cho bởi dữ liệu được lấy mẫu từ một phân phối Gaussian (xem Hình 2-19):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf0876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(random_state=42)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3753c8",
   "metadata": {},
   "source": [
    "**Hình 2-19. Tập dữ liệu đồ chơi hai chiều chứa ba lớp**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47af52",
   "metadata": {},
   "source": [
    "Bây giờ, chúng ta huấn luyện một bộ phân loại `LinearSVC` trên tập dữ liệu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f52a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm = LinearSVC().fit(X, y)\n",
    "print(\"Coefficient shape: \", linear_svm.coef_.shape)\n",
    "print(\"Intercept shape: \", linear_svm.intercept_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e883c184",
   "metadata": {},
   "source": [
    "Chúng ta thấy rằng hình dạng của `coef_` là (3, 2), có nghĩa là mỗi hàng của `coef_` chứa véc-tơ hệ số cho một trong ba lớp và mỗi cột giữ giá trị hệ số cho một đặc trưng cụ thể (có hai đặc trưng trong tập dữ liệu này). `intercept_` bây giờ là một mảng một chiều, lưu trữ các điểm cắt cho mỗi lớp.\n",
    "\n",
    "Hãy trực quan hóa các đường thẳng được cho bởi ba bộ phân loại nhị phân (Hình 2-20):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1becc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n",
    "                                ['b', 'r', 'g']):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "plt.ylim(-10, 15)\n",
    "plt.xlim(-10, 8)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n",
    "            'Line class 2'], loc=(1.01, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778031a",
   "metadata": {},
   "source": [
    "**Hình 2-20. Ranh giới quyết định được học bởi ba bộ phân loại một-so-với-phần-còn-lại**\n",
    "\n",
    "Bạn có thể thấy rằng tất cả các điểm thuộc lớp 0 trong dữ liệu huấn luyện đều nằm trên đường tương ứng với lớp 0, có nghĩa là chúng nằm ở phía \"lớp 0\" của bộ phân loại nhị phân này. Các điểm trong lớp 0 nằm trên đường tương ứng với lớp 2, có nghĩa là chúng được phân loại là \"phần còn lại\" bởi bộ phân loại nhị phân cho lớp 2. Các điểm thuộc lớp 0 nằm ở bên trái đường tương ứng với lớp 1, có nghĩa là bộ phân loại nhị phân cho lớp 1 cũng phân loại chúng là \"phần còn lại\". Do đó, bất kỳ điểm nào trong vùng này sẽ được phân loại là lớp 0 bởi bộ phân loại cuối cùng (kết quả của công thức tin cậy phân loại cho bộ phân loại 0 lớn hơn 0, trong khi nó nhỏ hơn 0 đối với hai lớp còn lại).\n",
    "\n",
    "Nhưng còn tam giác ở giữa biểu đồ thì sao? Tất cả ba bộ phân loại nhị phân đều phân loại các điểm ở đó là \"phần còn lại\". Một điểm ở đó sẽ được gán cho lớp nào? Câu trả lời là lớp có giá trị cao nhất cho công thức phân loại: lớp của đường gần nhất."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eddf742",
   "metadata": {},
   "source": [
    "Ví dụ sau (Hình 2-21) cho thấy các dự đoán cho tất cả các vùng của không gian 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81066c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n",
    "                                mglearn.cm3.colors):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n",
    "            'Line class 2'], loc=(1.01, 0.3))\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a023b",
   "metadata": {},
   "source": [
    "**Hình 2-21. Ranh giới quyết định đa lớp được suy ra từ ba bộ phân loại một-so-với-phần-còn-lại**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f412c9",
   "metadata": {},
   "source": [
    "#### Điểm mạnh, điểm yếu và tham số\n",
    "\n",
    "Tham số chính của các mô hình tuyến tính là tham số chính quy hóa, được gọi là `alpha` trong các mô hình hồi quy và `C` trong `LinearSVC` và `LogisticRegression`. Các giá trị lớn cho `alpha` hoặc các giá trị nhỏ cho `C` có nghĩa là các mô hình đơn giản. Đặc biệt đối với các mô hình hồi quy, việc điều chỉnh các tham số này là khá quan trọng. Thông thường `C` và `alpha` được tìm kiếm trên thang đo logarit. Quyết định khác bạn phải đưa ra là liệu bạn muốn sử dụng chính quy hóa L1 hay chính quy hóa L2. Nếu bạn cho rằng chỉ một vài đặc trưng của bạn thực sự quan trọng, bạn nên sử dụng L1. Nếu không, bạn nên mặc định sử dụng L2. L1 cũng có thể hữu ích nếu khả năng giải thích của mô hình là quan trọng. Vì L1 sẽ chỉ sử dụng một vài đặc trưng, nên việc giải thích đặc trưng nào quan trọng đối với mô hình và tác dụng của các đặc trưng này sẽ dễ dàng hơn.\n",
    "\n",
    "Các mô hình tuyến tính rất nhanh để huấn luyện và cũng nhanh để dự đoán. Chúng có thể mở rộng cho các tập dữ liệu rất lớn và hoạt động tốt với dữ liệu thưa thớt. Nếu dữ liệu của bạn bao gồm hàng trăm nghìn hoặc hàng triệu mẫu, bạn có thể muốn điều tra việc sử dụng tùy chọn `solver='sag'` trong `LogisticRegression` và `Ridge`, có thể nhanh hơn mặc định trên các tập dữ liệu lớn. Các tùy chọn khác là lớp `SGDClassifier` và lớp `SGDRegressor`, triển khai các phiên bản có khả năng mở rộng hơn của các mô hình tuyến tính được mô tả ở đây.\n",
    "\n",
    "Một điểm mạnh khác của các mô hình tuyến tính là chúng giúp việc hiểu cách một dự đoán được thực hiện tương đối dễ dàng, bằng cách sử dụng các công thức chúng ta đã thấy trước đó cho hồi quy và phân loại. Thật không may, thường không hoàn toàn rõ ràng tại sao các hệ số lại như vậy. Điều này đặc biệt đúng nếu tập dữ liệu của bạn có các đặc trưng tương quan cao; trong những trường hợp này, các hệ số có thể khó giải thích.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7118c",
   "metadata": {},
   "source": [
    "Các mô hình tuyến tính thường hoạt động tốt khi số lượng đặc trưng lớn so với số lượng mẫu. Chúng cũng thường được sử dụng trên các tập dữ liệu rất lớn, đơn giản vì việc huấn luyện các mô hình khác là không khả thi. Tuy nhiên, trong các không gian có chiều thấp hơn, các mô hình khác có thể mang lại hiệu suất tổng quát hóa tốt hơn. Chúng ta sẽ xem xét một số ví dụ trong đó các mô hình tuyến tính thất bại trong \"Máy véc-tơ hỗ trợ hạt nhân\" trên trang 92."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf1e4d",
   "metadata": {},
   "source": [
    "### Nối chuỗi phương thức\n",
    "\n",
    "Phương thức `fit` của tất cả các mô hình scikit-learn đều trả về `self`. Điều này cho phép bạn viết mã như sau, mà chúng ta đã sử dụng rộng rãi trong chương này:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d754a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# khởi tạo mô hình và điều chỉnh nó trong một dòng\n",
    "logreg = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e426037",
   "metadata": {},
   "source": [
    "Ở đây, chúng ta đã sử dụng giá trị trả về của `fit` (là `self`) để gán mô hình đã huấn luyện cho biến `logreg`. Việc nối các lệnh gọi phương thức này (ở đây là `__init__` và sau đó là `fit`) được gọi là nối chuỗi phương thức. Một ứng dụng phổ biến khác của việc nối chuỗi phương thức trong scikit-learn là điều chỉnh và dự đoán trong một dòng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f3e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "y_pred = logreg.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3f3c64",
   "metadata": {},
   "source": [
    "Cuối cùng, bạn thậm chí có thể thực hiện việc khởi tạo mô hình, điều chỉnh và dự đoán trong một dòng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc202135",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LogisticRegression().fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b18c1",
   "metadata": {},
   "source": [
    "Tuy nhiên, biến thể rất ngắn này không lý tưởng. Có rất nhiều thứ xảy ra trong một dòng duy nhất, điều này có thể làm cho mã khó đọc. Ngoài ra, mô hình hồi quy logistic đã điều chỉnh không được lưu trữ trong bất kỳ biến nào, vì vậy chúng ta không thể kiểm tra nó hoặc sử dụng nó để dự đoán trên bất kỳ dữ liệu nào khác."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e994bb3",
   "metadata": {},
   "source": [
    "### Bộ phân loại Naive Bayes\n",
    "\n",
    "Bộ phân loại Naive Bayes là một họ các bộ phân loại khá giống với các mô hình tuyến tính được thảo luận trong phần trước. Tuy nhiên, chúng có xu hướng huấn luyện nhanh hơn. Cái giá phải trả cho hiệu quả này là các mô hình Naive Bayes thường cung cấp hiệu suất tổng quát hóa kém hơn một chút so với các bộ phân loại tuyến tính như `LogisticRegression` và `LinearSVC`.\n",
    "\n",
    "Lý do các mô hình Naive Bayes rất hiệu quả là vì chúng học các tham số bằng cách xem xét từng đặc trưng riêng lẻ và thu thập các thống kê đơn giản theo từng lớp từ mỗi đặc trưng. Có ba loại bộ phân loại Naive Bayes được triển khai trong scikit-learn: `GaussianNB`, `BernoulliNB` và `MultinomialNB`. `GaussianNB` có thể được áp dụng cho bất kỳ dữ liệu liên tục nào, trong khi `BernoulliNB` giả định dữ liệu nhị phân và `MultinomialNB` giả định dữ liệu đếm (nghĩa là, mỗi đặc trưng đại diện cho một số đếm nguyên của một cái gì đó, như tần suất một từ xuất hiện trong một câu). `BernoulliNB` và `MultinomialNB` chủ yếu được sử dụng trong phân loại dữ liệu văn bản."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5649b181",
   "metadata": {},
   "source": [
    "Bộ phân loại `BernoulliNB` đếm tần suất mỗi đặc trưng của mỗi lớp khác không. Điều này dễ hiểu nhất qua một ví dụ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4572ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 1, 0, 1],\n",
    "              [1, 0, 1, 1],\n",
    "              [0, 0, 0, 1],\n",
    "              [1, 0, 1, 0]])\n",
    "y = np.array([0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160818b7",
   "metadata": {},
   "source": [
    "Ở đây, chúng ta có bốn điểm dữ liệu, mỗi điểm có bốn đặc trưng nhị phân. Có hai lớp, 0 và 1. Đối với lớp 0 (điểm dữ liệu thứ nhất và thứ ba), đặc trưng đầu tiên bằng không hai lần và khác không không lần, đặc trưng thứ hai bằng không một lần và khác không một lần, v.v. Các số đếm tương tự sau đó được tính toán cho các điểm dữ liệu trong lớp thứ hai. Việc đếm các mục khác không cho mỗi lớp về cơ bản trông như thế này:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91794dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for label in np.unique(y):\n",
    "    # lặp qua từng lớp\n",
    "    # đếm (tổng) các mục của 1 cho mỗi đặc trưng\n",
    "    counts[label] = X[y == label].sum(axis=0)\n",
    "print(\"Feature counts:\\n{}\".format(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b300e3",
   "metadata": {},
   "source": [
    "Hai mô hình Naive Bayes khác, `MultinomialNB` và `GaussianNB`, hơi khác nhau về loại thống kê mà chúng tính toán. `MultinomialNB` tính đến giá trị trung bình của mỗi đặc trưng cho mỗi lớp, trong khi `GaussianNB` lưu trữ giá trị trung bình cũng như độ lệch chuẩn của mỗi đặc trưng cho mỗi lớp.\n",
    "\n",
    "Để đưa ra dự đoán, một điểm dữ liệu được so sánh với các thống kê cho mỗi lớp, và lớp phù hợp nhất được dự đoán. Điều thú vị là đối với cả `MultinomialNB` và `BernoulliNB`, điều này dẫn đến một công thức dự đoán có cùng dạng như trong các mô hình tuyến tính (xem \"Mô hình tuyến tính cho phân loại\" trên trang 56). Thật không may, `coef_` cho các mô hình Naive Bayes có ý nghĩa hơi khác so với trong các mô hình tuyến tính, ở chỗ `coef_` không giống như w."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9bc9f",
   "metadata": {},
   "source": [
    "#### Điểm mạnh, điểm yếu và tham số\n",
    "\n",
    "`MultinomialNB` và `BernoulliNB` có một tham số duy nhất, `alpha`, kiểm soát độ phức tạp của mô hình. Cách `alpha` hoạt động là thuật toán thêm vào dữ liệu `alpha` nhiều điểm dữ liệu ảo có giá trị dương cho tất cả các đặc trưng. Điều này dẫn đến việc \"làm mịn\" các thống kê. Một `alpha` lớn có nghĩa là làm mịn nhiều hơn, dẫn đến các mô hình ít phức tạp hơn. Hiệu suất của thuật toán tương đối ổn định với cài đặt của `alpha`, có nghĩa là việc đặt `alpha` không quan trọng đối với hiệu suất tốt. Tuy nhiên, việc điều chỉnh nó thường cải thiện độ chính xác một chút.\n",
    "\n",
    "`GaussianNB` chủ yếu được sử dụng trên dữ liệu có chiều rất cao, trong khi hai biến thể khác của Naive Bayes được sử dụng rộng rãi cho dữ liệu đếm thưa thớt như văn bản. `MultinomialNB` thường hoạt động tốt hơn `BinaryNB`, đặc biệt là trên các tập dữ liệu có số lượng lớn tương đối các đặc trưng khác không (tức là, các tài liệu lớn).\n",
    "\n",
    "Các mô hình Naive Bayes có nhiều điểm mạnh và điểm yếu giống như các mô hình tuyến tính. Chúng rất nhanh để huấn luyện và dự đoán, và quy trình huấn luyện dễ hiểu. Các mô hình hoạt động rất tốt với dữ liệu thưa thớt có chiều cao và tương đối ổn định với các tham số. Các mô hình Naive Bayes là các mô hình cơ sở tuyệt vời và thường được sử dụng trên các tập dữ liệu rất lớn, nơi việc huấn luyện ngay cả một mô hình tuyến tính cũng có thể mất quá nhiều thời gian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67bc930",
   "metadata": {},
   "source": [
    "### Cây quyết định\n",
    "\n",
    "Cây quyết định là các mô hình được sử dụng rộng rãi cho các tác vụ phân loại và hồi quy. Về cơ bản, chúng học một hệ thống phân cấp các câu hỏi nếu/thì, dẫn đến một quyết định.\n",
    "\n",
    "Những câu hỏi này tương tự như những câu hỏi bạn có thể hỏi trong một trò chơi 20 câu hỏi. Hãy tưởng tượng bạn muốn phân biệt giữa bốn loài động vật sau: gấu, diều hâu, chim cánh cụt và cá heo. Mục tiêu của bạn là đi đến câu trả lời đúng bằng cách hỏi càng ít câu hỏi nếu/thì càng tốt. Bạn có thể bắt đầu bằng cách hỏi xem con vật có lông vũ không, một câu hỏi thu hẹp các loài động vật có thể có của bạn xuống chỉ còn hai. Nếu câu trả lời là \"có\", bạn có thể hỏi một câu hỏi khác có thể giúp bạn phân biệt giữa diều hâu và chim cánh cụt. Ví dụ, bạn có thể hỏi xem con vật có thể bay không. Nếu con vật không có lông vũ, các lựa chọn động vật có thể có của bạn là cá heo và gấu, và bạn sẽ cần hỏi một câu hỏi để phân biệt giữa hai loài động vật này—ví dụ, hỏi xem con vật có vây không.\n",
    "\n",
    "Chuỗi câu hỏi này có thể được biểu diễn dưới dạng một cây quyết định, như được hiển thị trong Hình 2-22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbecb3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_animal_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba985c",
   "metadata": {},
   "source": [
    "**Hình 2-22. Một cây quyết định để phân biệt giữa một số loài động vật**\n",
    "\n",
    "Trong hình minh họa này, mỗi nút trong cây hoặc đại diện cho một câu hỏi hoặc một nút cuối (còn gọi là lá) chứa câu trả lời. Các cạnh nối các câu trả lời cho một câu hỏi với câu hỏi tiếp theo bạn sẽ hỏi.\n",
    "\n",
    "Theo thuật ngữ học máy, chúng ta đã xây dựng một mô hình để phân biệt giữa bốn lớp động vật (diều hâu, chim cánh cụt, cá heo và gấu) bằng cách sử dụng ba đặc trưng \"có lông vũ\", \"có thể bay\" và \"có vây\". Thay vì xây dựng các mô hình này bằng tay, chúng ta có thể học chúng từ dữ liệu bằng cách sử dụng học có giám sát."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba2053",
   "metadata": {},
   "source": [
    "#### Xây dựng cây quyết định\n",
    "\n",
    "Hãy đi qua quá trình xây dựng một cây quyết định cho tập dữ liệu phân loại 2D được hiển thị trong Hình 2-23. Tập dữ liệu bao gồm hai hình bán nguyệt, với mỗi lớp bao gồm 75 điểm dữ liệu. Chúng ta sẽ gọi tập dữ liệu này là `two_moons`.\n",
    "\n",
    "Việc học một cây quyết định có nghĩa là học một chuỗi các câu hỏi nếu/thì để đưa chúng ta đến câu trả lời đúng một cách nhanh nhất. Trong cài đặt học máy, những câu hỏi này được gọi là các bài kiểm tra (không nên nhầm lẫn với tập kiểm tra, là dữ liệu chúng ta sử dụng để kiểm tra xem mô hình của chúng ta có khả năng tổng quát hóa như thế nào). Thông thường, dữ liệu không có dạng các đặc trưng nhị phân có/không như trong ví dụ về động vật, mà thay vào đó được biểu diễn dưới dạng các đặc trưng liên tục như trong tập dữ liệu 2D được hiển thị trong Hình 2-23. Các bài kiểm tra được sử dụng trên dữ liệu liên tục có dạng \"Đặc trưng i có lớn hơn giá trị a không?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dbd56b",
   "metadata": {},
   "source": [
    "**Hình 2-23. Tập dữ liệu `two_moons` mà cây quyết định sẽ được xây dựng trên đó**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e38419",
   "metadata": {},
   "source": [
    "Để xây dựng một cây, thuật toán tìm kiếm trên tất cả các bài kiểm tra có thể có và tìm ra bài kiểm tra có nhiều thông tin nhất về biến mục tiêu. Hình 2-24 cho thấy bài kiểm tra đầu tiên được chọn. Việc chia tập dữ liệu theo chiều dọc tại `x[1]=0.0596` mang lại nhiều thông tin nhất; nó tách tốt nhất các điểm trong lớp 1 khỏi các điểm trong lớp 2. Nút trên cùng, còn được gọi là gốc, đại diện cho toàn bộ tập dữ liệu, bao gồm 75 điểm thuộc lớp 0 và 75 điểm thuộc lớp 1. Việc chia được thực hiện bằng cách kiểm tra xem `x[1] <= 0.0596`, được chỉ ra bằng một đường màu đen. Nếu bài kiểm tra là đúng, một điểm được gán cho nút bên trái, chứa 2 điểm thuộc lớp 0 và 32 điểm thuộc lớp 1. Nếu không, điểm đó được gán cho nút bên phải, chứa 48 điểm thuộc lớp 0 và 18 điểm thuộc lớp 1. Hai nút này tương ứng với các vùng trên và dưới được hiển thị trong Hình 2-24. Mặc dù lần chia đầu tiên đã thực hiện tốt việc tách hai lớp, vùng dưới vẫn chứa các điểm thuộc lớp 0 và vùng trên vẫn chứa các điểm thuộc lớp 1. Chúng ta có thể xây dựng một mô hình chính xác hơn bằng cách lặp lại quá trình tìm kiếm bài kiểm tra tốt nhất trong cả hai vùng. Hình 2-25 cho thấy rằng lần chia tiếp theo có nhiều thông tin nhất cho vùng bên trái và bên phải dựa trên x[0]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546af144",
   "metadata": {},
   "source": [
    "**Hình 2-24. Ranh giới quyết định của cây có độ sâu 1 (trái) và cây tương ứng (phải)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bea529",
   "metadata": {},
   "source": [
    "**Hình 2-25. Ranh giới quyết định của cây có độ sâu 2 (trái) và cây quyết định tương ứng (phải)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9421ffa",
   "metadata": {},
   "source": [
    "Quá trình đệ quy này tạo ra một cây quyết định nhị phân, với mỗi nút chứa một bài kiểm tra. Ngoài ra, bạn có thể nghĩ về mỗi bài kiểm tra như là việc chia phần dữ liệu hiện đang được xem xét dọc theo một trục. Điều này mang lại một cái nhìn về thuật toán như là việc xây dựng một phân vùng phân cấp. Vì mỗi bài kiểm tra chỉ liên quan đến một đặc trưng duy nhất, các vùng trong phân vùng kết quả luôn có ranh giới song song với trục.\n",
    "\n",
    "Việc phân vùng đệ quy của dữ liệu được lặp lại cho đến khi mỗi vùng trong phân vùng (mỗi lá trong cây quyết định) chỉ chứa một giá trị mục tiêu duy nhất (một lớp duy nhất hoặc một giá trị hồi quy duy nhất). Một lá của cây chứa các điểm dữ liệu đều có cùng một giá trị mục tiêu được gọi là thuần khiết. Phân vùng cuối cùng cho tập dữ liệu này được hiển thị trong Hình 2-26."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1475a8ef",
   "metadata": {},
   "source": [
    "**Hình 2-26. Ranh giới quyết định của cây có độ sâu 9 (trái) và một phần của cây tương ứng (phải); cây đầy đủ khá lớn và khó hình dung**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ea9ba3",
   "metadata": {},
   "source": [
    "Một dự đoán trên một điểm dữ liệu mới được thực hiện bằng cách kiểm tra xem điểm đó nằm trong vùng nào của phân vùng của không gian đặc trưng, và sau đó dự đoán mục tiêu đa số (hoặc mục tiêu duy nhất trong trường hợp lá thuần khiết) trong vùng đó. Vùng có thể được tìm thấy bằng cách duyệt cây từ gốc và đi sang trái hoặc phải, tùy thuộc vào việc bài kiểm tra có được thỏa mãn hay không.\n",
    "\n",
    "Cũng có thể sử dụng cây cho các tác vụ hồi quy, bằng cách sử dụng kỹ thuật hoàn toàn tương tự. Để đưa ra dự đoán, chúng ta duyệt cây dựa trên các bài kiểm tra trong mỗi nút và tìm lá mà điểm dữ liệu mới rơi vào. Đầu ra cho điểm dữ liệu này là giá trị mục tiêu trung bình của các điểm huấn luyện trong lá này."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95245f3",
   "metadata": {},
   "source": [
    "#### Kiểm soát độ phức tạp của cây quyết định\n",
    "\n",
    "Thông thường, việc xây dựng một cây như được mô tả ở đây và tiếp tục cho đến khi tất cả các lá đều thuần khiết dẫn đến các mô hình rất phức tạp và bị quá khớp với dữ liệu huấn luyện. Sự hiện diện của các lá thuần khiết có nghĩa là một cây chính xác 100% trên tập huấn luyện; mỗi điểm dữ liệu trong tập huấn luyện đều nằm trong một lá có lớp đa số chính xác. Việc quá khớp có thể được nhìn thấy ở bên trái của Hình 2-26. Bạn có thể thấy các vùng được xác định là thuộc về lớp 1 ở giữa tất cả các điểm thuộc về lớp 0. Mặt khác, có một dải nhỏ được dự đoán là lớp 0 xung quanh điểm thuộc về lớp 0 ở phía xa bên phải. Đây không phải là cách người ta tưởng tượng ranh giới quyết định sẽ trông như thế nào, và ranh giới quyết định tập trung rất nhiều vào các điểm ngoại lệ đơn lẻ cách xa các điểm khác trong lớp đó.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91a28ec",
   "metadata": {},
   "source": [
    "Có hai chiến lược phổ biến để ngăn chặn việc quá khớp: dừng việc tạo cây sớm (còn được gọi là tỉa trước), hoặc xây dựng cây nhưng sau đó loại bỏ hoặc thu gọn các nút chứa ít thông tin (còn được gọi là tỉa sau hoặc chỉ đơn giản là tỉa). Các tiêu chí có thể có cho việc tỉa trước bao gồm giới hạn độ sâu tối đa của cây, giới hạn số lượng lá tối đa, hoặc yêu cầu một số lượng điểm tối thiểu trong một nút để tiếp tục chia nó. \n",
    "\n",
    "Cây quyết định trong scikit-learn được triển khai trong các lớp `DecisionTreeRegressor` và `DecisionTreeClassifier`. scikit-learn chỉ triển khai tỉa trước, không triển khai tỉa sau. \n",
    "\n",
    "Hãy xem xét chi tiết hơn về hiệu quả của việc tỉa trước trên tập dữ liệu Ung thư vú. Như mọi khi, chúng ta nhập tập dữ liệu và chia nó thành một tập huấn luyện và một tập kiểm tra. Sau đó, chúng ta xây dựng một mô hình bằng cách sử dụng cài đặt mặc định để phát triển đầy đủ cây (phát triển cây cho đến khi tất cả các lá đều thuần khiết). Chúng ta cố định `random_state` trong cây, được sử dụng để phá vỡ các trường hợp hòa bên trong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a932641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7545735c",
   "metadata": {},
   "source": [
    "Như mong đợi, độ chính xác trên tập huấn luyện là 100%—bởi vì các lá là thuần khiết, cây đã được phát triển đủ sâu để có thể ghi nhớ hoàn hảo tất cả các nhãn trên dữ liệu huấn luyện. Độ chính xác của tập kiểm tra kém hơn một chút so với các mô hình tuyến tính mà chúng ta đã xem xét trước đó, có độ chính xác khoảng 95%. \n",
    "\n",
    "Nếu chúng ta không giới hạn độ sâu của một cây quyết định, cây có thể trở nên sâu và phức tạp một cách tùy ý. Do đó, các cây không được tỉa dễ bị quá khớp và không tổng quát hóa tốt cho dữ liệu mới. Bây giờ hãy áp dụng tỉa trước cho cây, điều này sẽ dừng việc phát triển cây trước khi chúng ta khớp hoàn hảo với dữ liệu huấn luyện. Một tùy chọn là dừng việc xây dựng cây sau khi đạt đến một độ sâu nhất định. Ở đây chúng ta đặt `max_depth=4`, có nghĩa là chỉ có thể hỏi bốn câu hỏi liên tiếp (xem Hình 2-24 và 2-26). Việc giới hạn độ sâu của cây làm giảm việc quá khớp. Điều này dẫn đến độ chính xác thấp hơn trên tập huấn luyện, nhưng cải thiện trên tập kiểm tra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cbfa61",
   "metadata": {},
   "source": [
    "#### Phân tích cây quyết định\n",
    "Chúng ta có thể trực quan hóa cây bằng cách sử dụng hàm `export_graphviz` từ mô-đun `tree`. Hàm này ghi một tệp ở định dạng tệp dot, là một định dạng tệp văn bản để lưu trữ các đồ thị. Chúng ta đặt một tùy chọn để tô màu các nút để phản ánh lớp đa số trong mỗi nút và truyền tên lớp và tên đặc trưng để cây có thể được dán nhãn đúng cách:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"],\n",
    "                feature_names=cancer.feature_names, impurity=False, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa01fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf93133c",
   "metadata": {},
   "source": [
    "**Hình 2-27. Trực quan hóa cây quyết định được xây dựng trên tập dữ liệu Ung thư vú**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf20ea",
   "metadata": {},
   "source": [
    "Việc trực quan hóa cây cung cấp một cái nhìn sâu sắc tuyệt vời về cách thuật toán đưa ra dự đoán, và là một ví dụ điển hình về một thuật toán học máy có thể dễ dàng giải thích cho những người không chuyên. Tuy nhiên, ngay cả với một cây có độ sâu bốn, như được thấy ở đây, cây có thể trở nên hơi quá tải. Các cây sâu hơn (độ sâu 10 không phải là hiếm) thậm chí còn khó nắm bắt hơn. Một phương pháp kiểm tra cây có thể hữu ích là tìm ra con đường mà hầu hết dữ liệu thực sự đi theo. `n_samples` được hiển thị trong mỗi nút trong Hình 2-27 cho biết số lượng mẫu trong nút đó, trong khi `value` cung cấp số lượng mẫu cho mỗi lớp. Theo các nhánh sang phải, chúng ta thấy rằng `worst radius <= 16.795` tạo ra một nút chỉ chứa 8 mẫu lành tính nhưng 134 mẫu ác tính. Phần còn lại của phía này của cây sau đó sử dụng một số phân biệt tinh vi hơn để tách ra 8 mẫu lành tính còn lại này. Trong số 142 mẫu đi sang phải trong lần chia ban đầu, gần như tất cả chúng (132) đều nằm trong lá ở phía xa bên phải. \n",
    "\n",
    "Đi sang trái ở gốc, đối với `worst radius > 16.795`, chúng ta kết thúc với 25 mẫu ác tính và 259 mẫu lành tính. Gần như tất cả các mẫu lành tính đều nằm trong lá thứ hai từ bên phải, với hầu hết các lá khác chứa rất ít mẫu. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb06e8",
   "metadata": {},
   "source": [
    "#### Tầm quan trọng của đặc trưng trong cây\n",
    "\n",
    "Thay vì xem xét toàn bộ cây, điều này có thể gây mệt mỏi, có một số thuộc tính hữu ích mà chúng ta có thể suy ra để tóm tắt hoạt động của cây. Tóm tắt được sử dụng phổ biến nhất là tầm quan trọng của đặc trưng, đánh giá mức độ quan trọng của mỗi đặc trưng đối với quyết định mà một cây đưa ra. Đó là một con số từ 0 đến 1 cho mỗi đặc trưng, trong đó 0 có nghĩa là \"hoàn toàn không được sử dụng\" và 1 có nghĩa là \"dự đoán hoàn hảo mục tiêu\". Tầm quan trọng của các đặc trưng luôn có tổng bằng 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cae1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importances:\\n{}\".format(tree.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2dc2a",
   "metadata": {},
   "source": [
    "Chúng ta có thể trực quan hóa tầm quan trọng của đặc trưng theo cách tương tự như cách chúng ta trực quan hóa các hệ số trong mô hình tuyến tính (Hình 2-28):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990dde57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances_cancer(model):\n",
    "    n_features = cancer.data.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), cancer.feature_names)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "\n",
    "plot_feature_importances_cancer(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da42ae8",
   "metadata": {},
   "source": [
    "**Hình 2-28. Tầm quan trọng của đặc trưng được tính toán từ một cây quyết định đã học trên tập dữ liệu Ung thư vú**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed3af69",
   "metadata": {},
   "source": [
    "Ở đây chúng ta thấy rằng đặc trưng được sử dụng trong lần chia trên cùng (\"worst radius\") là đặc trưng quan trọng nhất cho đến nay. Điều này xác nhận quan sát của chúng ta trong việc phân tích cây rằng cấp độ đầu tiên đã phân tách hai lớp khá tốt. \n",
    "\n",
    "Tuy nhiên, nếu một đặc trưng có `feature_importance` thấp, điều đó không có nghĩa là đặc trưng này không có thông tin. Nó chỉ có nghĩa là đặc trưng đó không được cây chọn, có thể là do một đặc trưng khác mã hóa cùng một thông tin. \n",
    "\n",
    "Ngược lại với các hệ số trong các mô hình tuyến tính, tầm quan trọng của đặc trưng luôn dương và không mã hóa lớp nào mà một đặc trưng chỉ ra. Tầm quan trọng của các đặc trưng cho chúng ta biết rằng \"worst radius\" là quan trọng, nhưng không cho biết liệu một bán kính cao có chỉ ra một mẫu là lành tính hay ác tính hay không. Trên thực tế, có thể không có một mối quan hệ đơn giản như vậy giữa các đặc trưng và lớp, như bạn có thể thấy trong ví dụ sau (Hình 2-29 và 2-30): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e2d1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = mglearn.plots.plot_tree_not_monotone()\n",
    "display(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce655b12",
   "metadata": {},
   "source": [
    "**Hình 2-29. Một tập dữ liệu hai chiều trong đó đặc trưng trên trục y có mối quan hệ không đơn điệu với nhãn lớp, và các ranh giới quyết định được tìm thấy bởi một cây quyết định**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5977fe",
   "metadata": {},
   "source": [
    "**Hình 2-30. Cây quyết định được học trên dữ liệu được hiển thị trong Hình 2-29**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3564623",
   "metadata": {},
   "source": [
    "Biểu đồ cho thấy một tập dữ liệu với hai đặc trưng và hai lớp. Ở đây, tất cả thông tin đều nằm trong `X[1]`, và `X[0]` hoàn toàn không được sử dụng. Nhưng mối quan hệ giữa `X[1]` và lớp đầu ra không đơn điệu, có nghĩa là chúng ta không thể nói \"giá trị cao của X[0] có nghĩa là lớp 0, và giá trị thấp có nghĩa là lớp 1\" (hoặc ngược lại). \n",
    "\n",
    "Mặc dù chúng ta đã tập trung thảo luận ở đây về cây quyết định cho phân loại, tất cả những gì đã nói đều tương tự đúng đối với cây quyết định cho hồi quy, như được triển khai trong `DecisionTreeRegressor`. Việc sử dụng và phân tích các cây hồi quy rất giống với việc sử dụng và phân tích các cây phân loại. Tuy nhiên, có một thuộc tính đặc biệt của việc sử dụng các mô hình dựa trên cây cho hồi quy mà chúng ta muốn chỉ ra. `DecisionTreeRegressor` (và tất cả các mô hình hồi quy dựa trên cây khác) không thể ngoại suy, hoặc đưa ra dự đoán ngoài phạm vi của dữ liệu huấn luyện. \n",
    "\n",
    "Hãy xem xét chi tiết hơn về điều này, bằng cách sử dụng một tập dữ liệu về giá bộ nhớ máy tính (RAM) lịch sử. Hình 2-31 cho thấy tập dữ liệu, với ngày tháng trên trục x và giá của một megabyte RAM trong năm đó trên trục y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67446691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ram_prices = pd.read_csv(\"data/ram_price.csv\")\n",
    "\n",
    "plt.semilogy(ram_prices.date, ram_prices.price)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Price in $/Mbyte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e859db",
   "metadata": {},
   "source": [
    "**Hình 2-31. Sự phát triển lịch sử của giá RAM, được vẽ trên thang đo logarit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b488fd3f",
   "metadata": {},
   "source": [
    "Lưu ý thang đo logarit của trục y. Khi vẽ biểu đồ theo logarit, mối quan hệ có vẻ khá tuyến tính và do đó sẽ tương đối dễ dự đoán, ngoài một số điểm bất thường. \n",
    "\n",
    "Chúng ta sẽ đưa ra một dự báo cho các năm sau năm 2000 bằng cách sử dụng dữ liệu lịch sử cho đến thời điểm đó, với ngày tháng là đặc trưng duy nhất của chúng ta. Chúng ta sẽ so sánh hai mô hình đơn giản: một `DecisionTreeRegressor` và một `LinearRegression`. Chúng ta chia lại tỷ lệ giá bằng cách sử dụng logarit, để mối quan hệ tương đối tuyến tính. Điều này không tạo ra sự khác biệt cho `DecisionTreeRegressor`, nhưng nó tạo ra sự khác biệt lớn cho `LinearRegression` (chúng ta sẽ thảo luận chi tiết hơn về điều này trong Chương 4). Sau khi huấn luyện các mô hình và đưa ra dự đoán, chúng ta áp dụng ánh xạ mũ để hoàn tác phép biến đổi logarit. Chúng ta đưa ra dự đoán trên toàn bộ tập dữ liệu cho mục đích trực quan hóa ở đây, nhưng để đánh giá định lượng, chúng ta sẽ chỉ xem xét tập dữ liệu kiểm tra: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ca672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# sử dụng dữ liệu lịch sử để dự báo giá sau năm 2000\n",
    "data_train = ram_prices[ram_prices.date < 2000]\n",
    "data_test = ram_prices[ram_prices.date >= 2000]\n",
    "\n",
    "# dự đoán giá dựa trên ngày\n",
    "X_train = data_train.date[:, np.newaxis]\n",
    "# chúng ta sử dụng một phép biến đổi log để có được mối quan hệ đơn giản hơn giữa dữ liệu và mục tiêu\n",
    "y_train = np.log(data_train.price)\n",
    "\n",
    "tree = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "linear_reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# dự đoán trên tất cả dữ liệu\n",
    "X_all = ram_prices.date[:, np.newaxis]\n",
    "\n",
    "pred_tree = tree.predict(X_all)\n",
    "pred_lr = linear_reg.predict(X_all)\n",
    "\n",
    "# hoàn tác phép biến đổi log\n",
    "price_tree = np.exp(pred_tree)\n",
    "price_lr = np.exp(pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a14a7e",
   "metadata": {},
   "source": [
    "Hình 2-32, được tạo ở đây, so sánh các dự đoán của cây quyết định và mô hình hồi quy tuyến tính với sự thật cơ bản:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d4cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(data_train.date, data_train.price, label=\"Training data\")\n",
    "plt.semilogy(data_test.date, data_test.price, label=\"Test data\")\n",
    "plt.semilogy(ram_prices.date, price_tree, label=\"Tree prediction\")\n",
    "plt.semilogy(ram_prices.date, price_lr, label=\"Linear prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67748574",
   "metadata": {},
   "source": [
    "**Hình 2-32. So sánh các dự đoán được thực hiện bởi một mô hình tuyến tính và các dự đoán được thực hiện bởi một cây hồi quy trên dữ liệu giá RAM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c01d7",
   "metadata": {},
   "source": [
    "Sự khác biệt giữa các mô hình khá rõ rệt. Mô hình tuyến tính xấp xỉ dữ liệu bằng một đường thẳng, như chúng ta đã biết. Đường thẳng này cung cấp một dự báo khá tốt cho dữ liệu kiểm tra (các năm sau 2000), trong khi bỏ qua một số biến thể nhỏ hơn trong cả dữ liệu huấn luyện và dữ liệu kiểm tra. Mô hình cây, mặt khác, đưa ra các dự đoán hoàn hảo trên dữ liệu huấn luyện; chúng ta không giới hạn độ phức tạp của cây, vì vậy nó đã học thuộc lòng toàn bộ tập dữ liệu. Tuy nhiên, một khi chúng ta rời khỏi phạm vi dữ liệu mà mô hình có dữ liệu, mô hình chỉ đơn giản là tiếp tục dự đoán điểm cuối cùng đã biết. Cây không có khả năng tạo ra các phản hồi \"mới\", ngoài những gì đã được thấy trong dữ liệu huấn luyện. Thiếu sót này áp dụng cho tất cả các mô hình dựa trên cây.⁹\n",
    "\n",
    "--- \n",
    "⁹Thực tế có thể đưa ra các dự báo rất tốt với các mô hình dựa trên cây (ví dụ, khi cố gắng dự đoán liệu giá sẽ tăng hay giảm). Điểm của ví dụ này không phải là để cho thấy rằng cây là một mô hình tồi cho chuỗi thời gian, mà là để minh họa một thuộc tính cụ thể về cách cây đưa ra dự đoán."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5947df85",
   "metadata": {},
   "source": [
    "#### Điểm mạnh, điểm yếu và tham số\n",
    "\n",
    "Như đã thảo luận trước đó, các tham số kiểm soát độ phức tạp của mô hình trong cây quyết định là các tham số tỉa trước giúp dừng việc xây dựng cây trước khi nó được phát triển đầy đủ. Thông thường, việc chọn một trong các chiến lược tỉa trước—đặt `max_depth`, `max_leaf_nodes`, hoặc `min_samples_leaf`—là đủ để ngăn chặn việc quá khớp. \n",
    "\n",
    "Cây quyết định có hai ưu điểm so với nhiều thuật toán mà chúng ta đã thảo luận cho đến nay: mô hình kết quả có thể dễ dàng được trực quan hóa và hiểu bởi những người không chuyên (ít nhất là đối với các cây nhỏ hơn), và các thuật toán hoàn toàn bất biến đối với việc chia tỷ lệ dữ liệu. Vì mỗi đặc trưng được xử lý riêng biệt và các lần chia có thể có của dữ liệu không phụ thuộc vào việc chia tỷ lệ, nên không cần tiền xử lý như chuẩn hóa hoặc tiêu chuẩn hóa các đặc trưng cho các thuật toán cây quyết định. Đặc biệt, cây quyết định hoạt động tốt khi bạn có các đặc trưng ở các thang đo hoàn toàn khác nhau, hoặc một hỗn hợp các đặc trưng nhị phân và liên tục. \n",
    "\n",
    "Nhược điểm chính của cây quyết định là ngay cả khi sử dụng tỉa trước, chúng vẫn có xu hướng quá khớp và cung cấp hiệu suất tổng quát hóa kém. Do đó, trong hầu hết các ứng dụng, các phương pháp tập hợp mà chúng ta sẽ thảo luận tiếp theo thường được sử dụng thay cho một cây quyết định duy nhất. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700fe8dc",
   "metadata": {},
   "source": [
    "### Tập hợp cây quyết định\n",
    "\n",
    "Tập hợp là các phương pháp kết hợp nhiều mô hình học máy để tạo ra các mô hình mạnh mẽ hơn. Có nhiều mô hình trong tài liệu học máy thuộc loại này, nhưng có hai mô hình tập hợp đã được chứng minh là hiệu quả trên một loạt các tập dữ liệu cho phân loại và hồi quy, cả hai đều sử dụng cây quyết định làm khối xây dựng của chúng: rừng ngẫu nhiên và cây quyết định tăng cường độ dốc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9154ad2",
   "metadata": {},
   "source": [
    "#### Rừng ngẫu nhiên\n",
    "\n",
    "Như chúng ta vừa quan sát, một nhược điểm chính của cây quyết định là chúng có xu hướng quá khớp dữ liệu huấn luyện. Rừng ngẫu nhiên là một cách để giải quyết vấn đề này. Một khu rừng ngẫu nhiên về cơ bản là một tập hợp các cây quyết định, trong đó mỗi cây hơi khác so với những cây khác. Ý tưởng đằng sau rừng ngẫu nhiên là mỗi cây có thể thực hiện một công việc dự đoán tương đối tốt, nhưng có khả năng sẽ quá khớp trên một phần của dữ liệu. Nếu chúng ta xây dựng nhiều cây, tất cả đều hoạt động tốt và quá khớp theo những cách khác nhau, chúng ta có thể giảm lượng quá khớp bằng cách lấy trung bình kết quả của chúng. Sự giảm thiểu quá khớp này, trong khi vẫn giữ được sức mạnh dự đoán của cây, có thể được chứng minh bằng toán học chặt chẽ. \n",
    "\n",
    "Để thực hiện chiến lược này, chúng ta cần xây dựng nhiều cây quyết định. Mỗi cây phải thực hiện một công việc chấp nhận được trong việc dự đoán mục tiêu, và cũng phải khác với các cây khác. Rừng ngẫu nhiên có tên của chúng từ việc đưa sự ngẫu nhiên vào việc xây dựng cây để đảm bảo mỗi cây là khác nhau. Có hai cách trong đó các cây trong một khu rừng ngẫu nhiên được ngẫu nhiên hóa: bằng cách chọn các điểm dữ liệu được sử dụng để xây dựng một cây và bằng cách chọn các đặc trưng trong mỗi bài kiểm tra chia. Hãy đi sâu vào quá trình này chi tiết hơn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d949f",
   "metadata": {},
   "source": [
    "**Xây dựng rừng ngẫu nhiên.** Để xây dựng một mô hình rừng ngẫu nhiên, bạn cần quyết định số lượng cây cần xây dựng (tham số `n_estimators` của `RandomForestRegressor` hoặc `RandomForestClassifier`). Giả sử chúng ta muốn xây dựng 10 cây. Những cây này sẽ được xây dựng hoàn toàn độc lập với nhau, và thuật toán sẽ đưa ra các lựa chọn ngẫu nhiên khác nhau cho mỗi cây để đảm bảo các cây là khác biệt. Để xây dựng một cây, trước tiên chúng ta lấy cái được gọi là một mẫu bootstrap của dữ liệu của chúng ta. Tức là, từ `n_samples` điểm dữ liệu của chúng ta, chúng ta liên tục rút một ví dụ một cách ngẫu nhiên có thay thế (có nghĩa là cùng một mẫu có thể được chọn nhiều lần), `n_samples` lần. Điều này sẽ tạo ra một tập dữ liệu lớn bằng tập dữ liệu ban đầu, nhưng một số điểm dữ liệu sẽ bị thiếu khỏi nó (khoảng một phần ba), và một số sẽ được lặp lại. \n",
    "\n",
    "Để minh họa, giả sử chúng ta muốn tạo một mẫu bootstrap của danh sách `['a', 'b', 'c', 'd']`. Một mẫu bootstrap có thể là `['b', 'd', 'd', 'c']`. Một mẫu có thể khác là `['d', 'a', 'd', 'a']`. \n",
    "\n",
    "Tiếp theo, một cây quyết định được xây dựng dựa trên tập dữ liệu mới được tạo này. Tuy nhiên, thuật toán chúng ta đã mô tả cho cây quyết định được sửa đổi một chút. Thay vì tìm kiếm bài kiểm tra tốt nhất cho mỗi nút, trong mỗi nút, thuật toán chọn ngẫu nhiên một tập hợp con của các đặc trưng, và nó tìm kiếm bài kiểm tra tốt nhất có thể liên quan đến một trong những đặc trưng này. Số lượng đặc trưng được chọn được kiểm soát bởi tham số `max_features`. Việc lựa chọn một tập hợp con của các đặc trưng này được lặp lại riêng biệt trong mỗi nút, để mỗi nút trong một cây có thể đưa ra quyết định bằng cách sử dụng một tập hợp con khác nhau của các đặc trưng. \n",
    "\n",
    "Việc lấy mẫu bootstrap dẫn đến mỗi cây quyết định trong rừng ngẫu nhiên được xây dựng trên một tập dữ liệu hơi khác nhau. Do việc lựa chọn các đặc trưng trong mỗi nút, mỗi lần chia trong mỗi cây hoạt động trên một tập hợp con khác nhau của các đặc trưng. Cùng với nhau, hai cơ chế này đảm bảo rằng tất cả các cây trong rừng ngẫu nhiên đều khác nhau. \n",
    "\n",
    "Một tham số quan trọng trong quá trình này là `max_features`. Nếu chúng ta đặt `max_features` thành `n_features`, điều đó có nghĩa là mỗi lần chia có thể xem xét tất cả các đặc trưng trong tập dữ liệu, và sẽ không có sự ngẫu nhiên nào được đưa vào việc lựa chọn đặc trưng (sự ngẫu nhiên do bootstrapping vẫn còn). Nếu chúng ta đặt `max_features` thành 1, điều đó có nghĩa là các lần chia không có lựa chọn nào về đặc trưng nào để kiểm tra, và chỉ có thể tìm kiếm trên các ngưỡng khác nhau cho đặc trưng đã được chọn ngẫu nhiên. Do đó, một `max_features` cao có nghĩa là các cây trong rừng ngẫu nhiên sẽ khá giống nhau, và chúng sẽ có thể dễ dàng khớp với dữ liệu, sử dụng các đặc trưng đặc biệt nhất. Một `max_features` thấp có nghĩa là các cây trong rừng ngẫu nhiên sẽ khá khác nhau, và mỗi cây có thể cần phải rất sâu để khớp tốt với dữ liệu. \n",
    "\n",
    "Để đưa ra dự đoán bằng cách sử dụng rừng ngẫu nhiên, thuật toán trước tiên đưa ra dự đoán cho mọi cây trong rừng. Đối với hồi quy, chúng ta có thể lấy trung bình các kết quả này để có được dự đoán cuối cùng của mình. Đối với phân loại, một chiến lược \"bỏ phiếu mềm\" được sử dụng. Điều này có nghĩa là mỗi thuật toán đưa ra một dự đoán \"mềm\", cung cấp một xác suất cho mỗi nhãn đầu ra có thể có. Các xác suất được dự đoán bởi tất cả các cây được lấy trung bình, và lớp có xác suất cao nhất được dự đoán. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bf99da",
   "metadata": {},
   "source": [
    "**Phân tích rừng ngẫu nhiên.** Hãy áp dụng một khu rừng ngẫu nhiên bao gồm năm cây cho tập dữ liệu `two_moons` mà chúng ta đã nghiên cứu trước đó:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f17fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=5, random_state=2)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b602fd1",
   "metadata": {},
   "source": [
    "Các cây được xây dựng như một phần của rừng ngẫu nhiên được lưu trữ trong thuộc tính `estimator_`. Hãy trực quan hóa các ranh giới quyết định được học bởi mỗi cây, cùng với dự đoán tổng hợp của chúng được thực hiện bởi rừng (Hình 2-33):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1952da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\n",
    "    ax.set_title(\"Tree {}\".format(i))\n",
    "    mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax)\n",
    "\n",
    "mglearn.plots.plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1],\n",
    "                                alpha=.4)\n",
    "axes[-1, -1].set_title(\"Random Forest\")\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c214bb1",
   "metadata": {},
   "source": [
    "**Hình 2-33. Ranh giới quyết định được tìm thấy bởi năm cây quyết định ngẫu nhiên và ranh giới quyết định thu được bằng cách lấy trung bình các xác suất dự đoán của chúng**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f78a19",
   "metadata": {},
   "source": [
    "Bạn có thể thấy rõ rằng các ranh giới quyết định được học bởi năm cây khá khác nhau. Mỗi cây đều mắc một số sai lầm, vì một số điểm huấn luyện được vẽ ở đây thực sự không được bao gồm trong các tập huấn luyện của cây, do việc lấy mẫu bootstrap. \n",
    "\n",
    "Rừng ngẫu nhiên ít bị quá khớp hơn bất kỳ cây riêng lẻ nào, và cung cấp một ranh giới quyết định trực quan hơn nhiều. Trong bất kỳ ứng dụng thực tế nào, chúng ta sẽ sử dụng nhiều cây hơn (thường là hàng trăm hoặc hàng nghìn), dẫn đến các ranh giới thậm chí còn mượt mà hơn. \n",
    "\n",
    "Như một ví dụ khác, hãy áp dụng một khu rừng ngẫu nhiên bao gồm 100 cây trên tập dữ liệu Ung thư vú:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c414b39c",
   "metadata": {},
   "source": [
    "Rừng ngẫu nhiên cho chúng ta độ chính xác 97%, tốt hơn các mô hình tuyến tính hoặc một cây quyết định duy nhất, mà không cần điều chỉnh bất kỳ tham số nào. Chúng ta có thể điều chỉnh cài đặt `max_features`, hoặc áp dụng tỉa trước như chúng ta đã làm cho cây quyết định duy nhất. Tuy nhiên, thường thì các tham số mặc định của rừng ngẫu nhiên đã hoạt động khá tốt. \n",
    "\n",
    "Tương tự như cây quyết định, rừng ngẫu nhiên cung cấp tầm quan trọng của đặc trưng, được tính toán bằng cách tổng hợp tầm quan trọng của đặc trưng trên các cây trong rừng. Thông thường, tầm quan trọng của đặc trưng được cung cấp bởi rừng ngẫu nhiên đáng tin cậy hơn so với tầm quan trọng được cung cấp bởi một cây duy nhất. Hãy xem Hình 2-34. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d28ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances_cancer(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2731e7d5",
   "metadata": {},
   "source": [
    "**Hình 2-34. Tầm quan trọng của đặc trưng được tính toán từ một rừng ngẫu nhiên được huấn luyện trên tập dữ liệu Ung thư vú**\n",
    "Như bạn có thể thấy, rừng ngẫu nhiên mang lại tầm quan trọng khác không cho nhiều đặc trưng hơn so với một cây duy nhất. Tương tự như cây quyết định duy nhất, rừng ngẫu nhiên cũng mang lại nhiều tầm quan trọng cho đặc trưng “worst radius”, nhưng nó thực sự chọn “worst perimeter” là đặc trưng có nhiều thông tin nhất nói chung. Sự ngẫu nhiên trong việc xây dựng rừng ngẫu nhiên buộc thuật toán phải xem xét nhiều giải thích có thể, kết quả là rừng ngẫu nhiên nắm bắt được một bức tranh rộng hơn nhiều về dữ liệu so với một cây duy nhất. \n",
    "**Điểm mạnh, điểm yếu và tham số.** Rừng ngẫu nhiên cho hồi quy và phân loại hiện là một trong những phương pháp học máy được sử dụng rộng rãi nhất. Chúng rất mạnh mẽ, thường hoạt động tốt mà không cần điều chỉnh nhiều các tham số, và không yêu cầu chia tỷ lệ dữ liệu. \n",
    "Về cơ bản, rừng ngẫu nhiên có chung tất cả các lợi ích của cây quyết định, trong khi bù đắp cho một số thiếu sót của chúng. Một lý do để vẫn sử dụng cây quyết định là nếu bạn cần một biểu diễn nhỏ gọn của quá trình ra quyết định. Về cơ bản, không thể giải thích chi tiết hàng chục hoặc hàng trăm cây, và các cây trong rừng ngẫu nhiên có xu hướng sâu hơn cây quyết định (do việc sử dụng các tập hợp con đặc trưng). Do đó, nếu bạn cần tóm tắt việc ra quyết định theo cách trực quan cho những người không chuyên, một cây quyết định duy nhất có thể là một lựa chọn tốt hơn. Mặc dù việc xây dựng rừng ngẫu nhiên trên các tập dữ liệu lớn có thể hơi tốn thời gian, nhưng nó có thể được song song hóa trên nhiều lõi CPU trong một máy tính. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f22b37d",
   "metadata": {},
   "source": [
    "Nếu bạn đang sử dụng một bộ xử lý đa lõi (như hầu hết các máy tính hiện đại đều có), bạn có thể sử dụng tham số `n_jobs` để điều chỉnh số lượng lõi cần sử dụng. Việc sử dụng nhiều lõi CPU hơn sẽ dẫn đến việc tăng tốc tuyến tính (sử dụng hai lõi, việc huấn luyện rừng ngẫu nhiên sẽ nhanh gấp đôi), nhưng việc chỉ định `n_jobs` lớn hơn số lượng lõi sẽ không hữu ích. Bạn có thể đặt `n_jobs=-1` để sử dụng tất cả các lõi trong máy tính của mình. \n",
    "\n",
    "Bạn nên nhớ rằng rừng ngẫu nhiên, về bản chất, là ngẫu nhiên, và việc đặt các trạng thái ngẫu nhiên khác nhau (hoặc hoàn toàn không đặt `random_state`) có thể thay đổi đáng kể mô hình được xây dựng. Càng có nhiều cây trong rừng, nó sẽ càng mạnh mẽ hơn trước sự lựa chọn của trạng thái ngẫu nhiên. Nếu bạn muốn có kết quả có thể tái tạo, điều quan trọng là phải cố định `random_state`. \n",
    "\n",
    "Rừng ngẫu nhiên không có xu hướng hoạt động tốt trên dữ liệu thưa thớt, có chiều rất cao, chẳng hạn như dữ liệu văn bản. Đối với loại dữ liệu này, các mô hình tuyến tính có thể phù hợp hơn. Rừng ngẫu nhiên thường hoạt động tốt ngay cả trên các tập dữ liệu rất lớn, và việc huấn luyện có thể dễ dàng được song song hóa trên nhiều lõi CPU trong một máy tính mạnh mẽ. Tuy nhiên, rừng ngẫu nhiên yêu cầu nhiều bộ nhớ hơn và chậm hơn để huấn luyện và dự đoán so với các mô hình tuyến tính. Nếu thời gian và bộ nhớ là quan trọng trong một ứng dụng, có thể có ý nghĩa hơn khi sử dụng một mô hình tuyến tính thay thế. \n",
    "\n",
    "Các tham số quan trọng cần điều chỉnh là `n_estimators`, `max_features`, và có thể là các tùy chọn tỉa trước như `max_depth`. Đối với `n_estimators`, lớn hơn luôn tốt hơn. Việc lấy trung bình nhiều cây hơn sẽ mang lại một tập hợp mạnh mẽ hơn bằng cách giảm việc quá khớp. Tuy nhiên, có những lợi ích giảm dần, và nhiều cây hơn cần nhiều bộ nhớ hơn và nhiều thời gian hơn để huấn luyện. Một quy tắc chung phổ biến là xây dựng \"nhiều nhất có thể trong thời gian/bộ nhớ cho phép\". \n",
    "\n",
    "Như đã mô tả trước đó, `max_features` xác định mức độ ngẫu nhiên của mỗi cây, và một `max_features` nhỏ hơn sẽ làm giảm việc quá khớp. Nói chung, một quy tắc chung tốt là sử dụng các giá trị mặc định: `max_features=sqrt(n_features)` cho phân loại và `max_features=log2(n_features)` cho hồi quy. Việc thêm `max_features` hoặc `max_leaf_nodes` đôi khi có thể cải thiện hiệu suất. Nó cũng có thể giảm đáng kể yêu cầu về không gian và thời gian để huấn luyện và dự đoán. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80858456",
   "metadata": {},
   "source": [
    "#### Cây hồi quy tăng cường độ dốc (máy tăng cường độ dốc)\n",
    "\n",
    "Cây hồi quy tăng cường độ dốc là một phương pháp tập hợp khác kết hợp nhiều cây quyết định để tạo ra một mô hình mạnh mẽ hơn. Mặc dù có từ \"hồi quy\" trong tên, các mô hình này có thể được sử dụng cho hồi quy và phân loại. Trái ngược với phương pháp rừng ngẫu nhiên, tăng cường độ dốc hoạt động bằng cách xây dựng các cây theo kiểu tuần tự, trong đó mỗi cây cố gắng sửa chữa những sai lầm của cây trước đó. Theo mặc định, không có sự ngẫu nhiên trong các cây hồi quy tăng cường độ dốc; thay vào đó, việc tỉa trước mạnh mẽ được sử dụng. Các cây tăng cường độ dốc thường sử dụng các cây rất nông, có độ sâu từ một đến năm, điều này làm cho mô hình nhỏ hơn về bộ nhớ và giúp dự đoán nhanh hơn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adfddf1",
   "metadata": {},
   "source": [
    "Ý tưởng chính đằng sau việc tăng cường độ dốc là kết hợp nhiều mô hình đơn giản (trong ngữ cảnh này được gọi là các bộ học yếu), như các cây nông. Mỗi cây chỉ có thể cung cấp các dự đoán tốt trên một phần của dữ liệu, và do đó, ngày càng có nhiều cây được thêm vào để cải thiện hiệu suất một cách lặp đi lặp lại. \n",
    "\n",
    "Các cây tăng cường độ dốc thường là các mục chiến thắng trong các cuộc thi học máy và được sử dụng rộng rãi trong ngành công nghiệp. Chúng thường nhạy cảm hơn một chút với các cài đặt tham số so với rừng ngẫu nhiên, nhưng có thể cung cấp độ chính xác tốt hơn nếu các tham số được đặt chính xác. \n",
    "\n",
    "Ngoài việc tỉa trước và số lượng cây trong tập hợp, một tham số quan trọng khác của việc tăng cường độ dốc là `learning_rate`, kiểm soát mức độ mạnh mẽ mà mỗi cây cố gắng sửa chữa những sai lầm của các cây trước đó. Một `learning_rate` cao hơn có nghĩa là mỗi cây có thể thực hiện các hiệu chỉnh mạnh mẽ hơn, cho phép các mô hình phức tạp hơn. Việc thêm nhiều cây hơn vào tập hợp, có thể được thực hiện bằng cách tăng `n_estimators`, cũng làm tăng độ phức tạp của mô hình, vì mô hình có nhiều cơ hội hơn để sửa chữa những sai lầm trên tập huấn luyện. \n",
    "\n",
    "Đây là một ví dụ về việc sử dụng `GradientBoostingClassifier` trên tập dữ liệu Ung thư vú. Theo mặc định, 100 cây có độ sâu tối đa là 3 và tốc độ học là 0,1 được sử dụng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2d8f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "gbrt = GradientBoostingClassifier(random_state=0)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc55a2c",
   "metadata": {},
   "source": [
    "Vì độ chính xác của tập huấn luyện là 100%, chúng ta có khả năng bị quá khớp. Để giảm thiểu tình trạng quá khớp, chúng ta có thể áp dụng tỉa trước mạnh hơn bằng cách giới hạn độ sâu tối đa hoặc giảm tốc độ học:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950dff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff0df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f6afc",
   "metadata": {},
   "source": [
    "Cả hai phương pháp giảm độ phức tạp của mô hình đều làm giảm độ chính xác của tập huấn luyện, như mong đợi. Trong trường hợp này, việc giảm độ sâu tối đa của cây đã mang lại một sự cải thiện đáng kể cho mô hình, trong khi việc giảm tốc độ học chỉ làm tăng nhẹ hiệu suất tổng quát hóa. \n",
    "\n",
    "Đối với các mô hình dựa trên cây quyết định khác, chúng ta có thể một lần nữa trực quan hóa tầm quan trọng của đặc trưng để có được cái nhìn sâu sắc hơn về mô hình của mình (Hình 2-35). Vì chúng ta đã sử dụng 100 cây, nên việc kiểm tra tất cả chúng là không thực tế, ngay cả khi tất cả chúng đều có độ sâu 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "plot_feature_importances_cancer(gbrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948ed29",
   "metadata": {},
   "source": [
    "**Hình 2-35. Tầm quan trọng của đặc trưng được tính toán từ một bộ phân loại tăng cường độ dốc được huấn luyện trên tập dữ liệu Ung thư vú**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b70731f",
   "metadata": {},
   "source": [
    "Chúng ta có thể thấy rằng tầm quan trọng của các đặc trưng của các cây tăng cường độ dốc hơi giống với tầm quan trọng của các đặc trưng của các khu rừng ngẫu nhiên, mặc dù việc tăng cường độ dốc đã hoàn toàn bỏ qua một số đặc trưng. \n",
    "\n",
    "Vì cả việc tăng cường độ dốc và rừng ngẫu nhiên đều hoạt động tốt trên các loại dữ liệu tương tự, một cách tiếp cận phổ biến là trước tiên hãy thử rừng ngẫu nhiên, hoạt động khá mạnh mẽ. Nếu rừng ngẫu nhiên hoạt động tốt nhưng thời gian dự đoán là ưu tiên hàng đầu, hoặc điều quan trọng là phải vắt kiệt phần trăm độ chính xác cuối cùng từ mô hình học máy, thì việc chuyển sang tăng cường độ dốc thường hữu ích. \n",
    "\n",
    "Nếu bạn muốn áp dụng việc tăng cường độ dốc cho một bài toán quy mô lớn, có thể đáng để xem xét gói `xgboost` và giao diện Python của nó, mà tại thời điểm viết bài này, nhanh hơn (và đôi khi dễ điều chỉnh hơn) so với việc triển khai việc tăng cường độ dốc của scikit-learn trên nhiều tập dữ liệu. \n",
    "\n",
    "**Điểm mạnh, điểm yếu và tham số.** Cây quyết định tăng cường độ dốc là một trong những mô hình mạnh mẽ và được sử dụng rộng rãi nhất cho học có giám sát. Nhược điểm chính của chúng là chúng yêu cầu điều chỉnh cẩn thận các tham số và có thể mất nhiều thời gian để huấn luyện. Tương tự như các mô hình dựa trên cây khác, thuật toán hoạt động tốt mà không cần chia tỷ lệ và trên một hỗn hợp các đặc trưng nhị phân và liên tục. Cũng như các mô hình dựa trên cây khác, nó cũng thường không hoạt động tốt trên dữ liệu thưa thớt có chiều cao. \n",
    "\n",
    "Các tham số chính của các mô hình cây tăng cường độ dốc là số lượng cây, `n_estimators`, và `learning_rate`, kiểm soát mức độ mà mỗi cây được phép sửa chữa những sai lầm của các cây trước đó. Hai tham số này có mối liên hệ mật thiết với nhau, vì một `learning_rate` thấp hơn có nghĩa là cần nhiều cây hơn để xây dựng một mô hình có độ phức tạp tương tự. Trái ngược với rừng ngẫu nhiên, nơi một giá trị `n_estimators` cao hơn luôn tốt hơn, việc tăng `n_estimators` trong việc tăng cường độ dốc dẫn đến một mô hình phức tạp hơn, có thể dẫn đến việc quá khớp. Một thực hành phổ biến là điều chỉnh `n_estimators` tùy thuộc vào ngân sách thời gian và bộ nhớ, và sau đó tìm kiếm trên các `learning_rate` khác nhau. \n",
    "\n",
    "Một tham số quan trọng khác là `max_depth` (hoặc `max_leaf_nodes`), để giảm độ phức tạp của mỗi cây. Thông thường `max_depth` được đặt rất thấp cho các mô hình tăng cường độ dốc, thường không sâu hơn năm lần chia. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3140238",
   "metadata": {},
   "source": [
    "### Máy véc-tơ hỗ trợ hạt nhân hóa\n",
    "\n",
    "Loại mô hình có giám sát tiếp theo mà chúng ta sẽ thảo luận là máy véc-tơ hỗ trợ hạt nhân hóa. Chúng ta đã khám phá việc sử dụng máy véc-tơ hỗ trợ tuyến tính cho phân loại trong “Mô hình tuyến tính cho phân loại” trên trang 56. Máy véc-tơ hỗ trợ hạt nhân hóa (thường chỉ được gọi là SVM) là một phần mở rộng cho phép các mô hình phức tạp hơn không được định nghĩa đơn giản bởi các siêu phẳng trong không gian đầu vào. Mặc dù có các máy véc-tơ hỗ trợ cho phân loại và hồi quy, chúng ta sẽ giới hạn mình trong trường hợp phân loại, như được triển khai trong `SVC`. Các khái niệm tương tự áp dụng cho hồi quy véc-tơ hỗ trợ, như được triển khai trong `SVR`. \n",
    "\n",
    "Toán học đằng sau máy véc-tơ hỗ trợ hạt nhân hóa hơi phức tạp và nằm ngoài phạm vi của cuốn sách này. Bạn có thể tìm thấy chi tiết trong Chương 1 của cuốn *The Elements of Statistical Learning* của Hastie, Tibshirani và Friedman. Tuy nhiên, chúng tôi sẽ cố gắng cung cấp cho bạn một số ý tưởng về phương pháp này. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b5545",
   "metadata": {},
   "source": [
    "#### Mô hình tuyến tính và các đặc trưng phi tuyến\n",
    "\n",
    "Như bạn đã thấy trong Hình 2-15, các mô hình tuyến tính có thể khá hạn chế trong các không gian có chiều thấp, vì các đường thẳng và siêu phẳng có tính linh hoạt hạn chế. Một cách để làm cho một mô hình tuyến tính linh hoạt hơn là bằng cách thêm nhiều đặc trưng hơn—ví dụ, bằng cách thêm các tương tác hoặc đa thức của các đặc trưng đầu vào. \n",
    "\n",
    "Hãy xem xét tập dữ liệu tổng hợp mà chúng ta đã sử dụng trong “Tầm quan trọng của đặc trưng trong cây” trên trang 77 (xem Hình 2-29): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f46ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(centers=4, random_state=8)\n",
    "y = y % 2\n",
    "\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de41fc7c",
   "metadata": {},
   "source": [
    "**Hình 2-36. Tập dữ liệu phân loại hai lớp trong đó các lớp không thể phân tách tuyến tính**\n",
    "Một mô hình tuyến tính cho phân loại chỉ có thể tách các điểm bằng một đường thẳng và sẽ không thể thực hiện tốt trên tập dữ liệu này (xem Hình 2-37): \n",
    "\n",
    "--- \n",
    "¹⁰Chúng tôi đã chọn đặc trưng cụ thể này để thêm vào cho mục đích minh họa. Sự lựa chọn không đặc biệt quan trọng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da25ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linear_svm = LinearSVC().fit(X, y)\n",
    "\n",
    "mglearn.plots.plot_2d_separator(linear_svm, X)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52deaba9",
   "metadata": {},
   "source": [
    "**Hình 2-37. Ranh giới quyết định được tìm thấy bởi một SVM tuyến tính**\n",
    "Bây giờ hãy mở rộng tập hợp các đặc trưng đầu vào, chẳng hạn bằng cách thêm `feature1 ** 2`, bình phương của đặc trưng thứ hai, làm một đặc trưng mới. Thay vì biểu diễn mỗi điểm dữ liệu dưới dạng một điểm hai chiều, `(feature0, feature1)`, bây giờ chúng ta biểu diễn nó dưới dạng một điểm ba chiều, `(feature0, feature1, feature1 ** 2)`.¹⁰ Biểu diễn mới này được minh họa trong Hình 2-38 trong một biểu đồ phân tán ba chiều: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06f0610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thêm đặc trưng đầu tiên bình phương\n",
    "X_new = np.hstack([X, X[:, 1:] ** 2])\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D, axes3d\n",
    "figure = plt.figure()\n",
    "# trực quan hóa trong 3D\n",
    "ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "# vẽ tất cả các điểm có y == 0 trước, sau đó là tất cả các điểm có y == 1\n",
    "mask = y == 0\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "           cmap=mglearn.cm2, s=60)\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
    "           cmap=mglearn.cm2, s=60)\n",
    "ax.set_xlabel(\"feature0\")\n",
    "ax.set_ylabel(\"feature1\")\n",
    "ax.set_zlabel(\"feature1 ** 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f854dc8a",
   "metadata": {},
   "source": [
    "**Hình 2-38. Mở rộng tập dữ liệu được hiển thị trong Hình 2-37, được tạo bằng cách thêm một đặc trưng thứ ba có nguồn gốc từ feature1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2657572",
   "metadata": {},
   "source": [
    "Trong biểu diễn mới của dữ liệu, bây giờ thực sự có thể tách hai lớp bằng một mô hình tuyến tính, một mặt phẳng trong không gian ba chiều. Chúng ta có thể xác nhận điều này bằng cách điều chỉnh một mô hình tuyến tính cho dữ liệu được tăng cường (xem Hình 2-39):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def17675",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm_3d = LinearSVC().fit(X_new, y)\n",
    "coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n",
    "\n",
    "# hiển thị ranh giới quyết định tuyến tính\n",
    "figure = plt.figure()\n",
    "ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "xx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\n",
    "yy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\n",
    "\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\n",
    "ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "           cmap=mglearn.cm2, s=60)\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
    "           cmap=mglearn.cm2, s=60)\n",
    "\n",
    "ax.set_xlabel(\"feature0\")\n",
    "ax.set_ylabel(\"feature1\")\n",
    "ax.set_zlabel(\"feature0 ** 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a4b46",
   "metadata": {},
   "source": [
    "**Hình 2-39. Ranh giới quyết định được tìm thấy bởi một SVM tuyến tính trên tập dữ liệu ba chiều được mở rộng**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d7640c",
   "metadata": {},
   "source": [
    "Là một hàm của các đặc trưng ban đầu, mô hình SVM tuyến tính không thực sự tuyến tính nữa. Nó không phải là một đường thẳng, mà giống một hình elip hơn, như bạn có thể thấy từ biểu đồ được tạo ở đây (Hình 2-40):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deac529",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZZ = YY ** 2\n",
    "dec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\n",
    "plt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\n",
    "             cmap=mglearn.cm2, alpha=0.5)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9360118",
   "metadata": {},
   "source": [
    "**Hình 2-40. Ranh giới quyết định từ Hình 2-39 như một hàm của hai đặc trưng ban đầu**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3b865e",
   "metadata": {},
   "source": [
    "#### Thủ thuật hạt nhân\n",
    "\n",
    "Bài học ở đây là việc thêm các đặc trưng phi tuyến vào biểu diễn của dữ liệu của chúng ta có thể làm cho các mô hình tuyến tính mạnh mẽ hơn nhiều. Tuy nhiên, thường thì chúng ta không biết nên thêm đặc trưng nào, và việc thêm nhiều đặc trưng (như tất cả các tương tác có thể có trong một không gian đặc trưng 100 chiều) có thể làm cho việc tính toán rất tốn kém. May mắn thay, có một thủ thuật toán học thông minh cho phép chúng ta học một bộ phân loại trong một không gian có chiều cao hơn mà không thực sự tính toán biểu diễn mới, có thể rất lớn. Điều này được gọi là thủ thuật hạt nhân, và nó hoạt động bằng cách tính toán trực tiếp khoảng cách (chính xác hơn là các tích vô hướng) của các điểm dữ liệu cho biểu diễn đặc trưng được mở rộng, mà không bao giờ thực sự tính toán sự mở rộng. \n",
    "\n",
    "Có hai cách để ánh xạ dữ liệu của bạn vào một không gian có chiều cao hơn thường được sử dụng với máy véc-tơ hỗ trợ: hạt nhân đa thức, tính toán tất cả các đa thức có thể có đến một bậc nhất định của các đặc trưng ban đầu (như `feature1 ** 2 * feature2 ** 5`); và hạt nhân hàm cơ sở bán kính (RBF), còn được gọi là hạt nhân Gaussian. Hạt nhân Gaussian hơi khó giải thích hơn một chút, vì nó tương ứng với một không gian đặc trưng có chiều vô hạn. Một cách để giải thích hạt nhân Gaussian là nó xem xét tất cả các đa thức có thể có của tất cả các bậc, nhưng tầm quan trọng của các đặc trưng giảm dần đối với các bậc cao hơn.¹¹ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d7414",
   "metadata": {},
   "source": [
    "Trên thực tế, các chi tiết toán học đằng sau SVM hạt nhân không quan trọng đến vậy, và cách một SVM với hạt nhân RBF đưa ra quyết định có thể được tóm tắt khá dễ dàng—chúng ta sẽ làm như vậy trong phần tiếp theo.\n",
    "\n",
    "--- \n",
    "¹¹Điều này xuất phát từ khai triển Taylor của ánh xạ mũ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd5b3c",
   "metadata": {},
   "source": [
    "#### Hiểu về SVM\n",
    "\n",
    "Trong quá trình huấn luyện, SVM học được mức độ quan trọng của mỗi điểm dữ liệu huấn luyện để biểu diễn ranh giới quyết định giữa hai lớp. Thông thường chỉ có một tập hợp con của các điểm huấn luyện quan trọng để xác định ranh giới quyết định: những điểm nằm trên ranh giới giữa các lớp. Chúng được gọi là các véc-tơ hỗ trợ và đặt tên cho máy véc-tơ hỗ trợ. \n",
    "\n",
    "Để đưa ra dự đoán cho một điểm mới, khoảng cách đến mỗi véc-tơ hỗ trợ được đo. Một quyết định phân loại được đưa ra dựa trên khoảng cách đến véc-tơ hỗ trợ và tầm quan trọng của các véc-tơ hỗ trợ đã được học trong quá trình huấn luyện (được lưu trữ trong thuộc tính `dual_coef_` của `SVC`). \n",
    "\n",
    "Khoảng cách giữa các điểm dữ liệu được đo bằng hạt nhân Gaussian:\n",
    "$k_{rbf}(x_1, x_2) = \\exp(-\\gamma ||x_1 - x_2||^2)$\n",
    "Ở đây, $x_1$ và $x_2$ là các điểm dữ liệu, $||x_1 - x_2||$ biểu thị khoảng cách Euclide, và $\\gamma$ (gamma) là một tham số kiểm soát độ rộng của hạt nhân Gaussian. \n",
    "\n",
    "Hình 2-41 cho thấy kết quả của việc huấn luyện một máy véc-tơ hỗ trợ trên một tập dữ liệu hai chiều hai lớp. Ranh giới quyết định được hiển thị bằng màu đen, và các véc-tơ hỗ trợ là các điểm lớn hơn có đường viền rộng. Đoạn mã sau đây tạo ra biểu đồ này bằng cách huấn luyện một SVM trên tập dữ liệu forge: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a642b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = mglearn.tools.make_handcrafted_dataset()\n",
    "svm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)\n",
    "mglearn.plots.plot_2d_separator(svm, X, eps=.5)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "# vẽ các véc-tơ hỗ trợ\n",
    "sv = svm.support_vectors_\n",
    "# các nhãn lớp của các véc-tơ hỗ trợ được cho bởi dấu của các hệ số kép\n",
    "sv_labels = svm.dual_coef_.ravel() > 0\n",
    "mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a3e0b",
   "metadata": {},
   "source": [
    "**Hình 2-41. Ranh giới quyết định và các véc-tơ hỗ trợ được tìm thấy bởi một SVM với hạt nhân RBF**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0d8e4",
   "metadata": {},
   "source": [
    "Trong trường hợp này, SVM mang lại một ranh giới rất mượt mà và phi tuyến (không phải là một đường thẳng). Chúng ta đã điều chỉnh hai tham số ở đây: tham số `C` và tham số `gamma`, mà chúng ta sẽ thảo luận chi tiết ngay bây giờ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649dd293",
   "metadata": {},
   "source": [
    "#### Điều chỉnh các tham số SVM\n",
    "Tham số `gamma` là tham số được hiển thị trong công thức được đưa ra trong phần trước, kiểm soát độ rộng của hạt nhân Gaussian. Nó xác định thang đo của ý nghĩa các điểm gần nhau. Tham số `C` là một tham số chính quy hóa, tương tự như tham số được sử dụng trong các mô hình tuyến tính. Nó giới hạn tầm quan trọng của mỗi điểm (hoặc chính xác hơn là `dual_coef_` của chúng). \n",
    "\n",
    "Hãy xem điều gì sẽ xảy ra khi chúng ta thay đổi các tham số này (Hình 2-42): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbebbabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "\n",
    "for ax, C in zip(axes, [-1, 0, 3]):\n",
    "    for a, gamma in zip(ax, range(-1, 2)):\n",
    "        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\n",
    "\n",
    "axes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"],\n",
    "                  ncol=4, loc=(.9, 1.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f0f74",
   "metadata": {},
   "source": [
    "**Hình 2-42. Ranh giới quyết định và các véc-tơ hỗ trợ cho các cài đặt khác nhau của các tham số C và gamma**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097cb21f",
   "metadata": {},
   "source": [
    "Đi từ trái sang phải, chúng ta tăng giá trị của tham số `gamma` từ 0,1 đến 10. Một `gamma` nhỏ có nghĩa là một bán kính lớn cho hạt nhân Gaussian, có nghĩa là nhiều điểm được coi là gần nhau. Điều này được phản ánh trong các ranh giới quyết định rất mượt mà ở bên trái, và các ranh giới tập trung nhiều hơn vào các điểm đơn lẻ ở phía bên phải. Một giá trị `gamma` thấp có nghĩa là ranh giới quyết định sẽ thay đổi chậm, mang lại một mô hình có độ phức tạp thấp, trong khi một giá trị `gamma` cao mang lại một mô hình phức tạp hơn. \n",
    "\n",
    "Đi từ trên xuống dưới, chúng ta tăng tham số `C` từ 0,1 đến 1000. Như với các mô hình tuyến tính, một `C` nhỏ có nghĩa là một mô hình rất bị hạn chế, trong đó mỗi điểm dữ liệu chỉ có thể có ảnh hưởng rất hạn chế. Bạn có thể thấy rằng ở phía trên bên trái, ranh giới quyết định trông gần như tuyến tính, với các điểm bị phân loại sai hầu như không có ảnh hưởng gì đến đường thẳng. Việc tăng `C`, như được hiển thị ở phía dưới bên phải, cho phép các điểm này có ảnh hưởng mạnh hơn đến mô hình và làm cho ranh giới quyết định uốn cong để phân loại chúng một cách chính xác. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3896542",
   "metadata": {},
   "source": [
    "Hãy áp dụng SVM hạt nhân RBF cho tập dữ liệu Ung thư vú. Theo mặc định, `C=1` và `gamma=1/n_features`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ecb564",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c20f7a6",
   "metadata": {},
   "source": [
    "Mô hình bị quá khớp khá đáng kể, với điểm số hoàn hảo trên tập huấn luyện và chỉ có độ chính xác 63% trên tập kiểm tra. Mặc dù SVM thường hoạt động khá tốt, chúng rất nhạy cảm với các cài đặt của các tham số và với việc chia tỷ lệ dữ liệu. Đặc biệt, chúng yêu cầu tất cả các đặc trưng phải thay đổi trên một thang đo tương tự. Hãy xem xét các giá trị tối thiểu và tối đa cho mỗi đặc trưng, được vẽ trong không gian log (Hình 2-43): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4651a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train.min(axis=0), 'o', label=\"min\")\n",
    "plt.plot(X_train.max(axis=0), '^', label=\"max\")\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"Feature magnitude\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cae975",
   "metadata": {},
   "source": [
    "**Hình 2-43. Phạm vi đặc trưng cho tập dữ liệu Ung thư vú (lưu ý rằng trục y có thang đo logarit)**\n",
    "Từ biểu đồ này, chúng ta có thể xác định rằng các đặc trưng trong tập dữ liệu Ung thư vú có các bậc độ lớn hoàn toàn khác nhau. Điều này có thể là một vấn đề đối với các mô hình khác (như các mô hình tuyến tính), nhưng nó có tác động tàn phá đối với SVM hạt nhân. Hãy xem xét một số cách để giải quyết vấn đề này. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6122ad6f",
   "metadata": {},
   "source": [
    "**Tiền xử lý dữ liệu cho SVM.** Một cách để giải quyết vấn đề này là bằng cách chia lại tỷ lệ mỗi đặc trưng để chúng đều xấp xỉ trên cùng một thang đo. Một phương pháp chia lại tỷ lệ phổ biến cho SVM hạt nhân là chia tỷ lệ dữ liệu sao cho tất cả các đặc trưng đều nằm trong khoảng từ 0 đến 1. Chúng ta sẽ xem cách thực hiện điều này bằng cách sử dụng phương pháp tiền xử lý `MinMaxScaler` trong Chương 3, nơi chúng ta sẽ cung cấp thêm chi tiết. Hiện tại, hãy thực hiện điều này \"bằng tay\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0116fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tính toán giá trị tối thiểu cho mỗi đặc trưng trên tập huấn luyện\n",
    "min_on_training = X_train.min(axis=0)\n",
    "# tính toán phạm vi của mỗi đặc trưng (max - min) trên tập huấn luyện\n",
    "range_on_training = (X_train - min_on_training).max(axis=0)\n",
    "\n",
    "# trừ đi min, và chia cho phạm vi\n",
    "# sau đó, min=0 và max=1 cho mỗi đặc trưng\n",
    "X_train_scaled = (X_train - min_on_training) / range_on_training\n",
    "\n",
    "print(\"Minimum for each feature\\n{}\".format(X_train_scaled.min(axis=0)))\n",
    "print(\"Maximum for each feature\\n {}\".format(X_train_scaled.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca0879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sử dụng CÙNG một phép biến đổi trên tập kiểm tra,\n",
    "# sử dụng min và phạm vi của tập huấn luyện (xem Chương 3 để biết chi tiết)\n",
    "X_test_scaled = (X_test - min_on_training) / range_on_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    svc.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa216d",
   "metadata": {},
   "source": [
    "Việc chia tỷ lệ dữ liệu đã tạo ra một sự khác biệt lớn! Bây giờ chúng ta thực sự đang ở trong một chế độ thiếu khớp, nơi hiệu suất của tập huấn luyện và tập kiểm tra khá giống nhau nhưng ít gần với độ chính xác 100%. Từ đây, chúng ta có thể thử tăng `C` hoặc `gamma` để phù hợp với một mô hình phức tạp hơn. Ví dụ: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3872df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=1000)\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    svc.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751ae91c",
   "metadata": {},
   "source": [
    "Ở đây, việc tăng `C` cho phép chúng ta cải thiện mô hình một cách đáng kể, dẫn đến độ chính xác 97,2%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d03e96",
   "metadata": {},
   "source": [
    "#### Điểm mạnh, điểm yếu và tham số\n",
    "\n",
    "Máy véc-tơ hỗ trợ hạt nhân hóa là các mô hình mạnh mẽ và hoạt động tốt trên nhiều tập dữ liệu khác nhau. SVM cho phép các ranh giới quyết định phức tạp, ngay cả khi dữ liệu chỉ có một vài đặc trưng. Chúng hoạt động tốt trên dữ liệu có chiều thấp và chiều cao (tức là, ít và nhiều đặc trưng), nhưng không mở rộng tốt với số lượng mẫu. Việc chạy một SVM trên dữ liệu có tới 10.000 mẫu có thể hoạt động tốt, nhưng việc làm việc với các tập dữ liệu có kích thước 100.000 trở lên có thể trở nên khó khăn về thời gian chạy và sử dụng bộ nhớ. \n",
    "\n",
    "Một nhược điểm khác của SVM là chúng yêu cầu tiền xử lý cẩn thận dữ liệu và điều chỉnh các tham số. Đây là lý do tại sao, ngày nay, hầu hết mọi người thay vào đó sử dụng các mô hình dựa trên cây như rừng ngẫu nhiên hoặc tăng cường độ dốc (yêu cầu ít hoặc không cần tiền xử lý) trong nhiều ứng dụng. Hơn nữa, các mô hình SVM khó kiểm tra; có thể khó hiểu tại sao một dự đoán cụ thể được đưa ra, và có thể khó giải thích mô hình cho một người không chuyên. \n",
    "\n",
    "Tuy nhiên, có thể đáng để thử SVM, đặc biệt nếu tất cả các đặc trưng của bạn đại diện cho các phép đo trong các đơn vị tương tự (ví dụ, tất cả đều là cường độ pixel) và chúng ở các thang đo tương tự. \n",
    "\n",
    "Các tham số quan trọng trong SVM hạt nhân là tham số chính quy hóa `C`, lựa chọn hạt nhân và các tham số cụ thể của hạt nhân. Mặc dù chúng ta chủ yếu tập trung vào hạt nhân RBF, các lựa chọn khác có sẵn trong scikit-learn. Hạt nhân RBF chỉ có một tham số, `gamma`, là nghịch đảo của độ rộng của hạt nhân Gaussian. `gamma` và `C` đều kiểm soát độ phức tạp của mô hình, với các giá trị lớn trong cả hai dẫn đến một mô hình phức tạp hơn. Do đó, các cài đặt tốt cho hai tham số thường có mối tương quan mạnh mẽ, và `C` và `gamma` nên được điều chỉnh cùng nhau. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30766f7",
   "metadata": {},
   "source": [
    "### Mạng nơ-ron (Học sâu)\n",
    "\n",
    "Một họ các thuật toán được gọi là mạng nơ-ron gần đây đã có sự hồi sinh dưới cái tên \"học sâu\". Mặc dù học sâu cho thấy nhiều hứa hẹn trong nhiều ứng dụng học máy, các thuật toán học sâu thường được điều chỉnh rất cẩn thận cho một trường hợp sử dụng cụ thể. Ở đây, chúng ta sẽ chỉ thảo luận về một số phương pháp tương đối đơn giản, cụ thể là perceptron đa lớp cho phân loại và hồi quy, có thể đóng vai trò là điểm khởi đầu cho các phương pháp học sâu phức tạp hơn. Perceptron đa lớp (MLP) còn được gọi là mạng nơ-ron truyền thẳng (vanilla), hoặc đôi khi chỉ là mạng nơ-ron. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93845fb1",
   "metadata": {},
   "source": [
    "#### Mô hình mạng nơ-ron\n",
    "\n",
    "MLP có thể được xem như là sự tổng quát hóa của các mô hình tuyến tính thực hiện nhiều giai đoạn xử lý để đi đến một quyết định.\n",
    "\n",
    "Hãy nhớ rằng dự đoán của một bộ hồi quy tuyến tính được cho bởi:\n",
    "\n",
    "$\\hat{y} = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b$\n",
    "\n",
    "Nói một cách đơn giản, ŷ là một tổng có trọng số của các đặc trưng đầu vào từ x[0] đến x[p], được gia quyền bởi các hệ số đã học từ w[0] đến w[p]. Chúng ta có thể trực quan hóa điều này một cách đồ họa như được hiển thị trong Hình 2-44:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eec12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mglearn.plots.plot_logistic_regression_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb13795",
   "metadata": {},
   "source": [
    "**Hình 2-44. Trực quan hóa hồi quy logistic, trong đó các đặc trưng đầu vào và dự đoán được hiển thị dưới dạng các nút, và các hệ số là các kết nối giữa các nút**\n",
    "Ở đây, mỗi nút ở bên trái đại diện cho một đặc trưng đầu vào, các đường nối đại diện cho các hệ số đã học, và nút ở bên phải đại diện cho đầu ra, là một tổng có trọng số của các đầu vào. \n",
    "\n",
    "Trong một MLP, quá trình tính toán các tổng có trọng số này được lặp lại nhiều lần, đầu tiên là tính toán các đơn vị ẩn đại diện cho một bước xử lý trung gian, sau đó lại được kết hợp bằng cách sử dụng các tổng có trọng số để mang lại kết quả cuối cùng (Hình 2-45): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99123c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mglearn.plots.plot_single_hidden_layer_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f2bd16",
   "metadata": {},
   "source": [
    "**Hình 2-45. Minh họa một perceptron đa lớp với một lớp ẩn duy nhất**\n",
    "Mô hình này có nhiều hệ số hơn (còn được gọi là trọng số) để học: có một hệ số giữa mỗi đầu vào và mỗi đơn vị ẩn (tạo nên lớp ẩn), và một hệ số giữa mỗi đơn vị trong lớp ẩn và đầu ra. \n",
    "\n",
    "Việc tính toán một chuỗi các tổng có trọng số về mặt toán học cũng giống như việc chỉ tính toán một tổng có trọng số, vì vậy để làm cho mô hình này thực sự mạnh hơn một mô hình tuyến tính, chúng ta cần một thủ thuật bổ sung. Sau khi tính toán một tổng có trọng số cho mỗi đơn vị ẩn, một hàm phi tuyến được áp dụng cho kết quả—thường là hàm phi tuyến chỉnh lưu (còn được gọi là đơn vị tuyến tính chỉnh lưu hoặc relu) hoặc tang hyperbol (tanh). Kết quả của hàm này sau đó được sử dụng trong tổng có trọng số tính toán đầu ra, ŷ. Hai hàm được trực quan hóa trong Hình 2-46. Hàm relu cắt các giá trị dưới 0, trong khi hàm tanh bão hòa đến –1 đối với các giá trị đầu vào thấp và +1 đối với các giá trị đầu vào cao. Cả hai hàm phi tuyến đều cho phép mạng nơ-ron học các hàm phức tạp hơn nhiều so với một mô hình tuyến tính có thể: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf74e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = np.linspace(-3, 3, 100)\n",
    "plt.plot(line, np.tanh(line), label=\"tanh\")\n",
    "plt.plot(line, np.maximum(line, 0), label=\"relu\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"relu(x), tanh(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7c2a9d",
   "metadata": {},
   "source": [
    "**Hình 2-46. Hàm kích hoạt tang hyperbol và hàm kích hoạt tuyến tính chỉnh lưu**\n",
    "Đối với mạng nơ-ron nhỏ được mô tả trong Hình 2-45, công thức đầy đủ để tính toán ŷ trong trường hợp hồi quy sẽ là (khi sử dụng hàm phi tuyến tanh):\n",
    "\n",
    "$h[0] = \\tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])$\n",
    "$h[1] = \\tanh(w[0, 1] * x[0] + w[1, 1] * x[1] + w[2, 1] * x[2] + w[3, 1] * x[3])$\n",
    "$h[2] = \\tanh(w[0, 2] * x[0] + w[1, 2] * x[1] + w[2, 2] * x[2] + w[3, 2] * x[3])$\n",
    "$\\hat{y} = v[0] * h[0] + v[1] * h[1] + v[2] * h[2]$\n",
    "\n",
    "Ở đây, w là các trọng số giữa đầu vào x và lớp ẩn h, và v là các trọng số giữa lớp ẩn h và đầu ra ŷ. Các trọng số v và w được học từ dữ liệu, x là các đặc trưng đầu vào, ŷ là đầu ra được tính toán, và h là các phép tính trung gian. Một tham số quan trọng cần được người dùng đặt là số lượng nút trong lớp ẩn. Con số này có thể nhỏ như 10 đối với các tập dữ liệu rất nhỏ hoặc đơn giản và lớn như 10.000 đối với dữ liệu rất phức tạp. Cũng có thể thêm các lớp ẩn bổ sung, như được hiển thị trong Hình 2-47: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4418ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_two_hidden_layer_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ceedd3",
   "metadata": {},
   "source": [
    "**Hình 2-47. Một perceptron đa lớp với hai lớp ẩn**\n",
    "\n",
    "Việc có các mạng nơ-ron lớn được tạo thành từ nhiều lớp tính toán này là điều đã truyền cảm hứng cho thuật ngữ \"học sâu\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e916e2f",
   "metadata": {},
   "source": [
    "#### Điều chỉnh mạng nơ-ron\n",
    "\n",
    "Hãy xem xét hoạt động của MLP bằng cách áp dụng `MLPClassifier` cho tập dữ liệu `two_moons` mà chúng ta đã sử dụng trước đó trong chương này. Kết quả được hiển thị trong Hình 2-48:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04beca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7835c",
   "metadata": {},
   "source": [
    "**Hình 2-48. Ranh giới quyết định được học bởi một mạng nơ-ron với 100 đơn vị ẩn trên tập dữ liệu `two_moons`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ebac3",
   "metadata": {},
   "source": [
    "Như bạn có thể thấy, mạng nơ-ron đã học được một ranh giới quyết định rất phi tuyến nhưng tương đối mượt mà. Chúng ta đã sử dụng `solver='lbfgs'`, mà chúng ta sẽ thảo luận sau. \n",
    "\n",
    "Theo mặc định, MLP sử dụng 100 nút ẩn, là khá nhiều đối với tập dữ liệu nhỏ này. Chúng ta có thể giảm số lượng (làm giảm độ phức tạp của mô hình) và vẫn có được một kết quả tốt (Hình 2-49): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3731d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='lbfgs', random_state=0, hidden_layer_sizes=[10])\n",
    "mlp.fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dccf6d",
   "metadata": {},
   "source": [
    "**Hình 2-49. Ranh giới quyết định được học bởi một mạng nơ-ron với 10 đơn vị ẩn trên tập dữ liệu `two_moons`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd24aa",
   "metadata": {},
   "source": [
    "Với chỉ 10 đơn vị ẩn, ranh giới quyết định trông có vẻ lởm chởm hơn. Hàm phi tuyến mặc định là `relu`, được hiển thị trong Hình 2-46. Với một lớp ẩn duy nhất, điều này có nghĩa là hàm quyết định sẽ được tạo thành từ 10 đoạn thẳng. Nếu chúng ta muốn một ranh giới quyết định mượt mà hơn, chúng ta có thể thêm nhiều đơn vị ẩn hơn (như trong Hình 2-49), thêm một lớp ẩn thứ hai (Hình 2-50), hoặc sử dụng hàm phi tuyến `tanh` (Hình 2-51): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf25514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sử dụng hai lớp ẩn, mỗi lớp có 10 đơn vị\n",
    "mlp = MLPClassifier(solver='lbfgs', random_state=0,\n",
    "                    hidden_layer_sizes=[10, 10])\n",
    "mlp.fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b91f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sử dụng hai lớp ẩn, mỗi lớp có 10 đơn vị, bây giờ với hàm phi tuyến tanh\n",
    "mlp = MLPClassifier(solver='lbfgs', activation='tanh',\n",
    "                    random_state=0, hidden_layer_sizes=[10, 10])\n",
    "mlp.fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14228a2a",
   "metadata": {},
   "source": [
    "**Hình 2-50. Ranh giới quyết định được học bằng cách sử dụng 2 lớp ẩn với mỗi lớp 10 đơn vị ẩn, với hàm kích hoạt rect**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b78104a",
   "metadata": {},
   "source": [
    "**Hình 2-51. Ranh giới quyết định được học bằng cách sử dụng 2 lớp ẩn với mỗi lớp 10 đơn vị ẩn, với hàm kích hoạt tanh**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625ce769",
   "metadata": {},
   "source": [
    "Cuối cùng, chúng ta cũng có thể kiểm soát độ phức tạp của một mạng nơ-ron bằng cách sử dụng một hình phạt l2 để thu nhỏ các trọng số về không, như chúng ta đã làm trong hồi quy Ridge và các bộ phân loại tuyến tính. Tham số cho điều này trong `MLPClassifier` là `alpha` (như trong các mô hình hồi quy tuyến tính), và nó được đặt thành một giá trị rất thấp (ít chính quy hóa) theo mặc định. Hình 2-52 cho thấy ảnh hưởng của các giá trị `alpha` khác nhau trên tập dữ liệu `two_moons`, sử dụng hai lớp ẩn gồm 10 hoặc 100 đơn vị mỗi lớp: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb39b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
    "for axx, n_hidden_nodes in zip(axes, [10, 100]):\n",
    "    for ax, alpha in zip(axx, [0.0001, 0.01, 0.1, 1]):\n",
    "        mlp = MLPClassifier(solver='lbfgs', random_state=0,\n",
    "                            hidden_layer_sizes=[n_hidden_nodes, n_hidden_nodes],\n",
    "                            alpha=alpha)\n",
    "        mlp.fit(X_train, y_train)\n",
    "        mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\n",
    "        mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\n",
    "        ax.set_title(\"n_hidden=[{}, {}]\\nalpha={:.4f}\".format(\n",
    "                      n_hidden_nodes, n_hidden_nodes, alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f22bc6",
   "metadata": {},
   "source": [
    "**Hình 2-52. Các hàm quyết định cho các số lượng đơn vị ẩn khác nhau và các cài đặt khác nhau của tham số alpha**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84ec30",
   "metadata": {},
   "source": [
    "Như bạn có thể đã nhận ra bây giờ, có nhiều cách để kiểm soát độ phức tạp của một mạng nơ-ron: số lượng lớp ẩn, số lượng đơn vị trong mỗi lớp ẩn, và sự chính quy hóa (alpha). Thực tế còn có nhiều hơn nữa, mà chúng ta sẽ không đi sâu vào đây. \n",
    "\n",
    "Một thuộc tính quan trọng của mạng nơ-ron là các trọng số của chúng được đặt ngẫu nhiên trước khi bắt đầu học, và sự khởi tạo ngẫu nhiên này ảnh hưởng đến mô hình được học. Điều đó có nghĩa là ngay cả khi sử dụng chính xác cùng một tham số, chúng ta có thể thu được các mô hình rất khác nhau khi sử dụng các hạt giống ngẫu nhiên khác nhau. Nếu các mạng lớn, và độ phức tạp của chúng được chọn đúng, điều này không nên ảnh hưởng đến độ chính xác quá nhiều, nhưng điều đáng ghi nhớ (đặc biệt là đối với các mạng nhỏ hơn). \n",
    "\n",
    "Hình 2-53 cho thấy các biểu đồ của một số mô hình, tất cả đều được học với cùng một cài đặt của các tham số: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    mlp = MLPClassifier(solver='lbfgs', random_state=i,\n",
    "                        hidden_layer_sizes=[100, 100])\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\n",
    "    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11561d",
   "metadata": {},
   "source": [
    "**Hình 2-53. Các hàm quyết định được học với cùng các tham số nhưng các khởi tạo ngẫu nhiên khác nhau**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f509407",
   "metadata": {},
   "source": [
    "Để hiểu rõ hơn về mạng nơ-ron trên dữ liệu thực tế, hãy áp dụng `MLPClassifier` cho tập dữ liệu Ung thư vú. Chúng ta bắt đầu với các tham số mặc định:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b278f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cancer data per-feature maxima:\\n{}\".format(cancer.data.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fb4583",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.2f}\".format(mlp.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(mlp.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f46895",
   "metadata": {},
   "source": [
    "Độ chính xác của MLP khá tốt, nhưng không tốt bằng các mô hình khác. Như trong ví dụ SVC trước đó, điều này có thể là do việc chia tỷ lệ dữ liệu. Mạng nơ-ron cũng mong đợi tất cả các đặc trưng đầu vào thay đổi theo một cách tương tự, và lý tưởng nhất là có giá trị trung bình là 0 và phương sai là 1. Chúng ta phải chia lại tỷ lệ dữ liệu của mình để nó đáp ứng các yêu cầu này. Một lần nữa, chúng ta sẽ thực hiện điều này bằng tay ở đây, nhưng chúng ta sẽ giới thiệu `StandardScaler` để thực hiện điều này tự động trong Chương 3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec8989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tính giá trị trung bình cho mỗi đặc trưng trên tập huấn luyện\n",
    "mean_on_train = X_train.mean(axis=0)\n",
    "# tính độ lệch chuẩn của mỗi đặc trưng trên tập huấn luyện\n",
    "std_on_train = X_train.std(axis=0)\n",
    "\n",
    "# trừ đi giá trị trung bình, và chia tỷ lệ bằng nghịch đảo độ lệch chuẩn\n",
    "# sau đó, mean=0 và std=1\n",
    "X_train_scaled = (X_train - mean_on_train) / std_on_train\n",
    "# sử dụng CÙNG một phép biến đổi (sử dụng trung bình và độ lệch chuẩn của tập huấn luyện) trên tập kiểm tra\n",
    "X_test_scaled = (X_test - mean_on_train) / std_on_train\n",
    "\n",
    "mlp = MLPClassifier(random_state=0)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    mlp.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a684cb1c",
   "metadata": {},
   "source": [
    "Các kết quả tốt hơn nhiều sau khi chia tỷ lệ, và đã khá cạnh tranh. Tuy nhiên, chúng ta đã nhận được một cảnh báo từ mô hình, cho chúng ta biết rằng số lần lặp tối đa đã đạt được. Đây là một phần của thuật toán `adam` để học mô hình, và cho chúng ta biết rằng chúng ta nên tăng số lần lặp: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=1000, random_state=0)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    mlp.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f1c39",
   "metadata": {},
   "source": [
    "Việc tăng số lần lặp chỉ làm tăng hiệu suất của tập huấn luyện, chứ không phải hiệu suất tổng quát hóa. Tuy nhiên, mô hình đang hoạt động khá tốt. Vì có một khoảng cách giữa hiệu suất của tập huấn luyện và tập kiểm tra, chúng ta có thể thử giảm độ phức tạp của mô hình để có được hiệu suất tổng quát hóa tốt hơn. Ở đây, chúng ta chọn tăng tham số `alpha` (khá mạnh, từ 0,0001 lên 1) để thêm sự chính quy hóa mạnh hơn cho các trọng số: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811da4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=1000, alpha=1, random_state=0)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    mlp.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee3137",
   "metadata": {},
   "source": [
    "Điều này dẫn đến một hiệu suất ngang bằng với các mô hình tốt nhất cho đến nay.¹²\n",
    "\n",
    "Mặc dù có thể phân tích những gì một mạng nơ-ron đã học, nhưng điều này thường khó hơn nhiều so với việc phân tích một mô hình tuyến tính hoặc một mô hình dựa trên cây. Một cách để xem xét nội tâm những gì đã được học là xem xét các trọng số trong mô hình. Bạn có thể xem một ví dụ về điều này trong thư viện ví dụ của scikit-learn. Đối với tập dữ liệu Ung thư vú, điều này có thể hơi khó hiểu. Biểu đồ sau (Hình 2-54) cho thấy các trọng số đã được học kết nối đầu vào với lớp ẩn đầu tiên. Các hàng trong biểu đồ này tương ứng với 30 đặc trưng đầu vào, trong khi các cột tương ứng với 100 đơn vị ẩn. Màu sáng đại diện cho các giá trị dương lớn, trong khi màu tối đại diện cho các giá trị âm: \n",
    "\n",
    "--- \n",
    "¹²Bạn có thể đã nhận thấy tại thời điểm này rằng nhiều mô hình hoạt động tốt đều đạt được chính xác cùng một độ chính xác là 0,972. Điều này có nghĩa là tất cả các mô hình đều mắc chính xác cùng một số lỗi, là bốn. Nếu bạn so sánh các dự đoán thực tế, bạn thậm chí có thể thấy rằng chúng mắc chính xác cùng một lỗi! Điều này có thể là một hệ quả của việc tập dữ liệu rất nhỏ, hoặc có thể là do những điểm này thực sự khác biệt so với phần còn lại."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27b372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.imshow(mlp.coefs_[0], interpolation='none', cmap='viridis')\n",
    "plt.yticks(range(30), cancer.feature_names)\n",
    "plt.xlabel(\"Columns in weight matrix\")\n",
    "plt.ylabel(\"Input feature\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c160768",
   "metadata": {},
   "source": [
    "**Hình 2-54. Bản đồ nhiệt của các trọng số lớp đầu tiên trong một mạng nơ-ron đã học trên tập dữ liệu Ung thư vú**\n",
    "\n",
    "Một suy luận có thể có mà chúng ta có thể đưa ra là các đặc trưng có trọng số rất nhỏ đối với tất cả các đơn vị ẩn là “kém quan trọng hơn” đối với mô hình. Chúng ta có thể thấy rằng “mean smoothness” và “mean compactness”, ngoài các đặc trưng được tìm thấy giữa “smoothness error” và “fractal dimension error”, có trọng số tương đối thấp so với các đặc trưng khác. Điều này có thể có nghĩa là đây là các đặc trưng kém quan trọng hơn hoặc có thể là chúng ta đã không biểu diễn chúng theo cách mà mạng nơ-ron có thể sử dụng.\n",
    "\n",
    "Chúng ta cũng có thể trực quan hóa các trọng số kết nối lớp ẩn với lớp đầu ra, nhưng những trọng số đó thậm chí còn khó giải thích hơn.\n",
    "\n",
    "Mặc dù `MLPClassifier` và `MLPRegressor` cung cấp các giao diện dễ sử dụng cho các kiến trúc mạng nơ-ron phổ biến nhất, chúng chỉ nắm bắt được một tập hợp con nhỏ của những gì có thể có với mạng nơ-ron. Nếu bạn quan tâm đến việc làm việc với các mô hình linh hoạt hơn hoặc lớn hơn, chúng tôi khuyến khích bạn xem xét ngoài scikit-learn vào các thư viện học sâu tuyệt vời hiện có. Đối với người dùng Python, các thư viện được thiết lập tốt nhất là keras, lasagna và tensor-flow. lasagna được xây dựng trên thư viện theano, trong khi keras có thể sử dụng tensor-flow hoặc theano. Các thư viện này cung cấp một giao diện linh hoạt hơn nhiều để xây dựng mạng nơ-ron và theo dõi sự tiến bộ nhanh chóng trong nghiên cứu học sâu. Tất cả các thư viện học sâu phổ biến cũng cho phép sử dụng các đơn vị xử lý đồ họa hiệu suất cao (GPU), mà scikit-learn không hỗ trợ. Việc sử dụng GPU cho phép chúng ta tăng tốc các phép tính theo hệ số từ 10x đến 100x, và chúng rất cần thiết để áp dụng các phương pháp học sâu cho các tập dữ liệu quy mô lớn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db847e",
   "metadata": {},
   "source": [
    "#### Điểm mạnh, điểm yếu và tham số\n",
    "Mạng nơ-ron đã tái xuất hiện như là các mô hình tiên tiến trong nhiều ứng dụng học máy. Một trong những ưu điểm chính của chúng là chúng có khả năng nắm bắt thông tin chứa trong một lượng lớn dữ liệu và xây dựng các mô hình vô cùng phức tạp. Với đủ thời gian tính toán, dữ liệu và việc điều chỉnh cẩn thận các tham số, mạng nơ-ron thường đánh bại các thuật toán học máy khác (đối với các tác vụ phân loại và hồi quy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122804de",
   "metadata": {},
   "source": [
    "Điều này đưa chúng ta đến những nhược điểm. Mạng nơ-ron—đặc biệt là những mạng lớn và mạnh mẽ—thường mất nhiều thời gian để huấn luyện. Chúng cũng yêu cầu tiền xử lý cẩn thận dữ liệu, như chúng ta đã thấy ở đây. Tương tự như SVM, chúng hoạt động tốt nhất với dữ liệu “đồng nhất”, trong đó tất cả các đặc trưng có ý nghĩa tương tự. Đối với dữ liệu có các loại đặc trưng rất khác nhau, các mô hình dựa trên cây có thể hoạt động tốt hơn. Việc điều chỉnh các tham số mạng nơ-ron cũng là một nghệ thuật. Trong các thí nghiệm của chúng ta, chúng ta hầu như không chạm đến bề mặt của các cách có thể để điều chỉnh các mô hình mạng nơ-ron và cách huấn luyện chúng.\n",
    "\n",
    "**Ước tính độ phức tạp trong mạng nơ-ron.** Các tham số quan trọng nhất là số lượng lớp và số lượng đơn vị ẩn trên mỗi lớp. Bạn nên bắt đầu với một hoặc hai lớp ẩn, và có thể mở rộng từ đó. Số lượng nút trên mỗi lớp ẩn thường tương tự như số lượng đặc trưng đầu vào, nhưng hiếm khi cao hơn trong khoảng từ thấp đến trung bình hàng nghìn.\n",
    "\n",
    "Một thước đo hữu ích khi nghĩ về độ phức tạp của mô hình mạng nơ-ron là số lượng trọng số hoặc hệ số được học. Nếu bạn có một tập dữ liệu phân loại nhị phân với 100 đặc trưng, và bạn có 100 đơn vị ẩn, thì có 100 * 100 = 10.000 trọng số giữa đầu vào và lớp ẩn đầu tiên. Cũng có 100 * 1 = 100 trọng số giữa lớp ẩn và lớp đầu ra, với tổng số khoảng 10.100 trọng số. Nếu bạn thêm một lớp ẩn thứ hai với 100 đơn vị ẩn, sẽ có thêm 100 * 100 = 10.000 trọng số từ lớp ẩn đầu tiên đến lớp ẩn thứ hai, dẫn đến tổng số 20.100 trọng số. Nếu thay vào đó, bạn sử dụng một lớp với 1.000 đơn vị ẩn, bạn đang học 100 * 1.000 = 100.000 trọng số từ đầu vào đến lớp ẩn và 1.000 x 1 trọng số từ lớp ẩn đến lớp đầu ra, với tổng số 101.000. Nếu bạn thêm một lớp ẩn thứ hai, bạn thêm 1.000 * 1.000 = 1.000.000 trọng số, với tổng số khổng lồ là 1.101.000—lớn hơn 50 lần so với mô hình có hai lớp ẩn kích thước 100.\n",
    "\n",
    "Một cách phổ biến để điều chỉnh các tham số trong một mạng nơ-ron là trước tiên tạo ra một mạng đủ lớn để quá khớp, đảm bảo rằng nhiệm vụ thực sự có thể được học bởi mạng. Sau đó, một khi bạn biết dữ liệu huấn luyện có thể được học, hãy thu nhỏ mạng hoặc tăng alpha để thêm chính quy hóa, điều này sẽ cải thiện hiệu suất tổng quát hóa.\n",
    "\n",
    "Trong các thí nghiệm của chúng ta, chúng ta tập trung chủ yếu vào định nghĩa của mô hình: số lượng lớp và nút trên mỗi lớp, sự chính quy hóa và hàm phi tuyến. Những điều này xác định mô hình chúng ta muốn học. Cũng có câu hỏi về cách học mô hình, hoặc thuật toán được sử dụng để học các tham số, được đặt bằng cách sử dụng tham số `solver`. Có hai lựa chọn dễ sử dụng cho bộ giải. Mặc định là 'adam', hoạt động tốt trong hầu hết các tình huống nhưng khá nhạy cảm với việc chia tỷ lệ dữ liệu (vì vậy điều quan trọng là luôn chia tỷ lệ dữ liệu của bạn về trung bình 0 và phương sai đơn vị). Một lựa chọn khác là 'l-bfgs', khá mạnh mẽ nhưng có thể mất nhiều thời gian trên các mô hình lớn hơn hoặc các tập dữ liệu lớn hơn. Cũng có tùy chọn 'sgd' nâng cao hơn, là những gì nhiều nhà nghiên cứu học sâu sử dụng. Tùy chọn 'sgd' đi kèm với nhiều tham số bổ sung cần được điều chỉnh để có kết quả tốt nhất. Bạn có thể tìm thấy tất cả các tham số này và định nghĩa của chúng trong hướng dẫn sử dụng. Khi bắt đầu làm việc với MLP, chúng tôi khuyên bạn nên sử dụng 'adam' và 'l-bfgs'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3001e6",
   "metadata": {},
   "source": [
    "> #### `fit` Đặt lại một mô hình\n",
    "> \n",
    "> Một thuộc tính quan trọng của các mô hình scikit-learn là việc gọi `fit` sẽ luôn đặt lại mọi thứ mà một mô hình đã học trước đó. Vì vậy, nếu bạn xây dựng một mô hình trên một tập dữ liệu, và sau đó gọi `fit` một lần nữa trên một tập dữ liệu khác, mô hình sẽ \"quên\" mọi thứ nó đã học từ tập dữ liệu đầu tiên. Bạn có thể gọi `fit` bao nhiêu lần tùy thích trên một mô hình, và kết quả sẽ giống như việc gọi `fit` trên một mô hình \"mới\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a2747",
   "metadata": {},
   "source": [
    "### Ước tính sự không chắc chắn từ các bộ phân loại\n",
    "\n",
    "Một phần hữu ích khác của giao diện scikit-learn mà chúng ta chưa nói đến là khả năng của các bộ phân loại cung cấp các ước tính sự không chắc chắn của các dự đoán. Thường thì, bạn không chỉ quan tâm đến lớp nào mà một bộ phân loại dự đoán cho một điểm kiểm tra nhất định, mà còn quan tâm đến mức độ chắc chắn của nó rằng đây là lớp đúng. Trong thực tế, các loại sai lầm khác nhau dẫn đến các kết quả rất khác nhau trong các ứng dụng thực tế. Hãy tưởng tượng một ứng dụng y tế kiểm tra ung thư. Việc đưa ra một dự đoán dương tính giả có thể dẫn đến việc bệnh nhân phải trải qua các xét nghiệm bổ sung, trong khi một dự đoán âm tính giả có thể dẫn đến việc một căn bệnh nghiêm trọng không được điều trị. Chúng ta sẽ đi sâu hơn vào chủ đề này trong Chương 6.\n",
    "\n",
    "Có hai hàm khác nhau trong scikit-learn có thể được sử dụng để thu được các ước tính sự không chắc chắn từ các bộ phân loại: `decision_function` và `predict_proba`. Hầu hết (nhưng không phải tất cả) các bộ phân loại đều có ít nhất một trong hai hàm này, và nhiều bộ phân loại có cả hai. Hãy xem hai hàm này làm gì trên một tập dữ liệu hai chiều tổng hợp, khi xây dựng một bộ phân loại `GradientBoostingClassifier`, có cả phương thức `decision_function` và `predict_proba`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583ae339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_blobs, make_circles\n",
    "\n",
    "X, y = make_circles(noise=0.25, factor=0.5, random_state=1)\n",
    "\n",
    "# chúng ta đổi tên các lớp thành \"blue\" và \"red\" cho mục đích minh họa\n",
    "y_named = np.array([\"blue\", \"red\"])[y]\n",
    "\n",
    "# chúng ta có thể gọi train_test_split với số lượng mảng tùy ý;\n",
    "# tất cả sẽ được chia một cách nhất quán\n",
    "X_train, X_test, y_train_named, y_test_named, y_train, y_test = \\\n",
    "    train_test_split(X, y_named, y, random_state=0)\n",
    "\n",
    "# xây dựng mô hình tăng cường độ dốc\n",
    "gbrt = GradientBoostingClassifier(random_state=0)\n",
    "gbrt.fit(X_train, y_train_named)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84478353",
   "metadata": {},
   "source": [
    "#### Hàm Quyết định\n",
    "Trong trường hợp phân loại nhị phân, giá trị trả về của `decision_function` có hình dạng `(n_samples,)`, và nó trả về một số thực cho mỗi mẫu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ff74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_test.shape: {}\".format(X_test.shape))\n",
    "print(\"Decision function shape: {}\".format(\n",
    "    gbrt.decision_function(X_test).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e24b7a",
   "metadata": {},
   "source": [
    "Giá trị này mã hóa mức độ tin tưởng của mô hình rằng một điểm dữ liệu thuộc về lớp “dương”, trong trường hợp này là lớp 1. Các giá trị dương cho thấy sự ưu tiên cho lớp dương, và các giá trị âm cho thấy sự ưu tiên cho lớp “âm” (lớp còn lại):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29fed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiển thị một vài mục đầu tiên của decision_function\n",
    "print(\"Decision function:\\n{}\".format(gbrt.decision_function(X_test)[:6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ebcf56",
   "metadata": {},
   "source": [
    "Chúng ta có thể khôi phục dự đoán bằng cách chỉ nhìn vào dấu của hàm quyết định:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289de856",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Thresholded decision function:\\n{}\".format(\n",
    "    gbrt.decision_function(X_test) > 0))\n",
    "print(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6ffec",
   "metadata": {},
   "source": [
    "Đối với phân loại nhị phân, lớp “âm” luôn là mục đầu tiên của thuộc tính `classes_`, và lớp “dương” là mục thứ hai của thuộc tính `classes_`. Vì vậy, nếu bạn muốn khôi phục hoàn toàn đầu ra của `predict`, bạn cần sử dụng thuộc tính `classes_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chuyển đổi boolean True/False thành 0 và 1\n",
    "greater_zero = (gbrt.decision_function(X_test) > 0).astype(int)\n",
    "# sử dụng 0 và 1 làm chỉ số vào classes_\n",
    "pred = gbrt.classes_[greater_zero]\n",
    "# pred giống như đầu ra của gbrt.predict\n",
    "print(\"pred is equal to predictions: {}\".format(\n",
    "    np.all(pred == gbrt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b364d",
   "metadata": {},
   "source": [
    "Phạm vi của `decision_function` có thể tùy ý, và phụ thuộc vào dữ liệu và các tham số của mô hình:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b82542",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_function = gbrt.decision_function(X_test)\n",
    "print(\"Decision function minimum: {:.2f} maximum: {:.2f}\".format(\n",
    "    np.min(decision_function), np.max(decision_function)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b255e",
   "metadata": {},
   "source": [
    "Việc chia tỷ lệ tùy ý này làm cho đầu ra của `decision_function` thường khó giải thích.\n",
    "Trong ví dụ sau, chúng ta vẽ `decision_function` cho tất cả các điểm trong mặt phẳng 2D bằng cách sử dụng mã hóa màu, bên cạnh một trực quan hóa của ranh giới quyết định, như chúng ta đã thấy trước đó. Chúng ta hiển thị các điểm huấn luyện dưới dạng các vòng tròn và dữ liệu kiểm tra dưới dạng các hình tam giác (Hình 2-55):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de68e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "mglearn.tools.plot_2d_separator(gbrt, X, ax=axes[0], alpha=.4,\n",
    "                                fill=True, cm=mglearn.cm2)\n",
    "scores_image = mglearn.tools.plot_2d_scores(gbrt, X, ax=axes[1],\n",
    "                                             alpha=.4, cm=mglearn.ReBl)\n",
    "\n",
    "for ax in axes:\n",
    "    # vẽ các điểm huấn luyện và kiểm tra\n",
    "    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\n",
    "                             markers='^', ax=ax)\n",
    "    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\n",
    "                             markers='o', ax=ax)\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "cbar = plt.colorbar(scores_image, ax=axes.tolist())\n",
    "axes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\n",
    "                \"Train class 1\"], ncol=4, loc=(.1, 1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b421a0c",
   "metadata": {},
   "source": [
    "**Hình 2-55. Ranh giới quyết định (trái) và hàm quyết định (phải) cho một mô hình tăng cường độ dốc trên một tập dữ liệu đồ chơi hai chiều**\n",
    "\n",
    "Việc mã hóa không chỉ kết quả dự đoán mà còn mức độ chắc chắn của bộ phân loại cung cấp thông tin bổ sung. Tuy nhiên, trong trực quan hóa này, khó có thể phân biệt được ranh giới giữa hai lớp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865bf7e",
   "metadata": {},
   "source": [
    "#### Dự đoán xác suất\n",
    "\n",
    "Đầu ra của `predict_proba` là một xác suất cho mỗi lớp, và thường dễ hiểu hơn đầu ra của `decision_function`. Nó luôn có hình dạng `(n_samples, 2)` đối với phân loại nhị phân:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9602c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of probabilities: {}\".format(gbrt.predict_proba(X_test).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb1569",
   "metadata": {},
   "source": [
    "Mục đầu tiên trong mỗi hàng là xác suất ước tính của lớp đầu tiên, và mục thứ hai là xác suất ước tính của lớp thứ hai. Bởi vì nó là một xác suất, đầu ra của `predict_proba` luôn nằm trong khoảng từ 0 đến 1, và tổng các mục cho cả hai lớp luôn bằng 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiển thị một vài mục đầu tiên của predict_proba\n",
    "print(\"Predicted probabilities:\\n{}\".format(\n",
    "    gbrt.predict_proba(X_test[:6])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c34674",
   "metadata": {},
   "source": [
    "Bởi vì các xác suất cho hai lớp có tổng bằng 1, chính xác một trong các lớp sẽ có độ chắc chắn trên 50%. Lớp đó là lớp được dự đoán.¹³\n",
    "\n",
    "Bạn có thể thấy trong đầu ra trước đó rằng bộ phân loại tương đối chắc chắn đối với hầu hết các điểm. Mức độ không chắc chắn thực sự phản ánh sự không chắc chắn trong dữ liệu phụ thuộc vào mô hình và các tham số. Một mô hình bị quá khớp nhiều hơn có xu hướng đưa ra các dự đoán chắc chắn hơn, ngay cả khi chúng có thể sai. Một mô hình có độ phức tạp ít hơn thường có nhiều sự không chắc chắn hơn trong các dự đoán của nó. Một mô hình được gọi là đã được hiệu chỉnh nếu sự không chắc chắn được báo cáo thực sự khớp với mức độ chính xác của nó—trong một mô hình đã được hiệu chỉnh, một dự đoán được đưa ra với độ chắc chắn 70% sẽ đúng trong 70% trường hợp.\n",
    "\n",
    "Trong ví dụ sau (Hình 2-56), chúng ta lại hiển thị ranh giới quyết định trên tập dữ liệu, bên cạnh các xác suất lớp cho lớp 1:\n",
    "\n",
    "--- \n",
    "¹³Bởi vì các xác suất là các số thực, nên không có khả năng cả hai đều chính xác là 0,500. Tuy nhiên, nếu điều đó xảy ra, dự đoán được đưa ra một cách ngẫu nhiên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "mglearn.tools.plot_2d_separator(\n",
    "    gbrt, X, ax=axes[0], alpha=.4, fill=True, cm=mglearn.cm2)\n",
    "scores_image = mglearn.tools.plot_2d_scores(\n",
    "    gbrt, X, ax=axes[1], alpha=.5, cm=mglearn.ReBl, function='predict_proba')\n",
    "\n",
    "for ax in axes:\n",
    "    # vẽ các điểm huấn luyện và kiểm tra\n",
    "    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\n",
    "                             markers='^', ax=ax)\n",
    "    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\n",
    "                             markers='o', ax=ax)\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "cbar = plt.colorbar(scores_image, ax=axes.tolist())\n",
    "axes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\n",
    "                \"Train class 1\"], ncol=4, loc=(.1, 1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144745d7",
   "metadata": {},
   "source": [
    "**Hình 2-56. Ranh giới quyết định (trái) và các xác suất dự đoán cho mô hình tăng cường độ dốc được hiển thị trong Hình 2-55**\n",
    "Các ranh giới trong biểu đồ này được xác định rõ ràng hơn nhiều, và các vùng không chắc chắn nhỏ có thể nhìn thấy rõ ràng.\n",
    "\n",
    "Trang web scikit-learn có một so sánh tuyệt vời về nhiều mô hình và các ước tính không chắc chắn của chúng trông như thế nào. Chúng tôi đã sao chép điều này trong Hình 2-57, và chúng tôi khuyến khích bạn xem qua ví dụ ở đó."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ce1b6",
   "metadata": {},
   "source": [
    "**Hình 2-57. So sánh một số bộ phân loại trong scikit-learn trên các tập dữ liệu tổng hợp (hình ảnh được cung cấp bởi http://scikit-learn.org)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe8b400",
   "metadata": {},
   "source": [
    "#### Sự không chắc chắn trong Phân loại đa lớp\n",
    "\n",
    "Cho đến nay, chúng ta chỉ nói về các ước tính sự không chắc chắn trong phân loại nhị phân. Nhưng các phương thức `decision_function` và `predict_proba` cũng hoạt động trong cài đặt đa lớp. Hãy áp dụng chúng trên tập dữ liệu Iris, là một tập dữ liệu phân loại ba lớp:\n",
    "\n",
    "--- \n",
    "²Không nên nhầm lẫn với tập dữ liệu MNIST lớn hơn nhiều."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb76198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, random_state=42)\n",
    "\n",
    "gbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)\n",
    "gbrt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e4a492",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision function shape: {}\".format(gbrt.decision_function(X_test).shape))\n",
    "# vẽ một vài mục đầu tiên của hàm quyết định\n",
    "print(\"Decision function:\\n{}\".format(gbrt.decision_function(X_test)[:6, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5212ece3",
   "metadata": {},
   "source": [
    "Trong trường hợp đa lớp, `decision_function` có hình dạng `(n_samples, n_classes)` và mỗi cột cung cấp một “điểm chắc chắn” cho mỗi lớp, trong đó một điểm số lớn có nghĩa là một lớp có nhiều khả năng hơn và một điểm số nhỏ có nghĩa là lớp đó ít có khả năng hơn. Bạn có thể khôi phục các dự đoán từ các điểm số này bằng cách tìm mục lớn nhất cho mỗi điểm dữ liệu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359857e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Argmax of decision function:\\n{}\".format(\n",
    "    np.argmax(gbrt.decision_function(X_test), axis=1)))\n",
    "print(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65cde8e",
   "metadata": {},
   "source": [
    "Đầu ra của `predict_proba` có cùng hình dạng, `(n_samples, n_classes)`. Một lần nữa, các xác suất cho các lớp có thể có cho mỗi điểm dữ liệu có tổng bằng 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b01625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiển thị một vài mục đầu tiên của predict_proba\n",
    "print(\"Predicted probabilities:\\n{}\".format(gbrt.predict_proba(X_test)[:6]))\n",
    "# hiển thị rằng tổng trên các hàng là một\n",
    "print(\"Sums: {}\".format(gbrt.predict_proba(X_test)[:6].sum(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Argmax of predicted probabilities:\\n{}\".format(\n",
    "    np.argmax(gbrt.predict_proba(X_test), axis=1)))\n",
    "print(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd3a9c",
   "metadata": {},
   "source": [
    "Để tóm tắt, `predict_proba` và `decision_function` luôn có hình dạng `(n_samples, n_classes)`—ngoại trừ `decision_function` trong trường hợp nhị phân đặc biệt. Trong trường hợp nhị phân, `decision_function` chỉ có một cột, tương ứng với lớp “dương” `classes_[1]`. Điều này chủ yếu là do các lý do lịch sử.\n",
    "\n",
    "Bạn có thể khôi phục dự đoán khi có `n_classes` nhiều cột bằng cách tính toán `argmax` trên các cột. Tuy nhiên, hãy cẩn thận nếu các lớp của bạn là các chuỗi, hoặc bạn sử dụng các số nguyên nhưng chúng không liên tiếp và bắt đầu từ 0. Nếu bạn muốn so sánh các kết quả thu được với `predict` với các kết quả thu được qua `decision_function` hoặc `predict_proba`, hãy đảm bảo sử dụng thuộc tính `classes_` của bộ phân loại để có được tên lớp thực tế:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "\n",
    "# biểu diễn mỗi mục tiêu bằng tên lớp của nó trong tập dữ liệu iris\n",
    "named_target = iris.target_names[y_train]\n",
    "logreg.fit(X_train, named_target)\n",
    "print(\"unique classes in training data: {}\".format(logreg.classes_))\n",
    "print(\"predictions: {}\".format(logreg.predict(X_test)[:10]))\n",
    "argmax_dec_func = np.argmax(logreg.decision_function(X_test), axis=1)\n",
    "print(\"argmax of decision function: {}\".format(argmax_dec_func[:10]))\n",
    "print(\"argmax combined with classes_: {}\".format(\n",
    "    logreg.classes_[argmax_dec_func][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5692fde4",
   "metadata": {},
   "source": [
    "### Tóm tắt và Triển vọng\n",
    "\n",
    "Chúng ta đã bắt đầu chương này với một cuộc thảo luận về độ phức tạp của mô hình, sau đó thảo luận về sự tổng quát hóa, hoặc học một mô hình có thể hoạt động tốt trên dữ liệu mới, chưa từng thấy. Điều này đã dẫn chúng ta đến các khái niệm về thiếu khớp, mô tả một mô hình không thể nắm bắt được các biến thể có trong dữ liệu huấn luyện, và quá khớp, mô tả một mô hình tập trung quá nhiều vào dữ liệu huấn luyện và không thể tổng quát hóa tốt cho dữ liệu mới.\n",
    "\n",
    "Sau đó, chúng ta đã thảo luận về một loạt các mô hình học máy cho phân loại và hồi quy, những ưu điểm và nhược điểm của chúng, và cách kiểm soát độ phức tạp của mô hình cho mỗi mô hình. Chúng ta đã thấy rằng đối với nhiều thuật toán, việc đặt đúng các tham số là quan trọng để có được hiệu suất tốt. Một số thuật toán cũng nhạy cảm với cách chúng ta biểu diễn dữ liệu đầu vào, và đặc biệt là cách các đặc trưng được chia tỷ lệ. Do đó, việc áp dụng một thuật toán một cách mù quáng cho một tập dữ liệu mà không hiểu các giả định mà mô hình đưa ra và ý nghĩa của các cài đặt tham số sẽ hiếm khi dẫn đến một mô hình chính xác.\n",
    "\n",
    "Chương này chứa rất nhiều thông tin về các thuật toán, và không cần thiết để bạn phải nhớ tất cả các chi tiết này cho các chương sau. Tuy nhiên, một số kiến thức về các mô hình được mô tả ở đây—và mô hình nào nên sử dụng trong một tình huống cụ thể—là quan trọng để áp dụng thành công học máy trong thực tế. Dưới đây là một tóm tắt nhanh về thời điểm sử dụng mỗi mô hình:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ab343",
   "metadata": {},
   "source": [
    "**Láng giềng gần nhất**\n",
    "Đối với các tập dữ liệu nhỏ, tốt như một đường cơ sở, dễ giải thích.\n",
    "\n",
    "**Mô hình tuyến tính**\n",
    "Lựa chọn hàng đầu như một thuật toán đầu tiên để thử, tốt cho các tập dữ liệu rất lớn, tốt cho dữ liệu có chiều rất cao.\n",
    "\n",
    "**Naive Bayes**\n",
    "Chỉ dành cho phân loại. Thậm chí còn nhanh hơn các mô hình tuyến tính, tốt cho các tập dữ liệu rất lớn và dữ liệu có chiều cao. Thường kém chính xác hơn các mô hình tuyến tính.\n",
    "\n",
    "**Cây quyết định**\n",
    "Rất nhanh, không cần chia tỷ lệ dữ liệu, có thể được trực quan hóa và dễ dàng giải thích.\n",
    "\n",
    "**Rừng ngẫu nhiên**\n",
    "Hầu như luôn hoạt động tốt hơn một cây quyết định duy nhất, rất mạnh mẽ và mạnh mẽ. Không cần chia tỷ lệ dữ liệu. Không tốt cho dữ liệu thưa thớt có chiều rất cao.\n",
    "\n",
    "**Cây quyết định tăng cường độ dốc**\n",
    "Thường chính xác hơn một chút so với rừng ngẫu nhiên. Chậm hơn để huấn luyện nhưng nhanh hơn để dự đoán so với rừng ngẫu nhiên, và nhỏ hơn về bộ nhớ. Cần điều chỉnh tham số nhiều hơn so với rừng ngẫu nhiên.\n",
    "\n",
    "**Máy véc-tơ hỗ trợ**\n",
    "Mạnh mẽ đối với các tập dữ liệu cỡ trung bình của các đặc trưng có ý nghĩa tương tự. Yêu cầu chia tỷ lệ dữ liệu, nhạy cảm với các tham số.\n",
    "\n",
    "**Mạng nơ-ron**\n",
    "Có thể xây dựng các mô hình rất phức tạp, đặc biệt là đối với các tập dữ liệu lớn. Nhạy cảm với việc chia tỷ lệ dữ liệu và với việc lựa chọn các tham số. Các mô hình lớn cần nhiều thời gian để huấn luyện.\n",
    "\n",
    "Khi làm việc với một tập dữ liệu mới, nói chung là một ý tưởng tốt để bắt đầu với một mô hình đơn giản, chẳng hạn như một mô hình tuyến tính hoặc một bộ phân loại Naive Bayes hoặc láng giềng gần nhất, và xem bạn có thể đi được bao xa. Sau khi hiểu thêm về dữ liệu, bạn có thể xem xét chuyển sang một thuật toán có thể xây dựng các mô hình phức tạp hơn, chẳng hạn như rừng ngẫu nhiên, cây quyết định tăng cường độ dốc, SVM hoặc mạng nơ-ron.\n",
    "\n",
    "Bây giờ bạn nên ở một vị trí mà bạn có một số ý tưởng về cách áp dụng, điều chỉnh và phân tích các mô hình chúng ta đã thảo luận ở đây.Trong chương này, chúng ta tập trung vào trường hợp phân loại nhị phân, vì đây thường là trường hợp dễ hiểu nhất. Tuy nhiên, hầu hết các thuật toán được trình bày đều có các biến thể phân loại và hồi quy, và tất cả các thuật toán phân loại đều hỗ trợ cả phân loại nhị phân và đa lớp. Hãy thử áp dụng bất kỳ thuật toán nào trong số này cho các tập dữ liệu tích hợp sẵn trong scikit-learn, như tập dữ liệu `boston_housing` hoặc `diabetes` cho hồi quy, hoặc tập dữ liệu `digits` cho phân loại đa lớp. Việc thử nghiệm với các thuật toán trên các tập dữ liệu khác nhau sẽ giúp bạn có cảm nhận tốt hơn về thời gian chúng cần để huấn luyện, mức độ dễ dàng để phân tích các mô hình, và mức độ nhạy cảm của chúng đối với việc biểu diễn dữ liệu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ea1aad",
   "metadata": {},
   "source": [
    "Mặc dù chúng ta đã phân tích hậu quả của các cài đặt tham số khác nhau đối với các thuật toán mà chúng ta đã điều tra, việc xây dựng một mô hình thực sự tổng quát hóa tốt cho dữ liệu mới trong sản xuất phức tạp hơn một chút. Chúng ta sẽ xem cách điều chỉnh đúng các tham số và cách tự động tìm các tham số tốt trong Chương 6.\n",
    "\n",
    "Tuy nhiên, trước tiên, chúng ta sẽ đi sâu hơn vào chi tiết về học không giám sát và tiền xử lý trong chương tiếp theo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

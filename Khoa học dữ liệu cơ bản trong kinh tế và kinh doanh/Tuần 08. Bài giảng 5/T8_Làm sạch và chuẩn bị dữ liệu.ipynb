{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a35277-2812-41cc-a00e-c99ade49156f",
   "metadata": {},
   "source": [
    "# L√†m s·∫°ch v√† chu·∫©n b·ªã d·ªØ li·ªáu trong Khoa h·ªçc D·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fbd8d0",
   "metadata": {},
   "source": [
    "## X·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu (Missing Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb9881",
   "metadata": {},
   "source": [
    "### Hi·ªÉu v·ªÅ d·ªØ li·ªáu thi·∫øu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6d04b1",
   "metadata": {},
   "source": [
    "**üîç D·ªØ li·ªáu thi·∫øu l√† g√¨?**\n",
    "\n",
    "D·ªØ li·ªáu thi·∫øu (*missing data*) xu·∫•t hi·ªán khi **kh√¥ng c√≥ gi√° tr·ªã d·ªØ li·ªáu** ƒë∆∞·ª£c l∆∞u tr·ªØ cho m·ªôt bi·∫øn trong m·ªôt quan s√°t. Trong `pandas`, d·ªØ li·ªáu thi·∫øu ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng:\n",
    "\n",
    "- `NaN` (*Not a Number*) - cho d·ªØ li·ªáu s·ªë\n",
    "- `None` - cho d·ªØ li·ªáu ƒë·ªëi t∆∞·ª£ng  \n",
    "- `NaT` (*Not a Time*) - cho d·ªØ li·ªáu th·ªùi gian\n",
    "\n",
    "**üìã C√°c nguy√™n nh√¢n g√¢y ra d·ªØ li·ªáu thi·∫øu**\n",
    "\n",
    "- **L·ªói thu th·∫≠p d·ªØ li·ªáu**: Thi·∫øt b·ªã ghi nh·∫≠n b·ªã l·ªói, k·∫øt n·ªëi m·∫°ng b·ªã gi√°n ƒëo·∫°n\n",
    "- **Ng∆∞·ªùi d√πng kh√¥ng cung c·∫•p**: B·ªè qua c√¢u h·ªèi trong survey, kh√¥ng ƒëi·ªÅn th√¥ng tin c√° nh√¢n\n",
    "- **D·ªØ li·ªáu kh√¥ng t·ªìn t·∫°i**: M·ªôt s·ªë th√¥ng tin ch∆∞a ƒë∆∞·ª£c sinh ra t·∫°i th·ªùi ƒëi·ªÉm thu th·∫≠p\n",
    "- **L·ªói x·ª≠ l√Ω d·ªØ li·ªáu**: Trong qu√° tr√¨nh nh·∫≠p li·ªáu, chuy·ªÉn ƒë·ªïi ƒë·ªãnh d·∫°ng\n",
    "\n",
    "**‚ö†Ô∏è T√°c ƒë·ªông c·ªßa d·ªØ li·ªáu thi·∫øu**\n",
    "\n",
    "- **Gi·∫£m k√≠ch th∆∞·ªõc m·∫´u**: Lo·∫°i b·ªè c√°c h√†ng c√≥ d·ªØ li·ªáu thi·∫øu l√†m gi·∫£m s·ªë l∆∞·ª£ng quan s√°t\n",
    "- **Thi√™n l·ªách k·∫øt qu·∫£**: N·∫øu d·ªØ li·ªáu thi·∫øu kh√¥ng ng·∫´u nhi√™n, c√≥ th·ªÉ d·∫´n ƒë·∫øn k·∫øt lu·∫≠n sai l·ªách\n",
    "- **Gi·∫£m hi·ªáu qu·∫£ ph√¢n t√≠ch**: M·ªôt s·ªë thu·∫≠t to√°n machine learning kh√¥ng th·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e720916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# T·∫°o DataFrame m·∫´u v·ªõi d·ªØ li·ªáu thi·∫øu\n",
    "data_missing = {\n",
    "    'T√™n': ['An', 'B√¨nh', 'Chi', 'D≈©ng', 'Eva'],\n",
    "    'Tu·ªïi': [25, None, 30, 28, None],\n",
    "    'L∆∞∆°ng': [15000000, 18000000, None, 22000000, 16000000],\n",
    "    'Ph√≤ng ban': ['IT', 'Marketing', None, 'IT', 'Marketing']\n",
    "}\n",
    "\n",
    "df_missing = pd.DataFrame(data_missing)\n",
    "print(\"DataFrame v·ªõi d·ªØ li·ªáu thi·∫øu:\")\n",
    "print(df_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c84ff4",
   "metadata": {},
   "source": [
    "### C√°c ph∆∞∆°ng th·ª©c ph√°t hi·ªán d·ªØ li·ªáu thi·∫øu trong pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ba1fa",
   "metadata": {},
   "source": [
    "Pandas cung c·∫•p c√°c ph∆∞∆°ng th·ª©c chuy√™n d·ª•ng ƒë·ªÉ **ph√°t hi·ªán v√† ki·ªÉm tra d·ªØ li·ªáu thi·∫øu**:\n",
    "\n",
    "| Ph∆∞∆°ng th·ª©c | M√¥ t·∫£ | Tr·∫£ v·ªÅ |\n",
    "|-------------|-------|--------|\n",
    "| `isna()` / `isnull()` | Ki·ªÉm tra t·ª´ng ph·∫ßn t·ª≠ c√≥ thi·∫øu kh√¥ng | Boolean DataFrame/Series |\n",
    "| `notna()` / `notnull()` | Ki·ªÉm tra t·ª´ng ph·∫ßn t·ª≠ c√≥ d·ªØ li·ªáu kh√¥ng | Boolean DataFrame/Series |\n",
    "| `isna().sum()` | ƒê·∫øm s·ªë l∆∞·ª£ng d·ªØ li·ªáu thi·∫øu theo c·ªôt | Series v·ªõi s·ªë l∆∞·ª£ng |\n",
    "| `isna().any()` | Ki·ªÉm tra c√≥ c·ªôt n√†o thi·∫øu d·ªØ li·ªáu kh√¥ng | Boolean Series |\n",
    "\n",
    "**üìä H√£y xem c√°ch s·ª≠ d·ª•ng c√°c ph∆∞∆°ng th·ª©c n√†y:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda1550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# T·∫°o DataFrame m·∫´u v·ªõi d·ªØ li·ªáu thi·∫øu\n",
    "data_missing = {\n",
    "    'T√™n': ['An', 'B√¨nh', 'Chi', 'D≈©ng', 'Eva'],\n",
    "    'Tu·ªïi': [25, None, 30, 28, None],\n",
    "    'L∆∞∆°ng': [15000000, 18000000, None, 22000000, 16000000],\n",
    "    'Ph√≤ng ban': ['IT', 'Marketing', None, 'IT', 'Marketing']\n",
    "}\n",
    "\n",
    "df_missing = pd.DataFrame(data_missing)\n",
    "print(\"DataFrame v·ªõi d·ªØ li·ªáu thi·∫øu:\")\n",
    "print(df_missing)\n",
    "\n",
    "# 1. Ki·ªÉm tra d·ªØ li·ªáu thi·∫øu - tr·∫£ v·ªÅ Boolean DataFrame\n",
    "print(\"1. Ki·ªÉm tra t·ª´ng ph·∫ßn t·ª≠ c√≥ thi·∫øu d·ªØ li·ªáu kh√¥ng:\")\n",
    "print(df_missing.isna())\n",
    "\n",
    "# 2. ƒê·∫øm s·ªë l∆∞·ª£ng d·ªØ li·ªáu thi·∫øu theo t·ª´ng c·ªôt\n",
    "print(\"2. S·ªë l∆∞·ª£ng d·ªØ li·ªáu thi·∫øu theo c·ªôt:\")\n",
    "print(df_missing.isna().sum())\n",
    "\n",
    "# 3. Ki·ªÉm tra c·ªôt n√†o c√≥ d·ªØ li·ªáu thi·∫øu\n",
    "print(\"3. C√°c c·ªôt c√≥ d·ªØ li·ªáu thi·∫øu:\")\n",
    "print(df_missing.isna().any())\n",
    "\n",
    "# 4. T·ªïng s·ªë d·ªØ li·ªáu thi·∫øu trong to√†n b·ªô DataFrame\n",
    "print(\"4. T·ªïng s·ªë d·ªØ li·ªáu thi·∫øu:\")\n",
    "print(df_missing.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b23b439",
   "metadata": {},
   "source": [
    "### C√°c ph∆∞∆°ng ph√°p x·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27c281",
   "metadata": {},
   "source": [
    "**üõ†Ô∏è C√≥ 3 c√°ch ch√≠nh ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu:**\n",
    "\n",
    "1. **Lo·∫°i b·ªè** (*Deletion*): X√≥a c√°c h√†ng/c·ªôt c√≥ d·ªØ li·ªáu thi·∫øu\n",
    "2. **Thay th·∫ø** (*Imputation*): ƒêi·ªÅn c√°c gi√° tr·ªã ph√π h·ª£p v√†o ch·ªó thi·∫øu\n",
    "3. **D·ª± ƒëo√°n** (*Prediction*): S·ª≠ d·ª•ng thu·∫≠t to√°n ƒë·ªÉ d·ª± ƒëo√°n gi√° tr·ªã thi·∫øu\n",
    "\n",
    "**‚öñÔ∏è L·ª±a ch·ªçn ph∆∞∆°ng ph√°p ph√π h·ª£p:**\n",
    "\n",
    "- **T·ª∑ l·ªá d·ªØ li·ªáu thi·∫øu nh·ªè (<5%)**: C√≥ th·ªÉ lo·∫°i b·ªè\n",
    "- **D·ªØ li·ªáu thi·∫øu ng·∫´u nhi√™n**: Thay th·∫ø b·∫±ng mean/median/mode\n",
    "- **D·ªØ li·ªáu thi·∫øu c√≥ pattern**: C·∫ßn ph√¢n t√≠ch nguy√™n nh√¢n v√† x·ª≠ l√Ω c·∫©n th·∫≠n\n",
    "- **D·ªØ li·ªáu quan tr·ªçng**: S·ª≠ d·ª•ng k·ªπ thu·∫≠t d·ª± ƒëo√°n ph·ª©c t·∫°p h∆°n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8cddba",
   "metadata": {},
   "source": [
    "#### **Ph∆∞∆°ng ph√°p 1: Lo·∫°i b·ªè d·ªØ li·ªáu thi·∫øu (`dropna`)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b627f4",
   "metadata": {},
   "source": [
    "Ph∆∞∆°ng th·ª©c `dropna()` cho ph√©p lo·∫°i b·ªè c√°c h√†ng ho·∫∑c c·ªôt c√≥ d·ªØ li·ªáu thi·∫øu:\n",
    "\n",
    "**üìã C√°c tham s·ªë quan tr·ªçng c·ªßa `dropna()`:**\n",
    "\n",
    "| Tham s·ªë | Gi√° tr·ªã | M√¥ t·∫£ |\n",
    "|---------|---------|-------|\n",
    "| `axis` | 0 (default) / 1 | 0: lo·∫°i b·ªè h√†ng, 1: lo·∫°i b·ªè c·ªôt |\n",
    "| `how` | 'any' (default) / 'all' | 'any': c√≥ √≠t nh·∫•t 1 NaN, 'all': to√†n b·ªô l√† NaN |\n",
    "| `subset` | list | Ch·ªâ x√©t d·ªØ li·ªáu thi·∫øu trong c√°c c·ªôt ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh |\n",
    "| `thresh` | int | S·ªë l∆∞·ª£ng gi√° tr·ªã kh√¥ng null t·ªëi thi·ªÉu c·∫ßn c√≥ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d8b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# T·∫°o DataFrame m·∫´u v·ªõi d·ªØ li·ªáu thi·∫øu\n",
    "data_missing = {\n",
    "    'T√™n': ['An', 'B√¨nh', 'Chi', 'D≈©ng', 'Eva'],\n",
    "    'Tu·ªïi': [25, None, 30, 28, None],\n",
    "    'L∆∞∆°ng': [15000000, 18000000, None, 22000000, 16000000],\n",
    "    'Ph√≤ng ban': ['IT', 'Marketing', None, 'IT', 'Marketing']\n",
    "}\n",
    "\n",
    "df_missing = pd.DataFrame(data_missing)\n",
    "print(\"DataFrame v·ªõi d·ªØ li·ªáu thi·∫øu:\")\n",
    "print(df_missing)\n",
    "\n",
    "# 1. Lo·∫°i b·ªè t·∫•t c·∫£ h√†ng c√≥ √≠t nh·∫•t 1 gi√° tr·ªã thi·∫øu\n",
    "print(\"1. Lo·∫°i b·ªè h√†ng c√≥ d·ªØ li·ªáu thi·∫øu (how='any'):\")\n",
    "df_drop_any = df_missing.dropna()\n",
    "print(df_drop_any)\n",
    "print(f\"S·ªë h√†ng c√≤n l·∫°i: {len(df_drop_any)}\")\n",
    "\n",
    "# 2. Lo·∫°i b·ªè h√†ng ch·ªâ khi T·∫§T C·∫¢ gi√° tr·ªã ƒë·ªÅu thi·∫øu\n",
    "print(\"2. Lo·∫°i b·ªè h√†ng khi t·∫•t c·∫£ gi√° tr·ªã ƒë·ªÅu thi·∫øu (how='all'):\")\n",
    "df_drop_all = df_missing.dropna(how='all')\n",
    "print(df_drop_all)\n",
    "print(f\"S·ªë h√†ng c√≤n l·∫°i: {len(df_drop_all)}\")\n",
    "\n",
    "# 3. Lo·∫°i b·ªè c·ªôt c√≥ d·ªØ li·ªáu thi·∫øu\n",
    "print(\"3. Lo·∫°i b·ªè c·ªôt c√≥ d·ªØ li·ªáu thi·∫øu (axis=1):\")\n",
    "df_drop_cols = df_missing.dropna(axis=1)\n",
    "print(df_drop_cols)\n",
    "print(f\"S·ªë c·ªôt c√≤n l·∫°i: {len(df_drop_cols.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee315f8",
   "metadata": {},
   "source": [
    "#### **Ph∆∞∆°ng ph√°p 2: Thay th·∫ø d·ªØ li·ªáu thi·∫øu (`fillna`)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff3235",
   "metadata": {},
   "source": [
    "Ph∆∞∆°ng th·ª©c `fillna()` cho ph√©p **thay th·∫ø d·ªØ li·ªáu thi·∫øu** b·∫±ng c√°c gi√° tr·ªã c·ª• th·ªÉ:\n",
    "\n",
    "**üîß C√°c chi·∫øn l∆∞·ª£c thay th·∫ø ph·ªï bi·∫øn:**\n",
    "\n",
    "| Chi·∫øn l∆∞·ª£c | ·ª®ng d·ª•ng | V√≠ d·ª• |\n",
    "|------------|----------|-------|\n",
    "| **Gi√° tr·ªã c·ªë ƒë·ªãnh** | Thay th·∫ø b·∫±ng m·ªôt gi√° tr·ªã nh·∫•t ƒë·ªãnh | `fillna(0)`, `fillna('Unknown')` |\n",
    "| **Gi√° tr·ªã trung b√¨nh** | D·ªØ li·ªáu s·ªë, ph√¢n ph·ªëi chu·∫©n | `fillna(df['col'].mean())` |\n",
    "| **Gi√° tr·ªã trung v·ªã** | D·ªØ li·ªáu s·ªë, c√≥ outliers | `fillna(df['col'].median())` |\n",
    "| **Gi√° tr·ªã ph·ªï bi·∫øn nh·∫•t** | D·ªØ li·ªáu ph√¢n lo·∫°i | `fillna(df['col'].mode()[0])` |\n",
    "| **Forward fill** | D·ªØ li·ªáu chu·ªói th·ªùi gian | `fillna(method='ffill')` |\n",
    "| **Backward fill** | D·ªØ li·ªáu chu·ªói th·ªùi gian | `fillna(method='bfill')` |\n",
    "\n",
    "**üìä H√£y xem c√°c v√≠ d·ª• c·ª• th·ªÉ:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d63477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# T·∫°o DataFrame m·∫´u v·ªõi d·ªØ li·ªáu thi·∫øu\n",
    "data_missing = {\n",
    "    'T√™n': ['An', 'B√¨nh', 'Chi', 'D≈©ng', 'Eva'],\n",
    "    'Tu·ªïi': [25, None, 30, 28, None],\n",
    "    'L∆∞∆°ng': [15000000, 18000000, None, 22000000, 16000000],\n",
    "    'Ph√≤ng ban': ['IT', 'Marketing', None, 'IT', 'Marketing']\n",
    "}\n",
    "\n",
    "df_missing = pd.DataFrame(data_missing)\n",
    "print(\"DataFrame v·ªõi d·ªØ li·ªáu thi·∫øu:\")\n",
    "print(df_missing)\n",
    "\n",
    "# 1. Thay th·∫ø b·∫±ng gi√° tr·ªã c·ªë ƒë·ªãnh\n",
    "print(\"\\n1. Thay th·∫ø b·∫±ng gi√° tr·ªã c·ªë ƒë·ªãnh:\")\n",
    "df_fill_fixed = df_missing.fillna({'Tu·ªïi': 0, 'L∆∞∆°ng': 0, 'Ph√≤ng ban': 'Ch∆∞a x√°c ƒë·ªãnh'})\n",
    "print(df_fill_fixed)\n",
    "\n",
    "# 2. Thay th·∫ø b·∫±ng gi√° tr·ªã trung b√¨nh (cho d·ªØ li·ªáu s·ªë)\n",
    "print(\"\\n2. Thay th·∫ø b·∫±ng gi√° tr·ªã trung b√¨nh:\")\n",
    "df_fill_mean = df_missing.copy()\n",
    "df_fill_mean['Tu·ªïi'] = df_fill_mean['Tu·ªïi'].fillna(df_fill_mean['Tu·ªïi'].mean())\n",
    "df_fill_mean['L∆∞∆°ng'] = df_fill_mean['L∆∞∆°ng'].fillna(df_fill_mean['L∆∞∆°ng'].mean())\n",
    "print(df_fill_mean)\n",
    "print(f\"Tu·ªïi trung b√¨nh: {df_missing['Tu·ªïi'].mean():.1f}\")\n",
    "print(f\"L∆∞∆°ng trung b√¨nh: {df_missing['L∆∞∆°ng'].mean():,.0f}\")\n",
    "\n",
    "# 3. Thay th·∫ø b·∫±ng gi√° tr·ªã trung v·ªã (b·ªÅn v·ªØng v·ªõi outliers)\n",
    "print(\"\\n3. Thay th·∫ø b·∫±ng gi√° tr·ªã trung v·ªã:\")\n",
    "df_fill_median = df_missing.copy()\n",
    "df_fill_median['Tu·ªïi'] = df_fill_median['Tu·ªïi'].fillna(df_fill_median['Tu·ªïi'].median())\n",
    "df_fill_median['L∆∞∆°ng'] = df_fill_median['L∆∞∆°ng'].fillna(df_fill_median['L∆∞∆°ng'].median())\n",
    "print(df_fill_median)\n",
    "\n",
    "# 4. Thay th·∫ø b·∫±ng gi√° tr·ªã ph·ªï bi·∫øn nh·∫•t (mode) - cho d·ªØ li·ªáu ph√¢n lo·∫°i\n",
    "print(\"\\n4. Thay th·∫ø b·∫±ng gi√° tr·ªã ph·ªï bi·∫øn nh·∫•t (mode):\")\n",
    "df_fill_mode = df_missing.copy()\n",
    "df_fill_mode['Ph√≤ng ban'] = df_fill_mode['Ph√≤ng ban'].fillna(df_fill_mode['Ph√≤ng ban'].mode()[0])\n",
    "print(df_fill_mode)\n",
    "print(f\"Ph√≤ng ban ph·ªï bi·∫øn nh·∫•t: {df_missing['Ph√≤ng ban'].mode()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff8a4a",
   "metadata": {},
   "source": [
    "#### Ph∆∞∆°ng ph√°p 3: S·ª≠ d·ª•ng m√¥ h√¨nh d·ª± ƒëo√°n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68faa286",
   "metadata": {},
   "source": [
    "S·ª≠ d·ª•ng c√°c m√¥ h√¨nh **d·ª± ƒëo√°n** ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng gi√° tr·ªã thi·∫øu.\n",
    "\n",
    "**üîß C√°c chi·∫øn l∆∞·ª£c thay th·∫ø ph·ªï bi·∫øn:**\n",
    "\n",
    "| Chi·∫øn l∆∞·ª£c | ·ª®ng d·ª•ng | V√≠ d·ª• |\n",
    "|------------|----------|-------|\n",
    "| **H·ªìi quy** | D·ªØ li·ªáu s·ªë | S·ª≠ d·ª•ng h·ªìi quy tuy·∫øn t√≠nh ƒë·ªÉ d·ª± ƒëo√°n gi√° tr·ªã |\n",
    "| **Ph√¢n lo·∫°i** | D·ªØ li·ªáu ph√¢n lo·∫°i | S·ª≠ d·ª•ng c√¢y quy·∫øt ƒë·ªãnh ƒë·ªÉ ph√¢n lo·∫°i gi√° tr·ªã |\n",
    "| **Ph√¢n t√≠ch th·ªëng k√™** | D·ªØ li·ªáu s·ªë | S·ª≠ d·ª•ng th·ªëng k√™ ƒë·ªÉ d·ª± ƒëo√°n gi√° tr·ªã |\n",
    "\n",
    "**üìä H√£y xem c√°c v√≠ d·ª• c·ª• th·ªÉ:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d807bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# T·∫°o DataFrame m·∫´u v·ªõi d·ªØ li·ªáu thi·∫øu\n",
    "data_missing = {\n",
    "    'T√™n': ['An', 'B√¨nh', 'Chi', 'D≈©ng', 'Eva'],\n",
    "    'Tu·ªïi': [25, None, 30, 28, None],\n",
    "    'L∆∞∆°ng': [15000000, 18000000, None, 22000000, 16000000],\n",
    "    'Ph√≤ng ban': ['IT', 'Marketing', None, 'IT', 'Marketing']\n",
    "}\n",
    "\n",
    "df_missing = pd.DataFrame(data_missing)\n",
    "print(\"DataFrame v·ªõi d·ªØ li·ªáu thi·∫øu:\")\n",
    "print(df_missing)\n",
    "\n",
    "# Chu·∫©n b·ªã d·ªØ li·ªáu cho m√¥ h√¨nh d·ª± ƒëo√°n\n",
    "print(\"\\n1. D·ª± ƒëo√°n Tu·ªïi d·ª±a tr√™n L∆∞∆°ng v√† Ph√≤ng ban:\")\n",
    "\n",
    "# T·∫°o b·∫£n sao ƒë·ªÉ x·ª≠ l√Ω\n",
    "df_predict = df_missing.copy()\n",
    "\n",
    "# Encode categorical data\n",
    "le = LabelEncoder()\n",
    "df_predict['Ph√≤ng ban_encoded'] = le.fit_transform(df_predict['Ph√≤ng ban'].fillna('Unknown'))\n",
    "\n",
    "# D·ª± ƒëo√°n Tu·ªïi\n",
    "# L·∫•y d·ªØ li·ªáu kh√¥ng thi·∫øu ƒë·ªÉ train\n",
    "train_data_age = df_predict[df_predict['Tu·ªïi'].notna()]\n",
    "missing_data_age = df_predict[df_predict['Tu·ªïi'].isna()]\n",
    "\n",
    "if len(train_data_age) > 0 and len(missing_data_age) > 0:\n",
    "    # Train model\n",
    "    X_train_age = train_data_age[['L∆∞∆°ng', 'Ph√≤ng ban_encoded']].fillna(0)\n",
    "    y_train_age = train_data_age['Tu·ªïi']\n",
    "    \n",
    "    model_age = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "    model_age.fit(X_train_age, y_train_age)\n",
    "    \n",
    "    # Predict\n",
    "    X_missing_age = missing_data_age[['L∆∞∆°ng', 'Ph√≤ng ban_encoded']].fillna(0)\n",
    "    predicted_ages = model_age.predict(X_missing_age)\n",
    "    \n",
    "    # Fill predicted values\n",
    "    df_predict.loc[df_predict['Tu·ªïi'].isna(), 'Tu·ªïi'] = predicted_ages\n",
    "    \n",
    "    print(\"D·ªØ li·ªáu sau khi d·ª± ƒëo√°n Tu·ªïi:\")\n",
    "    print(df_predict[['T√™n', 'Tu·ªïi', 'L∆∞∆°ng', 'Ph√≤ng ban']])\n",
    "\n",
    "print(\"\\n2. D·ª± ƒëo√°n L∆∞∆°ng d·ª±a tr√™n Tu·ªïi v√† Ph√≤ng ban:\")\n",
    "\n",
    "# Reset df_predict\n",
    "df_predict = df_missing.copy()\n",
    "df_predict['Ph√≤ng ban_encoded'] = le.fit_transform(df_predict['Ph√≤ng ban'].fillna('Unknown'))\n",
    "\n",
    "# D·ª± ƒëo√°n L∆∞∆°ng\n",
    "train_data_salary = df_predict[df_predict['L∆∞∆°ng'].notna()]\n",
    "missing_data_salary = df_predict[df_predict['L∆∞∆°ng'].isna()]\n",
    "\n",
    "if len(train_data_salary) > 0 and len(missing_data_salary) > 0:\n",
    "    X_train_salary = train_data_salary[['Tu·ªïi', 'Ph√≤ng ban_encoded']].fillna(0)\n",
    "    y_train_salary = train_data_salary['L∆∞∆°ng']\n",
    "    \n",
    "    model_salary = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "    model_salary.fit(X_train_salary, y_train_salary)\n",
    "    \n",
    "    X_missing_salary = missing_data_salary[['Tu·ªïi', 'Ph√≤ng ban_encoded']].fillna(0)\n",
    "    predicted_salaries = model_salary.predict(X_missing_salary)\n",
    "    \n",
    "    df_predict.loc[df_predict['L∆∞∆°ng'].isna(), 'L∆∞∆°ng'] = predicted_salaries\n",
    "    \n",
    "    print(\"D·ªØ li·ªáu sau khi d·ª± ƒëo√°n L∆∞∆°ng:\")\n",
    "    print(df_predict[['T√™n', 'Tu·ªïi', 'L∆∞∆°ng', 'Ph√≤ng ban']])\n",
    "\n",
    "print(\"\\n3. S·ª≠ d·ª•ng KNN Imputation (ph∆∞∆°ng ph√°p n√¢ng cao):\")\n",
    "\n",
    "# Chu·∫©n b·ªã d·ªØ li·ªáu s·ªë cho KNN\n",
    "df_knn = df_missing.copy()\n",
    "df_knn['Ph√≤ng ban_encoded'] = le.fit_transform(df_knn['Ph√≤ng ban'].fillna('Unknown'))\n",
    "\n",
    "# Ch·ªâ l·∫•y c√°c c·ªôt s·ªë\n",
    "numerical_cols = ['Tu·ªïi', 'L∆∞∆°ng', 'Ph√≤ng ban_encoded']\n",
    "df_numerical = df_knn[numerical_cols]\n",
    "\n",
    "# √Åp d·ª•ng KNN Imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=2)\n",
    "df_knn_filled = knn_imputer.fit_transform(df_numerical)\n",
    "\n",
    "# T·∫°o DataFrame k·∫øt qu·∫£\n",
    "df_knn_result = df_missing.copy()\n",
    "df_knn_result['Tu·ªïi'] = df_knn_filled[:, 0]\n",
    "df_knn_result['L∆∞∆°ng'] = df_knn_filled[:, 1]\n",
    "\n",
    "print(\"D·ªØ li·ªáu sau KNN Imputation:\")\n",
    "print(df_knn_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1372d7dc",
   "metadata": {},
   "source": [
    "## X·ª≠ l√Ω d·ªØ li·ªáu tr√πng l·∫∑p (Duplicate Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77a20c",
   "metadata": {},
   "source": [
    "### Hi·ªÉu v·ªÅ d·ªØ li·ªáu tr√πng l·∫∑p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf5e95",
   "metadata": {},
   "source": [
    "**üîÑ D·ªØ li·ªáu tr√πng l·∫∑p l√† g√¨?**\n",
    "\n",
    "D·ªØ li·ªáu tr√πng l·∫∑p (*duplicate data*) l√† nh·ªØng **h√†ng c√≥ gi√° tr·ªã gi·ªëng h·ªát nhau** tr√™n t·∫•t c·∫£ ho·∫∑c m·ªôt s·ªë c·ªôt nh·∫•t ƒë·ªãnh. D·ªØ li·ªáu tr√πng l·∫∑p c√≥ th·ªÉ xu·∫•t hi·ªán do:\n",
    "\n",
    "- **L·ªói nh·∫≠p li·ªáu**: Ng∆∞·ªùi d√πng v√¥ t√¨nh nh·∫≠p c√πng m·ªôt th√¥ng tin nhi·ªÅu l·∫ßn\n",
    "- **L·ªói h·ªá th·ªëng**: H·ªá th·ªëng ghi nh·∫≠n c√πng m·ªôt s·ª± ki·ªán nhi·ªÅu l·∫ßn  \n",
    "- **G·ªôp d·ªØ li·ªáu**: Khi k·∫øt h·ª£p nhi·ªÅu ngu·ªìn d·ªØ li·ªáu c√≥ th√¥ng tin ch·ªìng ch√©o\n",
    "- **L·ªói thu th·∫≠p**: C·∫£m bi·∫øn ho·∫∑c thi·∫øt b·ªã ghi nh·∫≠n d·ªØ li·ªáu b·ªã l·∫∑p\n",
    "\n",
    "**‚ö†Ô∏è T√°c ƒë·ªông c·ªßa d·ªØ li·ªáu tr√πng l·∫∑p**\n",
    "\n",
    "- **Thi√™n l·ªách ph√¢n t√≠ch**: M·ªôt quan s√°t ƒë∆∞·ª£c t√≠nh nhi·ªÅu l·∫ßn, l√†m m√©o m√≥ k·∫øt qu·∫£\n",
    "- **Gi·∫£m hi·ªáu qu·∫£ t√≠nh to√°n**: X·ª≠ l√Ω d·ªØ li·ªáu th·ª´a l√†m ch·∫≠m thu·∫≠t to√°n\n",
    "- **TƒÉng k√≠ch th∆∞·ªõc d·ªØ li·ªáu**: L√£ng ph√≠ b·ªô nh·ªõ v√† kh√¥ng gian l∆∞u tr·ªØ\n",
    "- **·∫¢nh h∆∞·ªüng m√¥ h√¨nh**: Machine learning c√≥ th·ªÉ h·ªçc sai t·ª´ d·ªØ li·ªáu l·∫∑p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a749036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Kh·ªüi t·∫°o d·ªØ li·ªáu m·∫´u\n",
    "duplicate_data = {\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Alice'],\n",
    "    'age': [25, 30, 35, 25],\n",
    "    'city': ['New York', 'Los Angeles', 'Chicago', 'New York']\n",
    "}\n",
    "\n",
    "duplicate_data = pd.DataFrame(duplicate_data)\n",
    "\n",
    "# In d·ªØ li·ªáu m·∫´u\n",
    "print(duplicate_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed9e8f",
   "metadata": {},
   "source": [
    "### C√°c ph∆∞∆°ng ph√°p ph√°t hi·ªán v√† x·ª≠ l√Ω d·ªØ li·ªáu tr√πng l·∫∑p trong pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9479d87",
   "metadata": {},
   "source": [
    "Pandas cung c·∫•p c√°c ph∆∞∆°ng th·ª©c ƒë·ªÉ ph√°t hi·ªán v√† x·ª≠ l√Ω d·ªØ li·ªáu tr√πng l·∫∑p:\n",
    "\n",
    "| Ph∆∞∆°ng th·ª©c | M√¥ t·∫£ | Tr·∫£ v·ªÅ |\n",
    "|-------------|-------|--------|\n",
    "| `duplicated()` | Ki·ªÉm tra t·ª´ng h√†ng c√≥ b·ªã tr√πng l·∫∑p kh√¥ng | Boolean Series |\n",
    "| `drop_duplicates()` | Lo·∫°i b·ªè c√°c h√†ng tr√πng l·∫∑p | DataFrame kh√¥ng tr√πng l·∫∑p |\n",
    "\n",
    "**üìä H√£y xem c√°ch s·ª≠ d·ª•ng c√°c ph∆∞∆°ng th·ª©c n√†y:**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bdb44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Kh·ªüi t·∫°o d·ªØ li·ªáu m·∫´u\n",
    "duplicate_data = {\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Alice'],\n",
    "    'age': [25, 30, 35, 25],\n",
    "    'city': ['New York', 'Los Angeles', 'Chicago', 'New York']\n",
    "}\n",
    "\n",
    "duplicate_data = pd.DataFrame(duplicate_data)\n",
    "\n",
    "# In d·ªØ li·ªáu m·∫´u\n",
    "print(duplicate_data)\n",
    "\n",
    "# Ph√°t hi·ªán duplicate rows\n",
    "print(\"Ph√°t hi·ªán duplicate rows:\")\n",
    "print(duplicate_data.duplicated())\n",
    "\n",
    "print(\"\\nC√°c h√†ng b·ªã duplicate:\")\n",
    "print(duplicate_data[duplicate_data.duplicated()])\n",
    "\n",
    "print(\"\\nƒê·∫øm s·ªë l∆∞·ª£ng duplicate:\")\n",
    "print(f\"T·ªïng s·ªë duplicate: {duplicate_data.duplicated().sum()}\")\n",
    "\n",
    "print(\"\\n X·ª≠ l√Ω duplicate b·∫±ng c√°ch lo·∫°i b·ªè:\")\n",
    "print(duplicate_data.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e951b8d",
   "metadata": {},
   "source": [
    "## Bi·∫øn ƒë·ªïi v√† chu·∫©n h√≥a d·ªØ li·ªáu (Data Transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062b07aa",
   "metadata": {},
   "source": [
    "### T·ªïng quan v·ªÅ bi·∫øn ƒë·ªïi d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c449fd38",
   "metadata": {},
   "source": [
    "**üîÑ Bi·∫øn ƒë·ªïi d·ªØ li·ªáu l√† g√¨?**\n",
    "\n",
    "Bi·∫øn ƒë·ªïi d·ªØ li·ªáu (*data transformation*) l√† qu√° tr√¨nh **chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu** t·ª´ ƒë·ªãnh d·∫°ng n√†y sang ƒë·ªãnh d·∫°ng kh√°c ƒë·ªÉ:\n",
    "\n",
    "- **C·∫£i thi·ªán ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu**: L√†m cho d·ªØ li·ªáu ph√π h·ª£p h∆°n cho ph√¢n t√≠ch\n",
    "- **Chu·∫©n h√≥a thang ƒëo**: ƒê∆∞a c√°c bi·∫øn v·ªÅ c√πng m·ªôt thang ƒëo\n",
    "- **Gi·∫£m nhi·ªÖu**: Lo·∫°i b·ªè c√°c bi·∫øn ƒë·ªông kh√¥ng mong mu·ªën\n",
    "- **T·∫°o bi·∫øn m·ªõi**: K·∫øt h·ª£p ho·∫∑c bi·∫øn ƒë·ªïi bi·∫øn hi·ªán c√≥ ƒë·ªÉ t·∫°o th√¥ng tin m·ªõi\n",
    "\n",
    "**üéØ C√°c m·ª•c ti√™u ch√≠nh:**\n",
    "\n",
    "1. **Normalization**: ƒê∆∞a d·ªØ li·ªáu v·ªÅ kho·∫£ng [0,1]\n",
    "2. **Standardization**: ƒê∆∞a d·ªØ li·ªáu v·ªÅ ph√¢n ph·ªëi chu·∫©n (mean=0, std=1) \n",
    "3. **Scaling**: ƒêi·ªÅu ch·ªânh thang ƒëo cho ph√π h·ª£p\n",
    "4. **Encoding**: Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu ph√¢n lo·∫°i th√†nh s·ªë\n",
    "\n",
    "**üìã Khi n√†o c·∫ßn bi·∫øn ƒë·ªïi d·ªØ li·ªáu:**\n",
    "\n",
    "- C√°c bi·∫øn c√≥ **ƒë∆°n v·ªã ƒëo kh√°c nhau** (VND, USD, kg, cm)\n",
    "- D·ªØ li·ªáu c√≥ **ph·∫°m vi gi√° tr·ªã ch√™nh l·ªách l·ªõn** (1-10 vs 1000-10000)\n",
    "- S·ª≠ d·ª•ng **thu·∫≠t to√°n nh·∫°y c·∫£m v·ªõi thang ƒëo** (KNN, SVM, Neural Networks)\n",
    "- **C·∫£i thi·ªán hi·ªáu su·∫•t** c·ªßa m√¥ h√¨nh machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e529a255",
   "metadata": {},
   "source": [
    "### Min-Max Normalization (Chu·∫©n h√≥a Min-Max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05eac5",
   "metadata": {},
   "source": [
    "**üìê C√¥ng th·ª©c Min-Max Normalization:**\n",
    "\n",
    "$$X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "**üéØ ƒê·∫∑c ƒëi·ªÉm:**\n",
    "- ƒê∆∞a d·ªØ li·ªáu v·ªÅ kho·∫£ng **[0, 1]**\n",
    "- **B·∫£o to√†n ph√¢n ph·ªëi** g·ªëc c·ªßa d·ªØ li·ªáu\n",
    "- **Nh·∫°y c·∫£m v·ªõi outliers** (gi√° tr·ªã ngo·∫°i lai)\n",
    "- Ph√π h·ª£p khi bi·∫øt **gi·ªõi h·∫°n tr√™n v√† d∆∞·ªõi** c·ªßa d·ªØ li·ªáu\n",
    "\n",
    "**üîß S·ª≠ d·ª•ng `MinMaxScaler` t·ª´ scikit-learn:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import th∆∞ vi·ªán c·∫ßn thi·∫øt cho chu·∫©n h√≥a\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu m·∫´u v·ªõi c√°c thang ƒëo kh√°c nhau\n",
    "data_transform = {\n",
    "    'L∆∞∆°ng': [15000000, 25000000, 30000000, 18000000, 22000000],\n",
    "    'Tu·ªïi': [25, 35, 40, 28, 32],\n",
    "    'Kinh nghi·ªám': [2, 8, 12, 3, 6]\n",
    "}\n",
    "\n",
    "df_transform = pd.DataFrame(data_transform)\n",
    "print(\"D·ªØ li·ªáu g·ªëc:\")\n",
    "print(df_transform)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "norm_data_scaled = df_transform.copy()\n",
    "norm_data_scaled[['L∆∞∆°ng', 'Tu·ªïi', 'Kinh nghi·ªám']] = scaler.fit_transform(df_transform[['L∆∞∆°ng', 'Tu·ªïi', 'Kinh nghi·ªám']])\n",
    "\n",
    "print(\"Min-Max Normalization (0-1):\")\n",
    "print(norm_data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9978ba47",
   "metadata": {},
   "source": [
    "### Standard Scaler (Z-score Standardization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce3d87",
   "metadata": {},
   "source": [
    "**üìä C√¥ng th·ª©c Z-score Standardization:**\n",
    "\n",
    "$$Z = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "Trong ƒë√≥:\n",
    "- $X$ = gi√° tr·ªã g·ªëc\n",
    "- $\\mu$ = gi√° tr·ªã trung b√¨nh (mean)\n",
    "- $\\sigma$ = ƒë·ªô l·ªách chu·∫©n (standard deviation)\n",
    "\n",
    "**üéØ ƒê·∫∑c ƒëi·ªÉm:**\n",
    "- ƒê∆∞a d·ªØ li·ªáu v·ªÅ **ph√¢n ph·ªëi chu·∫©n** v·ªõi mean=0, std=1\n",
    "- **Kh√¥ng b·ªã ·∫£nh h∆∞·ªüng** b·ªüi outliers nhi·ªÅu nh∆∞ Min-Max\n",
    "- **B·∫£o to√†n th√¥ng tin** v·ªÅ ph√¢n ph·ªëi g·ªëc\n",
    "- Ph√π h·ª£p v·ªõi **c√°c thu·∫≠t to√°n gi·∫£ ƒë·ªãnh ph√¢n ph·ªëi chu·∫©n** (Linear Regression, Logistic Regression)\n",
    "\n",
    "**üîß S·ª≠ d·ª•ng `StandardScaler` t·ª´ scikit-learn:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu m·∫´u v·ªõi c√°c thang ƒëo kh√°c nhau\n",
    "data_transform = {\n",
    "    'L∆∞∆°ng': [15000000, 25000000, 30000000, 18000000, 22000000],\n",
    "    'Tu·ªïi': [25, 35, 40, 28, 32],\n",
    "    'Kinh nghi·ªám': [2, 8, 12, 3, 6]\n",
    "}\n",
    "\n",
    "# √Åp d·ª•ng Standard Scaler\n",
    "scaler_std = StandardScaler()\n",
    "\n",
    "# Fit v√† transform d·ªØ li·ªáu\n",
    "df_standard = df_transform.copy()\n",
    "df_standard[['L∆∞∆°ng', 'Tu·ªïi', 'Kinh nghi·ªám']] = scaler_std.fit_transform(\n",
    "    df_transform[['L∆∞∆°ng', 'Tu·ªïi', 'Kinh nghi·ªám']]\n",
    ")\n",
    "\n",
    "print(\"D·ªØ li·ªáu sau Standard Scaling:\")\n",
    "print(df_standard)\n",
    "print(f\"\\nM√¥ t·∫£ th·ªëng k√™ sau standardization:\")\n",
    "print(df_standard.describe().round(4))\n",
    "\n",
    "# Ki·ªÉm tra mean ‚âà 0 v√† std ‚âà 1\n",
    "print(f\"\\nMean c·ªßa c√°c c·ªôt: {df_standard[['L∆∞∆°ng', 'Tu·ªïi', 'Kinh nghi·ªám']].mean().round(4).values}\")\n",
    "print(f\"Std c·ªßa c√°c c·ªôt: {df_standard[['L∆∞∆°ng', 'Tu·ªïi', 'Kinh nghi·ªám']].std().round(4).values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d372e",
   "metadata": {},
   "source": [
    "### Robust Scaler (S·ª≠ d·ª•ng Median v√† IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d20b9",
   "metadata": {},
   "source": [
    "**üìà C√¥ng th·ª©c Robust Scaling:**\n",
    "\n",
    "$$X_{robust} = \\frac{X - X_{median}}{IQR}$$\n",
    "\n",
    "Trong ƒë√≥:\n",
    "- $X_{median}$ = gi√° tr·ªã trung v·ªã (median)\n",
    "- $IQR$ = Interquartile Range = Q3 - Q1\n",
    "\n",
    "**üõ°Ô∏è ƒê·∫∑c ƒëi·ªÉm:**\n",
    "- **R·∫•t b·ªÅn v·ªØng** (*robust*) tr∆∞·ªõc outliers\n",
    "- S·ª≠ d·ª•ng **median thay v√¨ mean**, **IQR thay v√¨ std**\n",
    "- **Kh√¥ng b·ªã m√©o** b·ªüi c√°c gi√° tr·ªã ngo·∫°i lai\n",
    "- Ph√π h·ª£p khi d·ªØ li·ªáu c√≥ **nhi·ªÅu outliers**\n",
    "\n",
    "**üéØ Khi n√†o s·ª≠ d·ª•ng Robust Scaler:**\n",
    "- **D·ªØ li·ªáu c√≥ nhi·ªÅu outliers** \n",
    "- **Kh√¥ng mu·ªën lo·∫°i b·ªè outliers** nh∆∞ng v·∫´n c·∫ßn chu·∫©n h√≥a\n",
    "- **D·ªØ li·ªáu kh√¥ng tu√¢n theo ph√¢n ph·ªëi chu·∫©n**\n",
    "\n",
    "**üîß S·ª≠ d·ª•ng `RobustScaler` t·ª´ scikit-learn:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a796d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu c√≥ outliers\n",
    "data_with_outliers = {\n",
    "    'L∆∞∆°ng': [15000000, 25000000, 30000000, 18000000, 22000000, 200000000],  # outlier: 200M\n",
    "    'Tu·ªïi': [25, 35, 40, 28, 32, 22],\n",
    "    'Kinh nghi·ªám': [2, 8, 12, 3, 6, 1]\n",
    "}\n",
    "\n",
    "df_outliers = pd.DataFrame(data_with_outliers)\n",
    "print(\"D·ªØ li·ªáu c√≥ outliers:\")\n",
    "print(df_outliers)\n",
    "print(f\"\\nM√¥ t·∫£ th·ªëng k√™:\")\n",
    "print(df_outliers.describe())\n",
    "\n",
    "# So s√°nh 3 ph∆∞∆°ng ph√°p scaling\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SO S√ÅNH C√ÅC PH∆Ø∆†NG PH√ÅP SCALING V·ªöI OUTLIERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# MinMax Scaler (b·ªã ·∫£nh h∆∞·ªüng m·∫°nh b·ªüi outliers)\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaled_minmax = scaler_minmax.fit_transform(df_outliers[['L∆∞∆°ng', 'Tu·ªïi', 'Kinh nghi·ªám']])\n",
    "print(\"\\n1. MinMax Scaler (b·ªã ·∫£nh h∆∞·ªüng b·ªüi outliers):\")\n",
    "print(pd.DataFrame(scaled_minmax, columns=['L∆∞∆°ng', 'Tu·ªïi', 'Kinh nghi·ªám']))\n",
    "\n",
    "# Standard Scaler (b·ªã ·∫£nh h∆∞·ªüng m·ªôt ph·∫ßn b·ªüi outliers)\n",
    "scaler_std = StandardScaler()\n",
    "scaled_std = scaler_std.fit_transform(df_outliers[['L∆∞∆°ng', 'Tu·ªïi', 'Kinh nghi·ªám']])\n",
    "print(\"\\n2. Standard Scaler (b·ªã ·∫£nh h∆∞·ªüng m·ªôt ph·∫ßn):\")\n",
    "print(pd.DataFrame(scaled_std, columns=['L∆∞∆°ng', 'Tu·ªïi', 'Kinh nghi·ªám']))\n",
    "\n",
    "# Robust Scaler (√≠t b·ªã ·∫£nh h∆∞·ªüng b·ªüi outliers)\n",
    "scaler_robust = RobustScaler()\n",
    "scaled_robust = scaler_robust.fit_transform(df_outliers[['L∆∞∆°ng', 'Tu·ªïi', 'Kinh nghi·ªám']])\n",
    "print(\"\\n3. Robust Scaler (b·ªÅn v·ªØng tr∆∞·ªõc outliers):\")\n",
    "print(pd.DataFrame(scaled_robust, columns=['L∆∞∆°ng', 'Tu·ªïi', 'Kinh nghi·ªám']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe95fbe",
   "metadata": {},
   "source": [
    "## X·ª≠ l√Ω chu·ªói k√Ω t·ª± (String Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e544b9d0",
   "metadata": {},
   "source": [
    "### T·∫ßm quan tr·ªçng c·ªßa vi·ªác x·ª≠ l√Ω chu·ªói k√Ω t·ª±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4834c8a",
   "metadata": {},
   "source": [
    "**üìù D·ªØ li·ªáu chu·ªói k√Ω t·ª± trong th·ª±c t·∫ø**\n",
    "\n",
    "D·ªØ li·ªáu chu·ªói k√Ω t·ª± (*string data*) chi·∫øm m·ªôt ph·∫ßn l·ªõn trong c√°c b·ªô d·ªØ li·ªáu th·ª±c t·∫ø:\n",
    "\n",
    "- **T√™n ng∆∞·ªùi, ƒë·ªãa ch·ªâ**: Th√¥ng tin c√° nh√¢n\n",
    "- **M√¥ t·∫£ s·∫£n ph·∫©m**: Trong th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠\n",
    "- **B√¨nh lu·∫≠n, ƒë√°nh gi√°**: Trong ph√¢n t√≠ch sentiment\n",
    "- **Danh m·ª•c, nh√£n**: D·ªØ li·ªáu ph√¢n lo·∫°i\n",
    "\n",
    "**üßπ C√°c v·∫•n ƒë·ªÅ th∆∞·ªùng g·∫∑p v·ªõi d·ªØ li·ªáu chu·ªói:**\n",
    "\n",
    "1. **Kh√¥ng nh·∫•t qu√°n v·ªÅ ƒë·ªãnh d·∫°ng**: \"iPhone\", \"iphone\", \"IPHONE\"\n",
    "2. **Kho·∫£ng tr·∫Øng th·ª´a**: \"  Apple  \", \"Apple \"\n",
    "3. **K√Ω t·ª± ƒë·∫∑c bi·ªát**: \"email@domain.com\", \"phone: +84-123-456-789\"\n",
    "4. **Vi·∫øt t·∫Øt kh√°c nhau**: \"Dr.\", \"Doctor\", \"BS\"\n",
    "5. **L·ªói ch√≠nh t·∫£**: \"Compnay\" thay v√¨ \"Company\"\n",
    "\n",
    "**üîß Pandas String Accessor (`.str`)**\n",
    "\n",
    "Pandas cung c·∫•p **accessor `.str`** cho ph√©p √°p d·ª•ng c√°c ph∆∞∆°ng th·ª©c x·ª≠ l√Ω chu·ªói l√™n to√†n b·ªô Series:\n",
    "\n",
    "```python\n",
    "# Thay v√¨ l√†m th·ªß c√¥ng t·ª´ng ph·∫ßn t·ª≠\n",
    "for i in range(len(df)):\n",
    "    df.loc[i, 'column'] = df.loc[i, 'column'].upper()\n",
    "\n",
    "# S·ª≠ d·ª•ng .str accessor\n",
    "df['column'] = df['column'].str.upper()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3bfbce",
   "metadata": {},
   "source": [
    "### C√°c ph∆∞∆°ng th·ª©c c∆° b·∫£n x·ª≠ l√Ω chu·ªói"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b337bbe7",
   "metadata": {},
   "source": [
    "**üìã B·∫£ng t·ªïng h·ª£p c√°c ph∆∞∆°ng th·ª©c quan tr·ªçng:**\n",
    "\n",
    "| Ph∆∞∆°ng th·ª©c | M√¥ t·∫£ | V√≠ d·ª• |\n",
    "|-------------|-------|-------|\n",
    "| `.str.lower()` | Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng | `\"HELLO\"` ‚Üí `\"hello\"` |\n",
    "| `.str.upper()` | Chuy·ªÉn v·ªÅ ch·ªØ hoa | `\"hello\"` ‚Üí `\"HELLO\"` |\n",
    "| `.str.title()` | Vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu | `\"hello world\"` ‚Üí `\"Hello World\"` |\n",
    "| `.str.strip()` | Lo·∫°i b·ªè kho·∫£ng tr·∫Øng ƒë·∫ßu/cu·ªëi | `\"  hello  \"` ‚Üí `\"hello\"` |\n",
    "| `.str.replace()` | Thay th·∫ø chu·ªói con | `\"hello\"` ‚Üí `\"hi\"` |\n",
    "| `.str.contains()` | Ki·ªÉm tra ch·ª©a chu·ªói con | `\"hello world\"` contains `\"world\"` ‚Üí `True` |\n",
    "| `.str.startswith()` | Ki·ªÉm tra b·∫Øt ƒë·∫ßu b·∫±ng | `\"hello\"` startswith `\"he\"` ‚Üí `True` |\n",
    "| `.str.endswith()` | Ki·ªÉm tra k·∫øt th√∫c b·∫±ng | `\"hello\"` endswith `\"lo\"` ‚Üí `True` |\n",
    "| `.str.len()` | ƒê·ªô d√†i chu·ªói | `\"hello\"` ‚Üí `5` |\n",
    "| `.str.split()` | T√°ch chu·ªói | `\"a,b,c\"` ‚Üí `[\"a\", \"b\", \"c\"]` |\n",
    "\n",
    "**üî• H√£y xem c√°c v√≠ d·ª• th·ª±c t·∫ø:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c701e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu\n",
    "sample_data = pd.DataFrame({\n",
    "    'id': ['1', '2', '3', '4', '5'],\n",
    "    'score': ['85.5', '90.0', '78.5', '92.0', '88.5'],\n",
    "    'date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05'],\n",
    "    'category': ['A', 'B', 'A', 'C', 'B']\n",
    "})\n",
    "\n",
    "print(\"D·ªØ li·ªáu g·ªëc v√† ki·ªÉu d·ªØ li·ªáu:\")\n",
    "print(sample_data.dtypes)\n",
    "print()\n",
    "print(sample_data)\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu\n",
    "sample_data['id'] = sample_data['id'].astype('int64')\n",
    "sample_data['score'] = sample_data['score'].astype('float64')\n",
    "sample_data['date'] = pd.to_datetime(sample_data['date'])\n",
    "sample_data['category'] = sample_data['category'].astype('category')\n",
    "\n",
    "print(\"\\nSau khi chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu:\")\n",
    "print(sample_data.dtypes)\n",
    "print()\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c64432",
   "metadata": {},
   "source": [
    "### Normalization v√† Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d22fbc",
   "metadata": {},
   "source": [
    "Normalization v√† standardization l√† c√°c k·ªπ thu·∫≠t quan tr·ªçng ƒë·ªÉ ƒë∆∞a d·ªØ li·ªáu v·ªÅ c√πng m·ªôt thang ƒëo, ƒë·∫∑c bi·ªát h·ªØu √≠ch cho machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbffc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# T·∫°o d·ªØ li·ªáu m·∫´u cho normalization\n",
    "norm_data = pd.DataFrame({\n",
    "    'height': [150, 160, 170, 180, 190],  # cm\n",
    "    'weight': [50, 60, 70, 80, 90],       # kg  \n",
    "    'income': [30000, 45000, 60000, 75000, 90000]  # VND/month\n",
    "})\n",
    "\n",
    "print(\"D·ªØ li·ªáu g·ªëc:\")\n",
    "print(norm_data)\n",
    "print(\"\\nM√¥ t·∫£ th·ªëng k√™:\")\n",
    "print(norm_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2fc07",
   "metadata": {},
   "source": [
    "### Regular Expressions (Regex) cho x·ª≠ l√Ω chu·ªói n√¢ng cao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf5f680",
   "metadata": {},
   "source": [
    "**üîç Regular Expressions l√† g√¨?**\n",
    "\n",
    "Regular Expressions (*regex*) l√† **ng√¥n ng·ªØ pattern matching** m·∫°nh m·∫Ω ƒë·ªÉ t√¨m ki·∫øm v√† thao t√°c v·ªõi chu·ªói k√Ω t·ª±:\n",
    "\n",
    "**üìã C√°c k√Ω t·ª± ƒë·∫∑c bi·ªát th∆∞·ªùng d√πng:**\n",
    "\n",
    "| Pattern | M√¥ t·∫£ | V√≠ d·ª• |\n",
    "|---------|-------|-------|\n",
    "| `\\d` | S·ªë (0-9) | `\"abc123\"` ‚Üí t√¨m th·∫•y `\"123\"` |\n",
    "| `\\w` | K√Ω t·ª± t·ª´ (a-z, A-Z, 0-9, _) | `\"hello_123\"` ‚Üí t√¨m th·∫•y t·∫•t c·∫£ |\n",
    "| `\\s` | Kho·∫£ng tr·∫Øng | `\"a b c\"` ‚Üí t√¨m th·∫•y 2 spaces |\n",
    "| `+` | 1 ho·∫∑c nhi·ªÅu l·∫ßn | `\\d+` ‚Üí `\"123\"` (nhi·ªÅu s·ªë li√™n ti·∫øp) |\n",
    "| `*` | 0 ho·∫∑c nhi·ªÅu l·∫ßn | `\\d*` ‚Üí c√≥ th·ªÉ kh√¥ng c√≥ s·ªë |\n",
    "| `?` | 0 ho·∫∑c 1 l·∫ßn | `\\d?` ‚Üí t·ªëi ƒëa 1 s·ªë |\n",
    "| `[]` | Nh√≥m k√Ω t·ª± | `[0-9]` t∆∞∆°ng ƒë∆∞∆°ng `\\d` |\n",
    "| `^` | B·∫Øt ƒë·∫ßu chu·ªói | `^Hello` ‚Üí chu·ªói b·∫Øt ƒë·∫ßu b·∫±ng \"Hello\" |\n",
    "| `$` | K·∫øt th√∫c chu·ªói | `world$` ‚Üí chu·ªói k·∫øt th√∫c b·∫±ng \"world\" |\n",
    "\n",
    "**üîß S·ª≠ d·ª•ng regex v·ªõi pandas `.str` accessor:**\n",
    "\n",
    "- `.str.contains(pattern)` - ki·ªÉm tra ch·ª©a pattern\n",
    "- `.str.extract(pattern)` - tr√≠ch xu·∫•t groups t·ª´ pattern  \n",
    "- `.str.replace(pattern, replacement, regex=True)` - thay th·∫ø v·ªõi regex\n",
    "- `.str.findall(pattern)` - t√¨m t·∫•t c·∫£ matches\n",
    "\n",
    "**üì± V√≠ d·ª• th·ª±c t·∫ø: X·ª≠ l√Ω s·ªë ƒëi·ªán tho·∫°i, email, m√£ zip**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce829d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu m·∫´u v·ªõi c√°c patterns ph·ª©c t·∫°p\n",
    "data_regex = {\n",
    "    'text': [\n",
    "        'Li√™n h·ªá: 0123-456-789 ho·∫∑c email: john@gmail.com',\n",
    "        'SDT: +84 98 765 4321, ƒë·ªãa ch·ªâ: 123 L√™ L·ª£i, Q1, TP.HCM', \n",
    "        'Phone: (024) 3825-7863, email: info@company.vn',\n",
    "        'Mobile: 0987654321, website: https://example.com',\n",
    "        'Hotline: 1900-1234, fax: (028) 3829-5678'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_regex = pd.DataFrame(data_regex)\n",
    "print(\"D·ªØ li·ªáu g·ªëc:\")\n",
    "print(df_regex)\n",
    "\n",
    "# 1. Tr√≠ch xu·∫•t s·ªë ƒëi·ªán tho·∫°i\n",
    "print(\"\\n1. Tr√≠ch xu·∫•t s·ªë ƒëi·ªán tho·∫°i:\")\n",
    "phone_pattern = r'(\\+?\\d{1,3}[\\s\\-]?)?\\(?0?\\d{2,3}\\)?[\\s\\-]?\\d{3,4}[\\s\\-]?\\d{3,4}'\n",
    "df_regex['phone'] = df_regex['text'].str.extract(phone_pattern, expand=False)\n",
    "print(df_regex[['text', 'phone']])\n",
    "\n",
    "# 2. Tr√≠ch xu·∫•t email\n",
    "print(\"\\n2. Tr√≠ch xu·∫•t email:\")\n",
    "email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n",
    "df_regex['email'] = df_regex['text'].str.extract(email_pattern, expand=False)\n",
    "print(df_regex[['text', 'email']])\n",
    "\n",
    "# 3. Ki·ªÉm tra ch·ª©a website\n",
    "print(\"\\n3. Ki·ªÉm tra ch·ª©a website/URL:\")\n",
    "url_pattern = r'https?://[^\\s]+'\n",
    "df_regex['has_url'] = df_regex['text'].str.contains(url_pattern, regex=True)\n",
    "print(df_regex[['text', 'has_url']])\n",
    "\n",
    "# 4. L√†m s·∫°ch v√† chu·∫©n h√≥a s·ªë ƒëi·ªán tho·∫°i\n",
    "print(\"\\n4. Chu·∫©n h√≥a s·ªë ƒëi·ªán tho·∫°i:\")\n",
    "def clean_phone(text):\n",
    "    # T√¨m t·∫•t c·∫£ s·ªë ƒëi·ªán tho·∫°i\n",
    "    phones = str(text).replace('nan', '')\n",
    "    # Ch·ªâ gi·ªØ l·∫°i s·ªë\n",
    "    cleaned = ''.join(filter(str.isdigit, phones))\n",
    "    # Format l·∫°i n·∫øu c√≥ ƒë·ªß s·ªë\n",
    "    if len(cleaned) >= 10:\n",
    "        if cleaned.startswith('84'):\n",
    "            return f\"+84-{cleaned[2:5]}-{cleaned[5:8]}-{cleaned[8:]}\"\n",
    "        elif cleaned.startswith('0'):\n",
    "            return f\"{cleaned[:4]}-{cleaned[4:7]}-{cleaned[7:]}\"\n",
    "    return cleaned if cleaned else None\n",
    "\n",
    "df_regex['phone_clean'] = df_regex['phone'].apply(clean_phone)\n",
    "print(df_regex[['phone', 'phone_clean']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b32509",
   "metadata": {},
   "source": [
    "## X·ª≠ l√Ω d·ªØ li·ªáu ph√¢n lo·∫°i (Categorical Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0020f9e7",
   "metadata": {},
   "source": [
    "### Hi·ªÉu v·ªÅ d·ªØ li·ªáu ph√¢n lo·∫°i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30a323b",
   "metadata": {},
   "source": [
    "**üè∑Ô∏è D·ªØ li·ªáu ph√¢n lo·∫°i l√† g√¨?**\n",
    "\n",
    "D·ªØ li·ªáu ph√¢n lo·∫°i (*categorical data*) l√† lo·∫°i d·ªØ li·ªáu c√≥ **s·ªë l∆∞·ª£ng gi√° tr·ªã h·ªØu h·∫°n** v√† th∆∞·ªùng ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng **nh√£n ho·∫∑c t√™n**:\n",
    "\n",
    "**üìä C√°c lo·∫°i d·ªØ li·ªáu ph√¢n lo·∫°i:**\n",
    "\n",
    "1. **Nominal** (Danh nghƒ©a): Kh√¥ng c√≥ th·ª© t·ª±\n",
    "   - Gi·ªõi t√≠nh: Nam, N·ªØ, Kh√°c\n",
    "   - M√†u s·∫Øc: ƒê·ªè, Xanh, V√†ng\n",
    "   - Qu·ªëc gia: Vi·ªát Nam, M·ªπ, Nh·∫≠t B·∫£n\n",
    "\n",
    "2. **Ordinal** (Th·ª© t·ª±): C√≥ th·ª© t·ª± √Ω nghƒ©a\n",
    "   - H·ªçc v·ªã: C·ª≠ nh√¢n < Th·∫°c sƒ© < Ti·∫øn sƒ©\n",
    "   - ƒê√°nh gi√°: K√©m < Trung b√¨nh < T·ªët < Xu·∫•t s·∫Øc\n",
    "   - K√≠ch c·ª°: S < M < L < XL\n",
    "\n",
    "**üîß X·ª≠ l√Ω d·ªØ li·ªáu ph√¢n lo·∫°i trong pandas:**\n",
    "\n",
    "- **Ki·ªÉu `category`**: Pandas c√≥ ki·ªÉu d·ªØ li·ªáu chuy√™n d·ª•ng cho categorical data\n",
    "- **Memory efficient**: Ti·∫øt ki·ªám b·ªô nh·ªõ khi c√≥ nhi·ªÅu gi√° tr·ªã l·∫∑p l·∫°i\n",
    "- **Performance**: TƒÉng t·ªëc c√°c ph√©p to√°n groupby v√† merge\n",
    "- **Validation**: Ki·ªÉm so√°t c√°c gi√° tr·ªã h·ª£p l·ªá\n",
    "\n",
    "**‚öôÔ∏è Khi n√†o s·ª≠ d·ª•ng ki·ªÉu `category`:**\n",
    "\n",
    "- C·ªôt c√≥ **√≠t gi√° tr·ªã duy nh·∫•t** so v·ªõi t·ªïng s·ªë h√†ng\n",
    "- **Nhi·ªÅu gi√° tr·ªã l·∫∑p l·∫°i** (high cardinality)\n",
    "- Mu·ªën **ki·ªÉm so√°t c√°c gi√° tr·ªã** c√≥ th·ªÉ xu·∫•t hi·ªán\n",
    "- C·∫ßn **t·ªëi ∆∞u h√≥a b·ªô nh·ªõ** v√† hi·ªáu su·∫•t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc6ba5",
   "metadata": {},
   "source": [
    "### Label Encoding - M√£ h√≥a nh√£n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd7819",
   "metadata": {},
   "source": [
    "**üî¢ Label Encoding l√† g√¨?**\n",
    "\n",
    "Label Encoding l√† k·ªπ thu·∫≠t chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu ph√¢n lo·∫°i th√†nh **s·ªë nguy√™n tu·∫ßn t·ª±**:\n",
    "\n",
    "- `\"Apple\"` ‚Üí `0`\n",
    "- `\"Banana\"` ‚Üí `1` \n",
    "- `\"Cherry\"` ‚Üí `2`\n",
    "\n",
    "**‚úÖ ∆Øu ƒëi·ªÉm:**\n",
    "- **ƒê∆°n gi·∫£n**: D·ªÖ hi·ªÉu v√† tri·ªÉn khai\n",
    "- **Ti·∫øt ki·ªám b·ªô nh·ªõ**: Ch·ªâ c·∫ßn 1 c·ªôt\n",
    "- **Ph√π h·ª£p v·ªõi d·ªØ li·ªáu ordinal**: B·∫£o to√†n th·ª© t·ª±\n",
    "\n",
    "**‚ùå Nh∆∞·ª£c ƒëi·ªÉm:**\n",
    "- **T·∫°o th·ª© t·ª± gi·∫£ t·∫°o**: Apple < Banana < Cherry (kh√¥ng ƒë√∫ng)\n",
    "- **Kh√¥ng ph√π h·ª£p v·ªõi nominal data**: C√°c thu·∫≠t to√°n c√≥ th·ªÉ hi·ªÉu sai quan h·ªá\n",
    "- **Bias trong m√¥ h√¨nh**: Gi√° tr·ªã l·ªõn h∆°n c√≥ th·ªÉ ƒë∆∞·ª£c coi l√† \"quan tr·ªçng\" h∆°n\n",
    "\n",
    "**üéØ Khi n√†o s·ª≠ d·ª•ng Label Encoding:**\n",
    "- **D·ªØ li·ªáu ordinal** c√≥ th·ª© t·ª± t·ª± nhi√™n\n",
    "- **Tree-based algorithms** (Decision Tree, Random Forest) - √≠t b·ªã ·∫£nh h∆∞·ªüng b·ªüi th·ª© t·ª±\n",
    "- **Target variable** trong classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e7de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu categorical ƒë·ªÉ demo One-Hot Encoding\n",
    "data_categorical = {\n",
    "    'T√™n': ['An', 'B√¨nh', 'Chi', 'D≈©ng', 'Eva'],\n",
    "    'Ph√≤ng ban': ['IT', 'Marketing', 'IT', 'HR', 'Marketing'],\n",
    "    'Tr√¨nh ƒë·ªô': ['C·ª≠ nh√¢n', 'Th·∫°c sƒ©', 'C·ª≠ nh√¢n', 'Ti·∫øn sƒ©', 'Th·∫°c sƒ©'],\n",
    "    'Th√†nh ph·ªë': ['H√† N·ªôi', 'TP.HCM', 'H√† N·ªôi', 'ƒê√† N·∫µng', 'TP.HCM']\n",
    "}\n",
    "\n",
    "df_categorical = pd.DataFrame(data_categorical)\n",
    "print(\"D·ªØ li·ªáu categorical g·ªëc:\")\n",
    "print(df_categorical)\n",
    "\n",
    "print(\"S·ª≠ d·ª•ng Label Encoding ƒë·ªëi v·ªõi d·ªØ li·ªáu categorical:\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Kh·ªüi t·∫°o LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# √Åp d·ª•ng Label Encoding cho t·ª´ng c·ªôt categorical\n",
    "for col in ['Ph√≤ng ban', 'Tr√¨nh ƒë·ªô', 'Th√†nh ph·ªë']:\n",
    "    df_categorical[col] = label_encoder.fit_transform(df_categorical[col])\n",
    "    print(f\"C√°c categoricals ƒë√£ ƒë∆∞·ª£c m√£ h√≥a ƒë·ªëi v·ªõi {col}: {label_encoder.classes_}\")\n",
    "\n",
    "print(\"K·∫øt qu·∫£ Label Encoding:\")\n",
    "print(df_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7abc1c",
   "metadata": {},
   "source": [
    "### **One-Hot Encoding - M√£ h√≥a One-Hot**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47859e9",
   "metadata": {},
   "source": [
    "**üî• One-Hot Encoding l√† g√¨?**\n",
    "\n",
    "One-Hot Encoding t·∫°o ra **binary columns** cho m·ªói category:\n",
    "\n",
    "**V√≠ d·ª•:** `[\"Apple\", \"Banana\", \"Cherry\"]` ‚Üí \n",
    "\n",
    "| Apple | Banana | Cherry |\n",
    "|-------|--------|--------|\n",
    "| 1     | 0      | 0      |\n",
    "| 0     | 1      | 0      |\n",
    "| 0     | 0      | 1      |\n",
    "\n",
    "**‚úÖ ∆Øu ƒëi·ªÉm:**\n",
    "- **Kh√¥ng t·∫°o th·ª© t·ª± gi·∫£ t·∫°o**: T·∫•t c·∫£ categories ƒë·ªÅu b√¨nh ƒë·∫≥ng\n",
    "- **Ph√π h·ª£p v·ªõi nominal data**: Apple ‚â† Banana ‚â† Cherry\n",
    "- **Ho·∫°t ƒë·ªông t·ªët** v·ªõi h·∫ßu h·∫øt machine learning algorithms\n",
    "- **Tr√°nh bias**: Kh√¥ng c√≥ category n√†o ƒë∆∞·ª£c coi l√† \"quan tr·ªçng\" h∆°n\n",
    "\n",
    "**‚ùå Nh∆∞·ª£c ƒëi·ªÉm:**\n",
    "- **Curse of dimensionality**: TƒÉng s·ªë l∆∞·ª£ng features ƒë√°ng k·ªÉ  \n",
    "- **Sparse matrix**: Nhi·ªÅu gi√° tr·ªã 0, t·ªën b·ªô nh·ªõ\n",
    "- **Multicollinearity**: C√°c c·ªôt c√≥ correlation v·ªõi nhau\n",
    "\n",
    "**üéØ Khi n√†o s·ª≠ d·ª•ng One-Hot Encoding:**\n",
    "- **D·ªØ li·ªáu nominal** kh√¥ng c√≥ th·ª© t·ª± t·ª± nhi√™n\n",
    "- **√çt categories** (< 10-15 gi√° tr·ªã duy nh·∫•t)\n",
    "- **Linear algorithms** (Linear/Logistic Regression, SVM)\n",
    "- **Neural Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f7870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu categorical ƒë·ªÉ demo One-Hot Encoding\n",
    "data_categorical = {\n",
    "    'T√™n': ['An', 'B√¨nh', 'Chi', 'D≈©ng', 'Eva'],\n",
    "    'Ph√≤ng ban': ['IT', 'Marketing', 'IT', 'HR', 'Marketing'],\n",
    "    'Tr√¨nh ƒë·ªô': ['C·ª≠ nh√¢n', 'Th·∫°c sƒ©', 'C·ª≠ nh√¢n', 'Ti·∫øn sƒ©', 'Th·∫°c sƒ©'],\n",
    "    'Th√†nh ph·ªë': ['H√† N·ªôi', 'TP.HCM', 'H√† N·ªôi', 'ƒê√† N·∫µng', 'TP.HCM']\n",
    "}\n",
    "\n",
    "df_categorical = pd.DataFrame(data_categorical)\n",
    "print(\"D·ªØ li·ªáu categorical g·ªëc:\")\n",
    "print(df_categorical)\n",
    "\n",
    "print(\"PH∆Ø∆†NG PH√ÅP 1: S·ª¨ D·ª§NG pandas.get_dummies()\")\n",
    "\n",
    "# Ph∆∞∆°ng ph√°p 1: S·ª≠ d·ª•ng pandas.get_dummies()\n",
    "df_onehot_pandas = pd.get_dummies(df_categorical, \n",
    "                                  columns=['Ph√≤ng ban', 'Tr√¨nh ƒë·ªô', 'Th√†nh ph·ªë'],\n",
    "                                  prefix=['PB', 'TD', 'TP'])\n",
    "\n",
    "print(\"K·∫øt qu·∫£ One-Hot Encoding v·ªõi pandas:\")\n",
    "print(df_onehot_pandas)\n",
    "\n",
    "print(f\"\\nS·ªë c·ªôt tr∆∞·ªõc: {len(df_categorical.columns)}\")\n",
    "print(f\"S·ªë c·ªôt sau: {len(df_onehot_pandas.columns)}\")\n",
    "\n",
    "print(\"PH∆Ø∆†NG PH√ÅP 2: S·ª¨ D·ª§NG sklearn.OneHotEncoder\")\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Ph∆∞∆°ng ph√°p 2: S·ª≠ d·ª•ng sklearn OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' ƒë·ªÉ tr√°nh multicollinearity\n",
    "\n",
    "# Ch·ªâ encode c√°c c·ªôt categorical (b·ªè qua c·ªôt 'T√™n')\n",
    "categorical_cols = ['Ph√≤ng ban', 'Tr√¨nh ƒë·ªô', 'Th√†nh ph·ªë']\n",
    "encoded_data = encoder.fit_transform(df_categorical[categorical_cols])\n",
    "\n",
    "# T·∫°o t√™n c·ªôt cho k·∫øt qu·∫£\n",
    "feature_names = encoder.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# T·∫°o DataFrame m·ªõi\n",
    "df_onehot_sklearn = pd.DataFrame(encoded_data, columns=feature_names)\n",
    "df_onehot_sklearn = pd.concat([df_categorical[['T√™n']], df_onehot_sklearn], axis=1)\n",
    "\n",
    "print(\"K·∫øt qu·∫£ One-Hot Encoding v·ªõi sklearn:\")\n",
    "print(df_onehot_sklearn)\n",
    "\n",
    "print(f\"\\nL∆∞u √Ω: sklearn v·ªõi drop='first' gi·∫£m s·ªë c·ªôt ƒë·ªÉ tr√°nh multicollinearity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011775ca",
   "metadata": {},
   "source": [
    "## C√¢u h·ªèi √¥n t·∫≠p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750156b9",
   "metadata": {},
   "source": [
    "**üìù H√£y tr·∫£ l·ªùi c√°c c√¢u h·ªèi sau ƒë·ªÉ ki·ªÉm tra hi·ªÉu bi·∫øt c·ªßa b·∫°n:**\n",
    "\n",
    "| **Ph∆∞∆°ng th·ª©c n√†o d√πng ƒë·ªÉ ph√°t hi·ªán d·ªØ li·ªáu thi·∫øu trong pandas?** | |\n",
    "|---|---|\n",
    "| `isna()` ho·∫∑c `isnull()` | |\n",
    "| `missing()` | |\n",
    "| `empty()` | |\n",
    "| `nan_check()` | |\n",
    "\n",
    "| **Ph∆∞∆°ng th·ª©c `fillna()` ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ l√†m g√¨?** | |\n",
    "|---|---|\n",
    "| Lo·∫°i b·ªè d·ªØ li·ªáu thi·∫øu | |\n",
    "| Thay th·∫ø d·ªØ li·ªáu thi·∫øu | |\n",
    "| Ph√°t hi·ªán d·ªØ li·ªáu thi·∫øu | |\n",
    "| ƒê·∫øm d·ªØ li·ªáu thi·∫øu | |\n",
    "\n",
    "| **MinMaxScaler ƒë∆∞a d·ªØ li·ªáu v·ªÅ kho·∫£ng gi√° tr·ªã n√†o?** | |\n",
    "|---|---|\n",
    "| [-1, 1] | |\n",
    "| [0, 1] | |\n",
    "| [0, 100] | |\n",
    "| [-100, 100] | |\n",
    "\n",
    "| **Ph∆∞∆°ng th·ª©c n√†o d√πng ƒë·ªÉ lo·∫°i b·ªè h√†ng tr√πng l·∫∑p?** | |\n",
    "|---|---|\n",
    "| `remove_duplicates()` | |\n",
    "| `drop_duplicates()` | |\n",
    "| `delete_duplicates()` | |\n",
    "| `unique()` | |\n",
    "\n",
    "| **Trong pandas, ƒë·ªÉ chuy·ªÉn chu·ªói v·ªÅ ch·ªØ th∆∞·ªùng ta s·ª≠ d·ª•ng?** | |\n",
    "|---|---|\n",
    "| `.str.lowercase()` | |\n",
    "| `.str.lower()` | |\n",
    "| `.str.downcase()` | |\n",
    "| `.str.small()` | |\n",
    "\n",
    "| **Label Encoding ph√π h·ª£p nh·∫•t v·ªõi lo·∫°i d·ªØ li·ªáu n√†o?** | |\n",
    "|---|---|\n",
    "| D·ªØ li·ªáu s·ªë li√™n t·ª•c | |\n",
    "| D·ªØ li·ªáu nominal | |\n",
    "| D·ªØ li·ªáu ordinal | |\n",
    "| D·ªØ li·ªáu th·ªùi gian | |\n",
    "\n",
    "| **StandardScaler chu·∫©n h√≥a d·ªØ li·ªáu c√≥ Mean v√† Standard Deviation l√† bao nhi√™u?** | |\n",
    "|---|---|\n",
    "| Mean=1, Std=0 | |\n",
    "| Mean=0, Std=1 | |\n",
    "| Mean=0.5, Std=0.5 | |\n",
    "| Mean=100, Std=10 | |\n",
    "\n",
    "| **Khi n√†o n√™n s·ª≠ d·ª•ng RobustScaler thay v√¨ MinMaxScaler?** | |\n",
    "|---|---|\n",
    "| Khi d·ªØ li·ªáu c√≥ nhi·ªÅu outliers | |\n",
    "| Khi d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a | |\n",
    "| Khi d·ªØ li·ªáu l√† categorical | |\n",
    "| Khi d·ªØ li·ªáu c√≥ k√≠ch th∆∞·ªõc nh·ªè | |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
